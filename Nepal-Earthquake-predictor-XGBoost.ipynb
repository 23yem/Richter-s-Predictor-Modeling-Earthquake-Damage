{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my Richter's Predictor Nepal Earthquake Damage Predictor XGBoost Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default code from Kaggle Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "# import kerastuner as kt\n",
    "# print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "# import keras_tuner as kt2\n",
    "# print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "# import platform\n",
    "# print(\"Python:\", platform.python_version())\n",
    "\n",
    "# import numpy as np\n",
    "# print(\"numpy:\", np.__version__)\n",
    "\n",
    "# import pandas as pd\n",
    "# print(\"pandas:\", pd.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "# import matplotlib\n",
    "# print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "# import seaborn as sns\n",
    "# print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# # WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
    "\n",
    "# # Tensorflow: 2.15.0\n",
    "# # C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_6936\\1753711907.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
    "# #   import kerastuner as kt\n",
    "# # kerastuner: 1.0.5\n",
    "# # keras_tuner: 1.3.5\n",
    "# # Python: 3.10.11\n",
    "# # numpy: 1.24.3\n",
    "# # pandas: 2.1.4\n",
    "# # sklearn version: 1.2.2\n",
    "# # sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
    "# # matplotlib: 3.8.2\n",
    "# # seaborn: 0.13.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_values are the features (X), and train_labels is the target/label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"train_values.csv\")\n",
    "train_Y = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "test_values = pd.read_csv(\"test_values.csv\")\n",
    "\n",
    "# print(\"train labels:\\n\", train_Y.head())\n",
    "\n",
    "# print(\"train values:\\n\", train_X.head())\n",
    "      \n",
    "# print(\"test_values:\\n\", test_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I want to find out which features to use since there are so many. Here are some common data science techniques:\n",
    "\n",
    "1. **Correlation Matrix with Heatmap**: Correlation states how the features are related to each other or the target variable. You can use a heatmap to visualize the correlation matrix.\n",
    "\n",
    "2. **Univariate Selection**: Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "3. **Recursive Feature Elimination (RFE)**: RFE is a popular feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached.\n",
    "\n",
    "4. **Feature Importance**: You can get the feature importance of each feature of your dataset by using the feature importance property of the model. For example, Decision Trees models in the scikit-learn library offer an importance property that can be accessed directly.\n",
    "\n",
    "For categorical features, you can convert them into numerical values using techniques like One-Hot Encoding or Label Encoding before applying these feature selection techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, I will try RFE (Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from lightgbm import LGBMRegressor #Lightgbm is a great gradient boosting model for large amount of data\n",
    "\n",
    "# # Assuming X is your feature set and y is the target value\n",
    "# X = train_X.drop('building_id', axis=1)\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# y = train_Y.drop('building_id', axis=1)\n",
    "# y = np.ravel(y) # converting dataframe to a one-dimensional array using the ravel function from numpy\n",
    "\n",
    "# estimator = LGBMRegressor(verbose = 0, random_state = 42)  # It's best to find the best model for you\n",
    "# selector = RFE(estimator, step=1)\n",
    "# selector = selector.fit(X, y)\n",
    "\n",
    "# # Assuming 'X' is your DataFrame with the feature data\n",
    "# feature_names = X.columns\n",
    "\n",
    "# # Map the feature names to the support array, which tells you which features were selected\n",
    "# support_dict = dict(zip(feature_names, selector.support_))\n",
    "\n",
    "# # Get the selected features\n",
    "# selected_features = [feature for feature, support in support_dict.items() if support]\n",
    "\n",
    "# # Print the selected features\n",
    "# print(\"Selected features:\\n\", selected_features)\n",
    "\n",
    "# # ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "# # 'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "# # 'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "# # 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "# # 'land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', \n",
    "# # 'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s',\n",
    "# # 'plan_configuration_u']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. \n",
    "# # Selected features are assigned rank 1.\n",
    "# # Map the feature names to the ranking array\n",
    "# ranking_dict = dict(zip(feature_names, selector.ranking_))\n",
    "# print(ranking_dict)\n",
    "\n",
    "\n",
    "# # [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2 18 10 31\n",
    "# #  27 33 28 25 30  6  1  1  3  1  8  1  1  5  1  1  1  1 34  1  1 16  9  1\n",
    "# #  23  7  4 14  1 29 15 20 17 35 32 24 21 13 26  1 11 22 12 19]\n",
    "\n",
    "\n",
    "# # {'geo_level_1_id': 1, 'geo_level_2_id': 1, 'geo_level_3_id': 1, 'count_floors_pre_eq': 1, 'age': 1, 'area_percentage': 1, \n",
    "# #  'height_percentage': 1, 'has_superstructure_adobe_mud': 1, 'has_superstructure_mud_mortar_stone': 1, 'has_superstructure_stone_flag': 1, \n",
    "# #  'has_superstructure_cement_mortar_stone': 1, 'has_superstructure_mud_mortar_brick': 1, 'has_superstructure_cement_mortar_brick': 1, \n",
    "# #  'has_superstructure_timber': 1, 'has_superstructure_bamboo': 1, 'has_superstructure_rc_non_engineered': 1, 'has_superstructure_rc_engineered': 1, \n",
    "# #  'has_superstructure_other': 1, 'count_families': 1, 'has_secondary_use': 1, 'has_secondary_use_agriculture': 2, 'has_secondary_use_hotel': 18, \n",
    "# #  'has_secondary_use_rental': 10, 'has_secondary_use_institution': 31, 'has_secondary_use_school': 27, 'has_secondary_use_industry': 33, \n",
    "# #  'has_secondary_use_health_post': 28, 'has_secondary_use_gov_office': 25, 'has_secondary_use_use_police': 30, 'has_secondary_use_other': 6, \n",
    "# #  'land_surface_condition_n': 1, 'land_surface_condition_o': 1, 'land_surface_condition_t': 3, 'foundation_type_h': 1, 'foundation_type_i': 8, \n",
    "# #  'foundation_type_r': 1, 'foundation_type_u': 1, 'foundation_type_w': 5, 'roof_type_n': 1, 'roof_type_q': 1, 'roof_type_x': 1, \n",
    "# #  'ground_floor_type_f': 1, 'ground_floor_type_m': 34, 'ground_floor_type_v': 1, 'ground_floor_type_x': 1, 'ground_floor_type_z': 16, \n",
    "# #  'other_floor_type_j': 9, 'other_floor_type_q': 1, 'other_floor_type_s': 23, 'other_floor_type_x': 7, 'position_j': 4, 'position_o': 14, \n",
    "# #  'position_s': 1, 'position_t': 29, 'plan_configuration_a': 15, 'plan_configuration_c': 20, 'plan_configuration_d': 17, 'plan_configuration_f': 35, \n",
    "# #  'plan_configuration_m': 32, 'plan_configuration_n': 24, 'plan_configuration_o': 21, 'plan_configuration_q': 13, 'plan_configuration_s': 26, \n",
    "# #  'plan_configuration_u': 1, 'legal_ownership_status_a': 11, 'legal_ownership_status_r': 22, 'legal_ownership_status_v': 12, \n",
    "# #  'legal_ownership_status_w': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "New features:\n",
      " ['land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', 'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s', 'plan_configuration_u']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "'land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', \n",
    "'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s',\n",
    "'plan_configuration_u']\n",
    "\n",
    "print(len(features))\n",
    "\n",
    "\n",
    "# Find out which features are created through one-hot-encoding\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original data\n",
    "original_data = pd.read_csv('train_values.csv')\n",
    "\n",
    "# Get the original feature names\n",
    "original_features = original_data.columns\n",
    "\n",
    "# Check which features are not in the original data\n",
    "new_features = [feature for feature in features if feature not in original_features]\n",
    "\n",
    "# Print the new features\n",
    "print(\"New features:\\n\", new_features)\n",
    "\n",
    "\n",
    "#Manually remove the one-hot-encoding that pd.get_dummies() used on categorial \n",
    "features_before_dummies = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position','plan_configuration']\n",
    "\n",
    "print(len(features_before_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis from ChatGPT-4 (second way to find best features):\n",
    "\n",
    "The categorical variables in the features dataset have been successfully encoded. Now, let's look at the correlation of these features with the `damage_grade`:\n",
    "\n",
    "#### Correlation with `damage_grade`\n",
    "The correlation values range between -1 and 1. A value closer to 1 indicates a strong positive correlation, meaning that as the feature increases, the `damage_grade` tends to increase. Conversely, a value closer to -1 indicates a strong negative correlation, where an increase in the feature leads to a decrease in `damage_grade`. Values around 0 imply weak or no linear correlation.\n",
    "\n",
    "#### Top Positively Correlated Features:\n",
    "- `has_superstructure_mud_mortar_stone`\n",
    "- `count_floors_pre_eq`\n",
    "- Other features like `legal_ownership_status`, `has_superstructure_stone_flag`, etc., also show positive correlation but to a lesser extent.\n",
    "\n",
    "#### Top Negatively Correlated Features:\n",
    "- `has_superstructure_cement_mortar_brick`\n",
    "- `ground_floor_type`\n",
    "- `has_superstructure_rc_engineered`\n",
    "- Other features like `roof_type`, `has_superstructure_rc_non_engineered`, etc., also show negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the selected features based on the correlation threshold (of 0.05):\n",
    "\n",
    "1. `has_superstructure_mud_mortar_stone`\n",
    "2. `count_floors_pre_eq`\n",
    "3. `legal_ownership_status`\n",
    "4. `has_superstructure_stone_flag`\n",
    "5. `count_families`\n",
    "6. `has_superstructure_adobe_mud`\n",
    "7. `position`\n",
    "8. `has_superstructure_cement_mortar_stone`\n",
    "9. `has_superstructure_bamboo`\n",
    "10. `has_superstructure_timber`\n",
    "11. `geo_level_1_id`\n",
    "12. `has_secondary_use`\n",
    "13. `has_secondary_use_rental`\n",
    "14. `has_secondary_use_hotel`\n",
    "15. `foundation_type`\n",
    "16. `area_percentage`\n",
    "17. `has_superstructure_rc_non_engineered`\n",
    "18. `roof_type`\n",
    "19. `has_superstructure_rc_engineered`\n",
    "20. `ground_floor_type`\n",
    "21. `has_superstructure_cement_mortar_brick`\n",
    "\n",
    "These features were chosen because they have a correlation with the target variable `damage_grade` greater than the specified threshold of 0.05 (in absolute value). You can use these features for building your predictive model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [\n",
    "    \"has_superstructure_mud_mortar_stone\",\n",
    "    \"count_floors_pre_eq\",\n",
    "    \"legal_ownership_status\",\n",
    "    \"has_superstructure_stone_flag\",\n",
    "    \"count_families\",\n",
    "    \"has_superstructure_adobe_mud\",\n",
    "    \"position\",\n",
    "    \"has_superstructure_cement_mortar_stone\",\n",
    "    \"has_superstructure_bamboo\",\n",
    "    \"has_superstructure_timber\",\n",
    "    \"geo_level_1_id\",\n",
    "    \"has_secondary_use\",\n",
    "    \"has_secondary_use_rental\",\n",
    "    \"has_secondary_use_hotel\",\n",
    "    \"foundation_type\",\n",
    "    \"area_percentage\",\n",
    "    \"has_superstructure_rc_non_engineered\",\n",
    "    \"roof_type\",\n",
    "    \"has_superstructure_rc_engineered\",\n",
    "    \"ground_floor_type\",\n",
    "    \"has_superstructure_cement_mortar_brick\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, I will try SelectKBest to find best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "# # Create the SelectKBest with the f_classif function. You can set the parameter \"k\" equal to a number if you want to limit the amount of features\n",
    "# selector = SelectKBest(f_classif, k = 20) # Getting the 20 best features\n",
    "\n",
    "# # Assuming X is your feature set and y is the target value\n",
    "# X = train_X.drop('building_id', axis=1)\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# y = train_Y.drop('building_id', axis=1)\n",
    "# y = np.ravel(y) # converting dataframe to a one-dimensional array using the ravel function from numpy\n",
    "\n",
    "\n",
    "# # Fit the selector to the data\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the boolean mask of the selected features\n",
    "# mask = selector.get_support()\n",
    "\n",
    "# # Get the names of the selected features\n",
    "# selected_features = X.columns[mask]\n",
    "\n",
    "# # Convert the Index object to a list\n",
    "# features3 = selected_features.tolist()\n",
    "\n",
    "# print(features3)\n",
    "\n",
    "# # ['geo_level_1_id', 'count_floors_pre_eq', 'area_percentage', 'has_superstructure_mud_mortar_stone', 'has_superstructure_cement_mortar_brick', \n",
    "# #  'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_secondary_use_hotel', 'has_secondary_use_rental', \n",
    "# #  'foundation_type_i', 'foundation_type_r', 'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_x', 'ground_floor_type_f', \n",
    "# #  'ground_floor_type_v', 'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s']\n",
    "\n",
    "\n",
    "# # Get the scores\n",
    "# scores = selector.scores_\n",
    "\n",
    "# # Create a DataFrame with the scores\n",
    "# features_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\n",
    "\n",
    "# # Sort the DataFrame by score in descending order\n",
    "# features_scores = features_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# # print the best 20 features\n",
    "# print(features_scores[0:19])\n",
    "\n",
    "# #                                    Feature         Score\n",
    "# # 35                       foundation_type_r  23787.275036\n",
    "# # 43                     ground_floor_type_v  20782.933584\n",
    "# # 40                             roof_type_x  16891.038184\n",
    "# # 8      has_superstructure_mud_mortar_stone  16490.386507\n",
    "# # 34                       foundation_type_i  16385.772905\n",
    "# # 12  has_superstructure_cement_mortar_brick  11120.193268\n",
    "# # 48                      other_floor_type_s  10507.484572\n",
    "# # 41                     ground_floor_type_f  10151.525359\n",
    "# # 16        has_superstructure_rc_engineered   7757.593854\n",
    "# # 47                      other_floor_type_q   7378.599061\n",
    "# # 15    has_superstructure_rc_non_engineered   4721.916051\n",
    "# # 37                       foundation_type_w   4568.674306\n",
    "# # 46                      other_floor_type_j   4533.708398\n",
    "# # 36                       foundation_type_u   2972.409108\n",
    "# # 0                           geo_level_1_id   2657.791274\n",
    "# # 3                      count_floors_pre_eq   2544.836052\n",
    "# # 5                          area_percentage   2529.046730\n",
    "# # 38                             roof_type_n   1776.396178\n",
    "# # 21                 has_secondary_use_hotel   1537.672773\n",
    "\n",
    "# # ...\n",
    "\n",
    "# # 22                has_secondary_use_rental   1342.099336\n",
    "# # 64                legal_ownership_status_a   1166.606551\n",
    "# # 19                       has_secondary_use    841.802928\n",
    "# # 39                             roof_type_q    761.885856\n",
    "# # 7             has_superstructure_adobe_mud    739.412821\n",
    "# # 13               has_superstructure_timber    659.199014\n",
    "# # 9            has_superstructure_stone_flag    576.438023\n",
    "# # 14               has_superstructure_bamboo    538.551492\n",
    "# # 66                legal_ownership_status_v    536.308634\n",
    "# # 11     has_superstructure_mud_mortar_brick    531.784659\n",
    "# # 63                    plan_configuration_u    515.087147\n",
    "# # 10  has_superstructure_cement_mortar_stone    478.844199\n",
    "# # 18                          count_families    476.562914\n",
    "# # 56                    plan_configuration_d    378.234531\n",
    "# # 53                              position_t    373.594539\n",
    "# # 6                        height_percentage    370.173817\n",
    "# # 20           has_secondary_use_agriculture    289.462856\n",
    "# # 1                           geo_level_2_id    264.447807\n",
    "# # 49                      other_floor_type_x    244.432657\n",
    "# # 4                                      age    219.626253\n",
    "# # 33                       foundation_type_h    209.425818\n",
    "# # 32                land_surface_condition_t    201.698101\n",
    "# # 30                land_surface_condition_n    182.152148\n",
    "# # 61                    plan_configuration_q    165.370472\n",
    "# # 23           has_secondary_use_institution    146.731486\n",
    "# # 17                has_superstructure_other    142.014204\n",
    "# # 50                              position_j    136.783490\n",
    "# # 67                legal_ownership_status_w    116.039402\n",
    "# # 52                              position_s    110.038831\n",
    "# # 55                    plan_configuration_c     75.423022\n",
    "# # 51                              position_o     75.104323\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = ['geo_level_1_id', 'count_floors_pre_eq', 'area_percentage', 'has_superstructure_mud_mortar_stone', 'has_superstructure_cement_mortar_brick', \n",
    " 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_secondary_use_hotel', 'has_secondary_use_rental', \n",
    " 'foundation_type_i', 'foundation_type_r', 'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_x', 'ground_floor_type_f', \n",
    " 'ground_floor_type_v', 'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if there are any missing values in the data. If so, we have to do imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in train_X: 0\n",
      "Number of missing values in train_Y: 0\n",
      "Number of missing values in test_values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_train_X = train_X.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_X:\", missing_train_X)\n",
    "\n",
    "missing_train_Y = train_Y.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_Y:\", missing_train_Y)\n",
    "\n",
    "missing_test_values = test_values.isnull().sum().sum()\n",
    "print(\"Number of missing values in test_values:\", missing_test_values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 0 missing values in each dataframe, we don't have to do imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have 3 different list of features (features, features2, and features3), which I found using RFE. Data Analysis ChatGPT-4, and SelectKBest respectively\n",
    "\n",
    "#### Now, I have to turn one-hot-encode the data using pd.get_dummies, and I'll be creating 3 seperate train_X, one for each list of possible best features. And also on the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. testX1 for the RFE features\n",
    "# Since the features from RFE are the one-hot-encoded features, we have to apply features after doing pd.get_dummies()\n",
    "\n",
    "trainX1 = pd.get_dummies(train_X)\n",
    "trainX1 = trainX1[features]\n",
    "\n",
    "\n",
    "# 2. testX2 for the Data Analysis ChatGPT-4\n",
    "# Since the features from Data Analysis are from the original feature set, we have to apply the features before doing pd.get_dummies()\n",
    "\n",
    "trainX2 = train_X[features2]\n",
    "trainX2 = pd.get_dummies(trainX2)\n",
    "\n",
    "\n",
    "\n",
    "# 3. testX3 for the SelectKBest\n",
    "# Since the features from RFE are the one-hot-encoded features, we have to apply features after doing pd.get_dummies()\n",
    "\n",
    "trainX3 = pd.get_dummies(train_X)\n",
    "trainX3 = trainX3[features3]\n",
    "\n",
    "\n",
    "\n",
    "# 4. Do pd.get_dummies() on test data. I will create a seperate test_data for each feature selection, since each test_data needs to have a certain set of features\n",
    "\n",
    "test_data1 = pd.get_dummies(test_values)\n",
    "test_data1 = test_data1[features + ['building_id'] ]\n",
    "\n",
    "test_data2 = test_values[features2 + ['building_id'] ]\n",
    "test_data2 = pd.get_dummies(test_data2)\n",
    "\n",
    "test_data3 = pd.get_dummies(test_values)\n",
    "test_data3 = test_data3[features3 + ['building_id'] ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's important to do pd.get_dummies() before doing the train_valid_test split. Now we can do the split\n",
    "I have to do train_valid_test split three times, one for each different train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. train_valid_test split for train_X1\n",
    "train_X1, test_X1, train_Y1, test_Y1 = train_test_split(trainX1, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X1, test_X1, valid_Y1, test_Y1 = train_test_split(test_X1, test_Y1, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)\n",
    "\n",
    "\n",
    "# 2. train_valid_test split for train_X2\n",
    "train_X2, test_X2, train_Y2, test_Y2 = train_test_split(trainX2, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X2, test_X2, valid_Y2, test_Y2 = train_test_split(test_X2, test_Y2, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)\n",
    "\n",
    "\n",
    "# 3. train_valid_test split for train_X3\n",
    "train_X3, test_X3, train_Y3, test_Y3 = train_test_split(trainX3, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X3, test_X3, valid_Y3, test_Y3 = train_test_split(test_X3, test_Y3, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Normalizing Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different Normalization Scalers:\n",
    "\n",
    "1. **MinMaxScaler**: This scaler scales and translates each feature individually such that it is in the given range on the training set, e.g., between zero and one.\n",
    "\n",
    "\n",
    "2. **StandardScaler**: This scaler standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "\n",
    "3. **RobustScaler**: This scaler scales features using statistics that are robust to outliers. It uses the Interquartile Range (IQR) to scale the data, making it a better choice for when the data has outliers.\n",
    "\n",
    "\n",
    "4. **Normalizer**: This scaler scales individual samples to have unit norm. This scaler works on the rows, not the columns!\n",
    "\n",
    "\n",
    "5. **MaxAbsScaler**: This scaler scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "\n",
    "\n",
    "Remember, the choice of scaler can depend on your specific dataset and the machine learning algorithm that you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've actually decided to not do normalization since almost all the columns are categorial columns, and the non-categorial columns are mostly normalized already in the dataset, so there's no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've actually decided to not do normalization since almost all the columns are categorial columns, and the non-categorial columns are mostly normalized already in the dataset, so there's no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model 1 for XGBoost (XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and testt sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 600), # Default is 100\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 9), # Default is 6\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.4, log=True), # Default is 0.3\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Default is 1\n",
    "#         #'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # Default is 1\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1, log=True), # Default is 0\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 2, log=True),  # Default is 1\n",
    "#     }\n",
    "\n",
    "#     model = xgb.XGBClassifier(**params)\n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#     score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
    "#     f1 = score.mean()\n",
    "#     return f1       \n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100) \n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "# # TOOK 1 HOUR AND 19 MIN UTES for 100 trials\n",
    "\n",
    "# # The trials can be found on the \"Optuna_Logs_XGBoost_Model1.txt\" file!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from xgboost import XGBClassifier, plot_importance\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "\n",
    "# # Get the hyperparameters of the model and print them\n",
    "# best_params = study.best_trial.params\n",
    "\n",
    "# # Add the static parameters\n",
    "# best_params.update({\n",
    "#     'objective': 'multi:softmax',\n",
    "#     'num_class': 3,\n",
    "#     'eval_metric': 'mlogloss',\n",
    "#     'random_state': 42,\n",
    "# })\n",
    "\n",
    "# # Write the hyperparameters to a new file\n",
    "# with open('hyperparameters_XGBoost_model1.json', 'w') as f:\n",
    "#     json.dump(best_params, f)\n",
    "\n",
    "# # Create the model with the best parameters\n",
    "# model1 = XGBClassifier(**best_params) # the ** operator is used to unpack a dictionary and pass it's key value pairs as keyword arguments to a function\n",
    "\n",
    "# # Fit the model to the full data\n",
    "# model1.fit(train_X1_full, train_Y1_temp_full)  # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Plot bar plot of the feature importances to visualize the model\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = plot_importance(model1)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_XGBoost_model1.png')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 1. Score: 0.7472!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model1.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model1, 'saved_XGBoost_model1.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model1 = load('model1.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model1.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_1.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [I 2024-01-10 00:04:59,965] Trial 82 finished with value: 0.7441874743381645 and parameters: {'n_estimators': 566, 'max_depth': 9, 'learning_rate': 0.06990259781853493, 'subsample': 0.8920015569105262, 'reg_alpha': 0.13284341131896668, 'reg_lambda': 1.6799006741969436e-05}. Best is trial 82 with value: 0.7441874743381645.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model2 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 82\n",
    "#     n_estimators=566,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.06990259781853493,\n",
    "#     subsample=0.8920015569105262,\n",
    "#     reg_alpha=0.13284341131896668,\n",
    "#     reg_lambda=1.6799006741969436e-05,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model2.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model2, 'saved_XGBoost_model2.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model2 = load('model2.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model2.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_2.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # [I 2024-01-10 00:02:07,361] Trial 79 finished with value: 0.7441491014999942 and parameters: {'n_estimators': 571, 'max_depth': 9, 'learning_rate': 0.07411020030631262, 'subsample': 0.9084524981347166, 'reg_alpha': 1.5596644878161967e-05, 'reg_lambda': 1.0216930414411583e-05}. Best is trial 79 with value: 0.7441491014999942.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model3 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 79\n",
    "#     n_estimators=571,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.07411020030631262,\n",
    "#     subsample=0.9084524981347166,\n",
    "#     reg_alpha=1.5596644878161967e-05,\n",
    "#     reg_lambda=1.0216930414411583e-05,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model3.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model3, 'saved_XGBoost_model3.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model3 = load('model3.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model3.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_3.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-10 00:00:03,173] Trial 77 finished with value: 0.7441107286618239 and parameters: {'n_estimators': 586, 'max_depth': 9, 'learning_rate': 0.07372054199547849, 'subsample': 0.9084441669124462, 'reg_alpha': 4.5869925086802365e-05, 'reg_lambda': 1.2047816033954858e-05}. Best is trial 77 with value: 0.7441107286618239.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model4 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 77\n",
    "#     n_estimators=586,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.07372054199547849,\n",
    "#     subsample=0.9084441669124462,\n",
    "#     reg_alpha=4.5869925086802365e-05,\n",
    "#     reg_lambda=1.2047816033954858e-05,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model4.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model4, 'saved_XGBoost_model4.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model4 = load('model4.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model4.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_4.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.75!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-09 23:44:28,643] Trial 62 finished with value: 0.7440032847149474 and parameters: {'n_estimators': 543, 'max_depth': 9, 'learning_rate': 0.09107783861185915, 'subsample': 0.9231950980125518, 'reg_alpha': 1.0170523313529933e-05, 'reg_lambda': 0.00023219813720820273}. Best is trial 62 with value: 0.7440032847149474.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model5 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 62\n",
    "#     n_estimators=543,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model5.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model5, 'saved_XGBoost_model5.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model5 = load('model5.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model5.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_5.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-09 23:41:19,458] Trial 58 finished with value: 0.7437500239830239 and parameters: {'n_estimators': 553, 'max_depth': 9, 'learning_rate': 0.084354574047343, 'subsample': 0.877643098301532, 'reg_alpha': 1.0942230630211092e-05, 'reg_lambda': 0.0004413581362295261}. Best is trial 58 with value: 0.7437500239830239.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model6 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 58\n",
    "#     n_estimators=571,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.07411020030631262,\n",
    "#     subsample=0.9084524981347166,\n",
    "#     reg_alpha=1.5596644878161967e-05,\n",
    "#     reg_lambda=1.0216930414411583e-05,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model6.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model6, 'saved_XGBoost_model6.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model6 = load('model6.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model6.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_6.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-09 23:21:33,026] Trial 33 finished with value: 0.743596532630343 and parameters: {'n_estimators': 567, 'max_depth': 8, 'learning_rate': 0.10083473171825715, 'subsample': 0.8492514815691262, 'reg_alpha': 3.448161073426747e-05, 'reg_lambda': 0.0020333024383809638}. Best is trial 33 with value: 0.743596532630343.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model7 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 33\n",
    "#     n_estimators=567,\n",
    "#     max_depth=8,\n",
    "#     learning_rate=0.10083473171825715,\n",
    "#     subsample=0.8492514815691262,\n",
    "#     reg_alpha=3.448161073426747e-05,\n",
    "#     reg_lambda=0.0020333024383809638,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model7.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model7, 'saved_XGBoost_model7.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model7 = load('model7.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model7.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_7.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-09 23:17:51,875] Trial 28 finished with value: 0.7435121123863685 and parameters: {'n_estimators': 503, 'max_depth': 9, 'learning_rate': 0.1003129145568426, 'subsample': 0.9339489199783957, 'reg_alpha': 7.287435099971498e-05, 'reg_lambda': 0.00041200226288994816}. Best is trial 28 with value: 0.7435121123863685.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model8 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 28\n",
    "#     n_estimators=571,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.07411020030631262,\n",
    "#     subsample=0.9084524981347166,\n",
    "#     reg_alpha=1.5596644878161967e-05,\n",
    "#     reg_lambda=1.0216930414411583e-05,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model8.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model8, 'saved_XGBoost_model8.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model8 = load('model8.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model8.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_8.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9, trying out some of the other good hyperparameters from the Model 1 optuna hyperparameter tuning. Score: 74.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-09 23:01:23,154] Trial 1 finished with value: 0.7433893193042237 and parameters: {'n_estimators': 587, 'max_depth': 9, 'learning_rate': 0.05088597132699915, 'subsample': 0.825629146173626, 'reg_alpha': 0.0001624842611518334, 'reg_lambda': 0.00012216763298662962}. Best is trial 1 with value: 0.7433893193042237.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model9 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # Best parameters from trial 1\n",
    "#     n_estimators=587,\n",
    "#     max_depth=9,\n",
    "#     learning_rate=0.05088597132699915,\n",
    "#     subsample=0.825629146173626,\n",
    "#     reg_alpha=0.0001624842611518334,\n",
    "#     reg_lambda=0.00012216763298662962,\n",
    "# )\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model9.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model9, 'saved_XGBoost_model9.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model9 = load('model9.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model9.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_9.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Optuna Trial: Try testing out higher max_depth and some other hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and testt sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "\n",
    "#         #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'eval_metric': 'mlogloss',\n",
    "#         'n_estimators': 543, # Default is 100\n",
    "#         'learning_rate': 0.09107783861185915, # Default is 0.3\n",
    "#         'subsample': 0.9231950980125518, # Default is 1\n",
    "#         'reg_alpha': 1.0170523313529933e-05, # Default is 0\n",
    "#         'reg_lambda': 0.00023219813720820273,  # Default is 1\n",
    "\n",
    "#         'max_depth': trial.suggest_int('max_depth', 9, 20), # Default is 6\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), # Default is 1\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1), # Default is 1\n",
    "        \n",
    "#     }\n",
    "\n",
    "#     model = xgb.XGBClassifier(**params)\n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#     score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
    "#     f1 = score.mean()\n",
    "#     return f1       \n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100) \n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "# # Took 63 minutes and I had to stop it since CPU was over heating (90 - 100 celcius and anything above 80 is high). I got up to trial 71\n",
    "\n",
    "# # The trials can be found on the \"Optuna_Logs_XGBoost_Model10.txt\" file!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10, get best trial from Optuna trial. Score: 74.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 20:26:11,314] Trial 66 finished with value: 0.7456533167562672 and parameters: {'max_depth': 11, 'min_child_weight': 4, 'gamma': 0.2336383419099693, 'colsample_bytree': 0.7218730892946489}. Best is trial 66 with value: 0.7456533167562672.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model10 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 66 \n",
    "#     max_depth=11,\n",
    "#     min_child_weight=4,\n",
    "#     gamma=0.2336383419099693,\n",
    "#     colsample_bytree=0.7218730892946489,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model10.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model10, 'saved_XGBoost_model10.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model10 = load('model10.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model10.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_10.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11, get other good trials from Optuna. Score: 74.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 19:29:16,904] Trial 0 finished with value: 0.7456840183248545 and parameters: {'max_depth': 13, 'min_child_weight': 9, 'gamma': 0.1318407354171367, 'colsample_bytree': 0.7163120700192489}. Best is trial 0 with value: 0.7456840183248545.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model11 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 0 (of the failed attempt, on the bottom of the \"Optuna_Logs_XGBoost_Model10.txt\" document)\n",
    "#     max_depth=13,\n",
    "#     min_child_weight=9,\n",
    "#     gamma=0.1318407354171367,\n",
    "#     colsample_bytree=0.7163120700192489,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model11.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model11, 'saved_XGBoost_model11.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model11 = load('model11.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model11.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_11.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12, get other good trials from Optuna. Score: 74.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 20:11:00,692] Trial 48 finished with value: 0.745607269350463 and parameters: {'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.2067431237467906, 'colsample_bytree': 0.7155459291820985}. Best is trial 48 with value: 0.745607269350463.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model12 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 48 \n",
    "#     max_depth=10,\n",
    "#     min_child_weight=5,\n",
    "#     gamma=0.2067431237467906,\n",
    "#     colsample_bytree=0.7155459291820985,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model12.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model12, 'saved_XGBoost_model12.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model12 = load('model12.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model12.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_12.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13, get other good trials from Optuna. Score: 74.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 20:07:56,662] Trial 44 finished with value: 0.7451851681305904 and parameters: {'max_depth': 11, 'min_child_weight': 7, 'gamma': 0.21294180730420548, 'colsample_bytree': 0.75951021933263}. Best is trial 44 with value: 0.7451851681305904.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model13 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 44\n",
    "#     max_depth=11,\n",
    "#     min_child_weight=7,\n",
    "#     gamma=0.21294180730420548,\n",
    "#     colsample_bytree=0.75951021933263,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model13.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model13, 'saved_XGBoost_model13.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model13 = load('model13.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model13.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_13.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14, get other good trials from Optuna. Score: 74.82! Highest score so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 19:39:15,441] Trial 8 finished with value: 0.745123771589518 and parameters: {'max_depth': 10, 'min_child_weight': 10, 'gamma': 0.2240455435057339, 'colsample_bytree': 0.7009330927233471}. Best is trial 8 with value: 0.745123771589518.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model14 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 8\n",
    "#     max_depth=10,\n",
    "#     min_child_weight=10,\n",
    "#     gamma=0.2240455435057339,\n",
    "#     colsample_bytree=0.7009330927233471,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model14.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model14, 'saved_XGBoost_model14.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model14 = load('model14.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model14.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_14.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 15, get other good trials from Optuna. Score: 74.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 19:35:20,905] Trial 4 finished with value: 0.744456084205356 and parameters: {'max_depth': 10, 'min_child_weight': 9, 'gamma': 0.1204892873574776, 'colsample_bytree': 0.983077445033478}. Best is trial 4 with value: 0.744456084205356.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model15 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 66 \n",
    "#     max_depth=10,\n",
    "#     min_child_weight=9,\n",
    "#     gamma=0.1204892873574776,\n",
    "#     colsample_bytree=0.983077445033478,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model15.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model15, 'saved_XGBoost_model15.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model15 = load('model15.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model15.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_15.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 16, try Optuna again but with CV = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and testt sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "\n",
    "#         #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'eval_metric': 'mlogloss',\n",
    "#         'n_estimators': 543, # Default is 100\n",
    "#         'learning_rate': 0.09107783861185915, # Default is 0.3\n",
    "#         'subsample': 0.9231950980125518, # Default is 1\n",
    "#         'reg_alpha': 1.0170523313529933e-05, # Default is 0\n",
    "#         'reg_lambda': 0.00023219813720820273,  # Default is 1\n",
    "\n",
    "#         'max_depth': trial.suggest_int('max_depth', 9, 20), # Default is 6\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), # Default is 1\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1), # Default is 1\n",
    "        \n",
    "#     }\n",
    "\n",
    "#     model = xgb.XGBClassifier(**params)\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=4)\n",
    "#     f1 = score.mean()\n",
    "#     return f1       \n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50) \n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "\n",
    "\n",
    "# # The trials can be found on the \"Optuna_Logs_XGBoost_Model10.txt\" file!!!\n",
    "\n",
    "# # c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "# #   from .autonotebook import tqdm as notebook_tqdm\n",
    "# # [I 2024-01-10 20:26:47,987] A new study created in memory with name: no-name-d5cbd81d-861c-404e-983f-e113bbc1aeb2\n",
    "# # [I 2024-01-10 20:27:47,171] Trial 0 finished with value: 0.7466548438861265 and parameters: {'max_depth': 13, 'min_child_weight': 10, 'gamma': 0.377206847613114, 'colsample_bytree': 0.8339161855632286}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:29:00,378] Trial 1 finished with value: 0.745307960196463 and parameters: {'max_depth': 14, 'min_child_weight': 2, 'gamma': 0.3103015639136577, 'colsample_bytree': 0.9190031191129362}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:32:44,147] Trial 2 finished with value: 0.731175245325819 and parameters: {'max_depth': 20, 'min_child_weight': 5, 'gamma': 0.07344816628567508, 'colsample_bytree': 0.8828090345804489}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:33:55,996] Trial 3 finished with value: 0.7465665900689232 and parameters: {'max_depth': 18, 'min_child_weight': 9, 'gamma': 0.47786972934464705, 'colsample_bytree': 0.7733168439787258}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:35:12,473] Trial 4 finished with value: 0.7464860087137399 and parameters: {'max_depth': 13, 'min_child_weight': 6, 'gamma': 0.23071171657194672, 'colsample_bytree': 0.7679305598380898}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:36:07,701] Trial 5 finished with value: 0.743684785784563 and parameters: {'max_depth': 9, 'min_child_weight': 5, 'gamma': 0.47446899100728207, 'colsample_bytree': 0.9356788297232858}. Best is trial 0 with value: 0.7466548438861265.\n",
    "# # [I 2024-01-10 20:37:16,536] Trial 6 finished with value: 0.7466778701069328 and parameters: {'max_depth': 12, 'min_child_weight': 2, 'gamma': 0.30654299104889127, 'colsample_bytree': 0.7976341034830331}. Best is trial 6 with value: 0.7466778701069328.\n",
    "# # [I 2024-01-10 20:39:28,533] Trial 7 finished with value: 0.7385389979627515 and parameters: {'max_depth': 19, 'min_child_weight': 10, 'gamma': 0.10661878802572344, 'colsample_bytree': 0.7943430858990438}. Best is trial 6 with value: 0.7466778701069328.\n",
    "# # [I 2024-01-10 20:40:29,887] Trial 8 finished with value: 0.7455880800451008 and parameters: {'max_depth': 16, 'min_child_weight': 9, 'gamma': 0.33645442238736545, 'colsample_bytree': 0.9417725575171005}. Best is trial 6 with value: 0.7466778701069328.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Model 16. Score: 74.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-10 20:37:16,536] Trial 6 finished with value: 0.7466778701069328 and parameters: {'max_depth': 12, 'min_child_weight': 2, 'gamma': 0.30654299104889127, 'colsample_bytree': 0.7976341034830331}. Best is trial 6 with value: 0.7466778701069328.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model16 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 6\n",
    "#     max_depth=12,\n",
    "#     min_child_weight=2,\n",
    "#     gamma=0.30654299104889127,\n",
    "#     colsample_bytree=0.7976341034830331,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model16.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model16, 'saved_XGBoost_model16.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model16 = load('model16.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model16.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_16.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 17, another good trial from Optuna used for Model 16. Score: 74.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # [I 2024-01-10 20:27:47,171] Trial 0 finished with value: 0.7466548438861265 and parameters: {'max_depth': 13, 'min_child_weight': 10, 'gamma': 0.377206847613114, 'colsample_bytree': 0.8339161855632286}. Best is trial 0 with value: 0.7466548438861265.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model17 = XGBClassifier(\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 0\n",
    "#     max_depth=13,\n",
    "#     min_child_weight=10,\n",
    "#     gamma=0.377206847613114,\n",
    "#     colsample_bytree=0.8339161855632286,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model17.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model17, 'saved_XGBoost_model17.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model17 = load('model17.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model17.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_17.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna trials with GPU Acceleration (on XGBoost) now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# import pandas as pd\n",
    "# from optuna.samplers import TPESampler\n",
    "# from optuna.pruners import MedianPruner\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and test sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'tree_method': 'hist',  # Use histogram-based algorithm\n",
    "#         'device': 'cuda',  # Use cuda for GPU acceleration\n",
    "#         #'predictor': 'cpu_predictor', # This is to make the GPU acceleration more smooth. Making the CPU as the predictor will lighten the load for the GPU\n",
    "\n",
    "#         #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'eval_metric': 'mlogloss',\n",
    "#         #'n_estimators': 543, # Default is 100 , not needed because of the num_boost_round hyperparameter in the cross validation method\n",
    "#         'learning_rate': 0.09107783861185915, # Default is 0.3\n",
    "#         'subsample': 0.9231950980125518, # Default is 1\n",
    "#         'reg_alpha': 1.0170523313529933e-05, # Default is 0\n",
    "#         'reg_lambda': 0.00023219813720820273,  # Default is 1\n",
    "\n",
    "\n",
    "#         # These below are the ones we are testing and tuning\n",
    "#         'max_depth': trial.suggest_int('max_depth', 9, 20), # Default is 6\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), # Default is 1\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "#         #'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1), # Default is 1\n",
    "        \n",
    "#     }\n",
    "\n",
    "#     dtrain = xgb.DMatrix(train_X1_full, label=train_Y1_temp_full) # This is change the pandas data into something our GPU can handle better (DMatrix)\n",
    "\n",
    "#     cv_results = xgb.cv(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round=543,\n",
    "#         nfold=5,\n",
    "#         metrics={'merror'}, # This is for multiclass classification error\n",
    "#         early_stopping_rounds=10\n",
    "#     )\n",
    "\n",
    "#      # We want to maximize accuracy, so we return the negative of the error rate\n",
    "#     return 1 - cv_results['test-merror-mean'].min()\n",
    "\n",
    "# #sampler = CmaEsSampler() # Think about using CmaEsSampler, especially when you use ALL the features, since it's good at handling that. Also good when we are trying to tune 10+ parameters at the same time\n",
    "# sampler = TPESampler()\n",
    "\n",
    "# pruner = optuna.pruners.MedianPruner(n_warmup_steps=10, interval_steps=1) # n_warmup_steps=10 means that it won't think about pruning until after 10 steps, and interval_steps=1 means each trial is a step\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', pruner=pruner, sampler = sampler)\n",
    "\n",
    "\n",
    "# study.optimize(objective, n_trials=50) \n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "# # Took 60 minutes but because of GPU acceleration, my CPU was not hot! And neither was my GPU! It's so good for my computer health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Model 18, which uses Optuna with GPU accelerated XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-14 18:27:57,799] Trial 17 finished with value: 0.746693225853537 and parameters: {'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.01778659172817425}. Best is trial 17 with value: 0.746693225853537.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model18 = XGBClassifier( \n",
    "#     #tree_method = 'hist', It seems like turning off GPU acceleration for predictions improved score, at least for this instance\n",
    "#     #device = 'cuda', It seems like turning off GPU acceleration for predictions improved score, at least for this instance\n",
    "\n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 17\n",
    "#     max_depth=10,\n",
    "#     min_child_weight=7,\n",
    "#     gamma=0.01778659172817425,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model18.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model18, 'saved_XGBoost_model18.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model18 = load('model18.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model18.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_18.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")\n",
    "\n",
    "# # Successfully Submitted!\n",
    "# # /home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "# # Potential solutions:\n",
    "# # - Use a data structure that matches the device ordinal in the booster.\n",
    "# # - Set the device for booster before call to inplace_predict.\n",
    "\n",
    "# # This warning will only be shown once.\n",
    "\n",
    "# #   warnings.warn(smsg, UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-14 18:27:57,799] Trial 17 finished with value: 0.746693225853537 and parameters: {'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.01778659172817425}. Best is trial 17 with value: 0.746693225853537.\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "# #import cudf\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# model18 = XGBClassifier( \n",
    "#     tree_method = 'hist',  # Use histogram-based algorithm\n",
    "#     device = 'cuda',  # Use cuda for GPU acceleration\n",
    "#     gpu_id = 0,\n",
    "   \n",
    "#     objective = 'multi:softmax',\n",
    "#     num_class = 3,\n",
    "#     eval_metric = 'mlogloss',\n",
    "#     random_state = 42,\n",
    "\n",
    "#     # From Model 5\n",
    "#     n_estimators=543,\n",
    "#     learning_rate=0.09107783861185915,\n",
    "#     subsample=0.9231950980125518,\n",
    "#     reg_alpha=1.0170523313529933e-05,\n",
    "#     reg_lambda=0.00023219813720820273,\n",
    "\n",
    "#     # From trial 17\n",
    "#     max_depth=10,\n",
    "#     min_child_weight=7,\n",
    "#     gamma=0.01778659172817425,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# dtrain = xgb.DMatrix(full_X, label=full_Y_temp)\n",
    "\n",
    "# params = {\n",
    "#     'tree_method': 'hist',\n",
    "#     #'device' : 'cuda',\n",
    "#     'gpu_id': 0,\n",
    "#     'objective': 'multi:softmax',\n",
    "#     'num_class': 3,\n",
    "#     'eval_metric': 'mlogloss',\n",
    "#     'random_state': 42,\n",
    "#     'n_estimators': 543,\n",
    "#     'learning_rate': 0.09107783861185915,\n",
    "#     'subsample': 0.9231950980125518,\n",
    "#     'reg_alpha': 1.0170523313529933e-05,\n",
    "#     'reg_lambda': 0.00023219813720820273,\n",
    "#     'max_depth': 10,\n",
    "#     'min_child_weight': 7,\n",
    "#     'gamma': 0.01778659172817425\n",
    "# }\n",
    "\n",
    "# # Specify the gpu_id parameter\n",
    "# #params = {'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
    "\n",
    "\n",
    "# # Now you can fit the model on the GPU data\n",
    "# model18 = xgb.train(params, dtrain)\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# #model18.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model18, 'saved_XGBoost_model18.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model18 = load('model18.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# dtest = xgb.DMatrix(competition_test_X)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model18.predict(dtest)\n",
    "\n",
    "# competition_y_pred = competition_y_pred.astype(int)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_18.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")\n",
    "\n",
    "# # Successfully Submitted!\n",
    "# # /home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "# # Potential solutions:\n",
    "# # - Use a data structure that matches the device ordinal in the booster.\n",
    "# # - Set the device for booster before call to inplace_predict.\n",
    "\n",
    "# # This warning will only be shown once.\n",
    "\n",
    "# #   warnings.warn(smsg, UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and testt sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "\n",
    "#         'predictor': 'cpu_predictor', # This is to make the GPU acceleration more smooth. Making the CPU as the predictor will lighten the load for the GPU\n",
    "\n",
    "#         #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'eval_metric': 'mlogloss',\n",
    "#         'n_estimators': 543, # Default is 100\n",
    "#         'learning_rate': 0.09107783861185915, # Default is 0.3\n",
    "#         'subsample': 0.9231950980125518, # Default is 1\n",
    "#         'reg_alpha': 1.0170523313529933e-05, # Default is 0\n",
    "#         'reg_lambda': 0.00023219813720820273,  # Default is 1\n",
    "\n",
    "#         'max_depth': trial.suggest_int('max_depth', 9, 20), # Default is 6\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), # Default is 1\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "#         #'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1), # Default is 1\n",
    "        \n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     model = xgb.XGBClassifier(tree_method='hist', # This is for GPU Acceleration\n",
    "#                               device = 'cuda', # This is for GPU Acceleration\n",
    "#                               **params # This is for the hyper parameters above\n",
    "#                               )\n",
    "\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
    "#     f1 = score.mean()\n",
    "#     return f1       \n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50) \n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "# # Took 63 minutes and I had to stop it since CPU was over heating (90 - 100 celcius and anything above 80 is high). I got up to trial 71\n",
    "\n",
    "# # The trials can be found on the \"Optuna_Logs_XGBoost_Model18.txt\" file!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Optuna's integration with XGBoost, which is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-15 13:43:22,895] A new study created in memory with name: no-name-6ed04805-7080-4caf-95aa-dfed8d5308f5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-15 13:43:35,382] Trial 0 finished with value: 0.7317984139166027 and parameters: {'max_depth': 10, 'learning_rate': 0.026093609193315992, 'subsample': 0.7620363996518136, 'colsample_bytree': 0.9145162497256114, 'reg_alpha': 0.0003369748675678591, 'reg_lambda': 1.4056376987866263e-05, 'min_child_weight': 15, 'gamma': 0.3363479166642375, 'num_boost_round': 722}. Best is trial 0 with value: 0.7317984139166027.\n",
      "[I 2024-01-15 13:43:36,846] Trial 1 finished with value: 0.6958045535942696 and parameters: {'max_depth': 10, 'learning_rate': 0.012189580077301065, 'subsample': 0.7844944961316269, 'colsample_bytree': 0.8701102946174467, 'reg_alpha': 0.0002028051820445292, 'reg_lambda': 0.00024395950472575745, 'min_child_weight': 5, 'gamma': 0.20659328494799817, 'num_boost_round': 713}. Best is trial 0 with value: 0.7317984139166027.\n",
      "[I 2024-01-15 13:43:48,496] Trial 2 finished with value: 0.7363008442056792 and parameters: {'max_depth': 8, 'learning_rate': 0.03842002029060157, 'subsample': 0.8271109696077547, 'colsample_bytree': 0.8635214670888116, 'reg_alpha': 0.00036614903429229906, 'reg_lambda': 0.0007354384693611818, 'min_child_weight': 10, 'gamma': 0.3205184631478992, 'num_boost_round': 610}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:43:49,199] Trial 3 finished with value: 0.6652596572013303 and parameters: {'max_depth': 7, 'learning_rate': 0.016528997598469028, 'subsample': 0.8421002588978052, 'colsample_bytree': 0.7526507981839012, 'reg_alpha': 0.0002093760574537399, 'reg_lambda': 0.0001160805800530646, 'min_child_weight': 12, 'gamma': 0.35856547251591675, 'num_boost_round': 712}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:43:58,085] Trial 4 finished with value: 0.7280122793553339 and parameters: {'max_depth': 6, 'learning_rate': 0.056085478096993414, 'subsample': 0.9239938830997674, 'colsample_bytree': 0.8495325512768681, 'reg_alpha': 0.0003014863215277711, 'reg_lambda': 0.0006073095182307995, 'min_child_weight': 14, 'gamma': 0.18880944479216527, 'num_boost_round': 724}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:06,210] Trial 5 finished with value: 0.7248912765413149 and parameters: {'max_depth': 6, 'learning_rate': 0.04861679946417096, 'subsample': 0.7932770930033634, 'colsample_bytree': 0.8532680668006615, 'reg_alpha': 0.0002585260691959699, 'reg_lambda': 0.0017218331737857376, 'min_child_weight': 11, 'gamma': 0.22286405518571858, 'num_boost_round': 594}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:17,265] Trial 6 finished with value: 0.7280122793553339 and parameters: {'max_depth': 7, 'learning_rate': 0.03674518983413901, 'subsample': 0.8702633077534812, 'colsample_bytree': 0.8695915532653152, 'reg_alpha': 0.00027109283790433474, 'reg_lambda': 5.60565524633484e-05, 'min_child_weight': 10, 'gamma': 0.24227808679413737, 'num_boost_round': 661}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:26,898] Trial 7 finished with value: 0.7313635200818625 and parameters: {'max_depth': 9, 'learning_rate': 0.03534400760091924, 'subsample': 0.9102152541402495, 'colsample_bytree': 0.9388414729626532, 'reg_alpha': 0.01062206974851081, 'reg_lambda': 0.0008824207093106772, 'min_child_weight': 11, 'gamma': 0.21009417625925997, 'num_boost_round': 560}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:35,555] Trial 8 finished with value: 0.7331286774111025 and parameters: {'max_depth': 8, 'learning_rate': 0.044985336347375976, 'subsample': 0.8800436380375993, 'colsample_bytree': 0.8991697587234763, 'reg_alpha': 0.0014656955626651436, 'reg_lambda': 0.0017386910485813732, 'min_child_weight': 15, 'gamma': 0.3871053199424862, 'num_boost_round': 627}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:46,471] Trial 9 finished with value: 0.7357124584292658 and parameters: {'max_depth': 9, 'learning_rate': 0.03865896501192388, 'subsample': 0.8112737817628987, 'colsample_bytree': 0.8570778574216622, 'reg_alpha': 0.00923458207891905, 'reg_lambda': 0.0011476317654839547, 'min_child_weight': 2, 'gamma': 0.3269029155966804, 'num_boost_round': 760}. Best is trial 2 with value: 0.7363008442056792.\n",
      "[I 2024-01-15 13:44:56,655] Trial 10 finished with value: 0.7410335124072653 and parameters: {'max_depth': 8, 'learning_rate': 0.057006443048200524, 'subsample': 0.8420680632094022, 'colsample_bytree': 0.9358442517443278, 'reg_alpha': 4.074818753647432e-05, 'reg_lambda': 0.0008156045826366203, 'min_child_weight': 11, 'gamma': 0.28438233791220124, 'num_boost_round': 585}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:45:10,263] Trial 11 finished with value: 0.7386799693016117 and parameters: {'max_depth': 8, 'learning_rate': 0.036170547447541695, 'subsample': 0.7758247202582922, 'colsample_bytree': 0.854998278524193, 'reg_alpha': 6.885278359309754e-06, 'reg_lambda': 6.02200624622287e-05, 'min_child_weight': 7, 'gamma': 0.2464262102729983, 'num_boost_round': 656}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:45:24,698] Trial 12 finished with value: 0.7386288053210539 and parameters: {'max_depth': 9, 'learning_rate': 0.03154358959229871, 'subsample': 0.8422104619459783, 'colsample_bytree': 0.8512361625530883, 'reg_alpha': 1.000161875545666e-05, 'reg_lambda': 0.003991733647796162, 'min_child_weight': 10, 'gamma': 0.36677867039269907, 'num_boost_round': 590}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:45:37,467] Trial 13 finished with value: 0.7405474545919672 and parameters: {'max_depth': 8, 'learning_rate': 0.043293525794649616, 'subsample': 0.8934873557542666, 'colsample_bytree': 0.9190441770060183, 'reg_alpha': 0.00018110857009019207, 'reg_lambda': 0.0002631514628558009, 'min_child_weight': 9, 'gamma': 0.3557553673459929, 'num_boost_round': 552}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:45:50,996] Trial 14 finished with value: 0.740598618572525 and parameters: {'max_depth': 9, 'learning_rate': 0.037103192286411506, 'subsample': 0.8004420666928624, 'colsample_bytree': 0.9367504712365712, 'reg_alpha': 0.003629031960245147, 'reg_lambda': 0.0008726818449829609, 'min_child_weight': 6, 'gamma': 0.30926463031088536, 'num_boost_round': 603}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:45:57,220] Trial 15 finished with value: 0.7404962906114096 and parameters: {'max_depth': 10, 'learning_rate': 0.07915914882358549, 'subsample': 0.8143450126442094, 'colsample_bytree': 0.8251168614574541, 'reg_alpha': 0.00040921861826283983, 'reg_lambda': 0.00024484355735587354, 'min_child_weight': 11, 'gamma': 0.3459574550917288, 'num_boost_round': 582}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:46:07,035] Trial 16 finished with value: 0.7353798925556408 and parameters: {'max_depth': 7, 'learning_rate': 0.051172247599970466, 'subsample': 0.8375087207577525, 'colsample_bytree': 0.9623060071873976, 'reg_alpha': 7.035626297326523e-06, 'reg_lambda': 0.007695196948159138, 'min_child_weight': 7, 'gamma': 0.29875115366772825, 'num_boost_round': 627}. Best is trial 10 with value: 0.7410335124072653.\n",
      "[I 2024-01-15 13:46:14,198] Trial 17 finished with value: 0.7415195702225633 and parameters: {'max_depth': 7, 'learning_rate': 0.1100704836104404, 'subsample': 0.7929829332548822, 'colsample_bytree': 0.930633916867441, 'reg_alpha': 0.000838894119187267, 'reg_lambda': 0.008537054950415658, 'min_child_weight': 9, 'gamma': 0.38257399486278243, 'num_boost_round': 651}. Best is trial 17 with value: 0.7415195702225633.\n",
      "[I 2024-01-15 13:46:21,258] Trial 18 finished with value: 0.7350217446917371 and parameters: {'max_depth': 8, 'learning_rate': 0.06587019876542421, 'subsample': 0.9218949162572678, 'colsample_bytree': 0.9033323271962773, 'reg_alpha': 0.00012256923332982974, 'reg_lambda': 0.02816218199910941, 'min_child_weight': 6, 'gamma': 0.3860816562366172, 'num_boost_round': 538}. Best is trial 17 with value: 0.7415195702225633.\n",
      "[I 2024-01-15 13:46:31,741] Trial 19 finished with value: 0.740112560757227 and parameters: {'max_depth': 9, 'learning_rate': 0.0489216432963971, 'subsample': 0.8007256915275558, 'colsample_bytree': 0.9360338230686438, 'reg_alpha': 0.0112253480143015, 'reg_lambda': 0.2525745930225413, 'min_child_weight': 13, 'gamma': 0.19398736108526982, 'num_boost_round': 695}. Best is trial 17 with value: 0.7415195702225633.\n",
      "[I 2024-01-15 13:46:39,179] Trial 20 finished with value: 0.7422358659503709 and parameters: {'max_depth': 9, 'learning_rate': 0.08543648828672465, 'subsample': 0.8374644873089128, 'colsample_bytree': 0.8724333399017952, 'reg_alpha': 4.8520440546655074e-05, 'reg_lambda': 0.002352526640091158, 'min_child_weight': 9, 'gamma': 0.31615297615723914, 'num_boost_round': 670}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:46:46,616] Trial 21 finished with value: 0.7422358659503709 and parameters: {'max_depth': 8, 'learning_rate': 0.1022632953990756, 'subsample': 0.8303150822324386, 'colsample_bytree': 0.87332466258502, 'reg_alpha': 0.00036183229540937364, 'reg_lambda': 0.001895901375867775, 'min_child_weight': 7, 'gamma': 0.47235608498932086, 'num_boost_round': 626}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:46:53,150] Trial 22 finished with value: 0.7415963161933998 and parameters: {'max_depth': 8, 'learning_rate': 0.09363096140991703, 'subsample': 0.7812612059183481, 'colsample_bytree': 0.8447399073865838, 'reg_alpha': 0.00027307214335506487, 'reg_lambda': 0.0001092463461032051, 'min_child_weight': 9, 'gamma': 0.38896043696641097, 'num_boost_round': 546}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:08,918] Trial 23 finished with value: 0.7409056024558711 and parameters: {'max_depth': 9, 'learning_rate': 0.03356617920811711, 'subsample': 0.8866463132451632, 'colsample_bytree': 0.8700429491776505, 'reg_alpha': 3.7970307924630886e-05, 'reg_lambda': 5.3872940176828075e-05, 'min_child_weight': 8, 'gamma': 0.3493626425285006, 'num_boost_round': 691}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:23,025] Trial 24 finished with value: 0.734842670759785 and parameters: {'max_depth': 9, 'learning_rate': 0.02793293905540372, 'subsample': 0.8410258358487989, 'colsample_bytree': 0.9136049357234152, 'reg_alpha': 2.9638600134500112e-05, 'reg_lambda': 0.0032027400237143954, 'min_child_weight': 6, 'gamma': 0.3399131020576676, 'num_boost_round': 535}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:29,717] Trial 25 finished with value: 0.7405218726016884 and parameters: {'max_depth': 8, 'learning_rate': 0.0991164357167723, 'subsample': 0.7438880001070682, 'colsample_bytree': 0.8151583573325242, 'reg_alpha': 0.00553349823175561, 'reg_lambda': 0.009786708050521717, 'min_child_weight': 15, 'gamma': 0.47555425588768463, 'num_boost_round': 560}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:39,517] Trial 26 finished with value: 0.7407009465336403 and parameters: {'max_depth': 9, 'learning_rate': 0.05425797171939576, 'subsample': 0.8546009376109697, 'colsample_bytree': 0.8862852369817968, 'reg_alpha': 4.012462215189088e-06, 'reg_lambda': 0.003753265798744748, 'min_child_weight': 9, 'gamma': 0.35752222499846376, 'num_boost_round': 662}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:49,789] Trial 27 finished with value: 0.7385008953696597 and parameters: {'max_depth': 9, 'learning_rate': 0.046416998471712045, 'subsample': 0.7975770845845174, 'colsample_bytree': 0.8948829992343704, 'reg_alpha': 8.817253807430509e-05, 'reg_lambda': 0.00039397199267126465, 'min_child_weight': 8, 'gamma': 0.27787652404504143, 'num_boost_round': 655}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:47:59,290] Trial 28 finished with value: 0.7408544384753134 and parameters: {'max_depth': 8, 'learning_rate': 0.06189724778263737, 'subsample': 0.8213370897243486, 'colsample_bytree': 0.9084332524394703, 'reg_alpha': 0.0013781371975923765, 'reg_lambda': 0.0014176587052126212, 'min_child_weight': 16, 'gamma': 0.2698701471699663, 'num_boost_round': 635}. Best is trial 20 with value: 0.7422358659503709.\n",
      "[I 2024-01-15 13:48:14,656] Trial 29 finished with value: 0.7443847531337938 and parameters: {'max_depth': 10, 'learning_rate': 0.042010685455630574, 'subsample': 0.8261303040269653, 'colsample_bytree': 0.8654973526975905, 'reg_alpha': 0.00026453129021481115, 'reg_lambda': 0.011167316835024498, 'min_child_weight': 6, 'gamma': 0.36620755070047867, 'num_boost_round': 622}. Best is trial 29 with value: 0.7443847531337938.\n",
      "[I 2024-01-15 13:48:24,993] Trial 30 finished with value: 0.7397799948836019 and parameters: {'max_depth': 9, 'learning_rate': 0.04566037081805044, 'subsample': 0.7612041108309369, 'colsample_bytree': 0.8612601224201096, 'reg_alpha': 0.0004862660082207307, 'reg_lambda': 0.00015018241817055082, 'min_child_weight': 7, 'gamma': 0.2822606831783045, 'num_boost_round': 636}. Best is trial 29 with value: 0.7443847531337938.\n",
      "[I 2024-01-15 13:48:29,787] Trial 31 finished with value: 0.7416730621642365 and parameters: {'max_depth': 9, 'learning_rate': 0.13499593945789276, 'subsample': 0.8143374933648949, 'colsample_bytree': 0.8791043035005188, 'reg_alpha': 0.0010667827875234927, 'reg_lambda': 0.0001832676448706226, 'min_child_weight': 5, 'gamma': 0.27667797119078297, 'num_boost_round': 640}. Best is trial 29 with value: 0.7443847531337938.\n",
      "[I 2024-01-15 13:48:46,791] Trial 32 finished with value: 0.7412125863392172 and parameters: {'max_depth': 9, 'learning_rate': 0.029803448269811798, 'subsample': 0.834749398747366, 'colsample_bytree': 0.8922140869980804, 'reg_alpha': 1.4949595140204673e-05, 'reg_lambda': 0.0014535895809862315, 'min_child_weight': 7, 'gamma': 0.34138260463885733, 'num_boost_round': 583}. Best is trial 29 with value: 0.7443847531337938.\n",
      "[I 2024-01-15 13:48:55,573] Trial 33 finished with value: 0.7457661806088514 and parameters: {'max_depth': 9, 'learning_rate': 0.10910193134977265, 'subsample': 0.8180630604297976, 'colsample_bytree': 0.87895808063113, 'reg_alpha': 4.00736025404199e-05, 'reg_lambda': 0.0004686890065474283, 'min_child_weight': 4, 'gamma': 0.4531862780828521, 'num_boost_round': 625}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:08,072] Trial 34 finished with value: 0.743540547454592 and parameters: {'max_depth': 9, 'learning_rate': 0.05379951515897172, 'subsample': 0.8309162088085198, 'colsample_bytree': 0.8339374041415482, 'reg_alpha': 0.00046057184704759705, 'reg_lambda': 0.0012501856423540818, 'min_child_weight': 2, 'gamma': 0.446597144726408, 'num_boost_round': 698}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:17,099] Trial 35 finished with value: 0.741903300076746 and parameters: {'max_depth': 9, 'learning_rate': 0.06488044139554978, 'subsample': 0.8117481658693724, 'colsample_bytree': 0.845366838209034, 'reg_alpha': 0.00019098899451971076, 'reg_lambda': 0.006982383793918128, 'min_child_weight': 11, 'gamma': 0.4333945688970432, 'num_boost_round': 571}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:29,147] Trial 36 finished with value: 0.7386288053210539 and parameters: {'max_depth': 10, 'learning_rate': 0.035467790785698075, 'subsample': 0.8470085010273775, 'colsample_bytree': 0.8436592824189861, 'reg_alpha': 0.0001411966977983099, 'reg_lambda': 0.005735189929664691, 'min_child_weight': 7, 'gamma': 0.49514330921834526, 'num_boost_round': 701}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:38,108] Trial 37 finished with value: 0.7257354822205168 and parameters: {'max_depth': 7, 'learning_rate': 0.03670990816809333, 'subsample': 0.8451435877236108, 'colsample_bytree': 0.8997926732167101, 'reg_alpha': 0.001409360161180529, 'reg_lambda': 0.007211967884886097, 'min_child_weight': 9, 'gamma': 0.31399059348601577, 'num_boost_round': 660}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:56,814] Trial 38 finished with value: 0.7400358147863904 and parameters: {'max_depth': 10, 'learning_rate': 0.02591263686954006, 'subsample': 0.8709002714876328, 'colsample_bytree': 0.8682948038715248, 'reg_alpha': 5.959756081039734e-05, 'reg_lambda': 0.0035155299428652957, 'min_child_weight': 6, 'gamma': 0.4499302582716211, 'num_boost_round': 645}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:49:57,744] Trial 39 finished with value: 0.6914044512663085 and parameters: {'max_depth': 9, 'learning_rate': 0.02826205022301518, 'subsample': 0.7939763174829885, 'colsample_bytree': 0.8782564041275907, 'reg_alpha': 0.0861123367538374, 'reg_lambda': 0.005123982594181169, 'min_child_weight': 8, 'gamma': 0.46146015385175737, 'num_boost_round': 578}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:50:17,232] Trial 40 finished with value: 0.7349194167306217 and parameters: {'max_depth': 9, 'learning_rate': 0.019005026720663367, 'subsample': 0.8478509675919064, 'colsample_bytree': 0.8371856121751698, 'reg_alpha': 0.00019556455775674895, 'reg_lambda': 0.007017617593419321, 'min_child_weight': 3, 'gamma': 0.32677092734378554, 'num_boost_round': 638}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:50:23,400] Trial 41 finished with value: 0.7433103095420823 and parameters: {'max_depth': 9, 'learning_rate': 0.1164891104371324, 'subsample': 0.7266942485830375, 'colsample_bytree': 0.8461067920446019, 'reg_alpha': 1.6052498586618435e-05, 'reg_lambda': 0.0031264742283485486, 'min_child_weight': 7, 'gamma': 0.39899348177313554, 'num_boost_round': 556}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:50:30,164] Trial 42 finished with value: 0.7447684829879765 and parameters: {'max_depth': 10, 'learning_rate': 0.107771223469781, 'subsample': 0.8049763607830654, 'colsample_bytree': 0.9009358438455295, 'reg_alpha': 0.004541501245343987, 'reg_lambda': 0.0004284801635590152, 'min_child_weight': 4, 'gamma': 0.4774429017924674, 'num_boost_round': 619}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:50:43,425] Trial 43 finished with value: 0.7402916346891788 and parameters: {'max_depth': 8, 'learning_rate': 0.040710741048041975, 'subsample': 0.7892405264665582, 'colsample_bytree': 0.8836730192228818, 'reg_alpha': 0.0003332912701214068, 'reg_lambda': 0.0003397114468817193, 'min_child_weight': 10, 'gamma': 0.344201610443776, 'num_boost_round': 593}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:50:55,397] Trial 44 finished with value: 0.7403427986697365 and parameters: {'max_depth': 8, 'learning_rate': 0.04373433211219496, 'subsample': 0.8429200059182599, 'colsample_bytree': 0.8556365667547033, 'reg_alpha': 6.590537314816897e-05, 'reg_lambda': 0.0008862519346840503, 'min_child_weight': 7, 'gamma': 0.3401400289284393, 'num_boost_round': 630}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:00,182] Trial 45 finished with value: 0.7356612944487081 and parameters: {'max_depth': 8, 'learning_rate': 0.0965724114891158, 'subsample': 0.8031677226531843, 'colsample_bytree': 0.8726325383751435, 'reg_alpha': 0.001833081217652136, 'reg_lambda': 0.0010202447013498114, 'min_child_weight': 8, 'gamma': 0.4575702412301274, 'num_boost_round': 597}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:09,552] Trial 46 finished with value: 0.7390892811460732 and parameters: {'max_depth': 8, 'learning_rate': 0.05926745603238922, 'subsample': 0.8425128644416936, 'colsample_bytree': 0.8650885674638826, 'reg_alpha': 0.0005999675253008463, 'reg_lambda': 0.00011354747882277502, 'min_child_weight': 3, 'gamma': 0.4711274523534292, 'num_boost_round': 728}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:17,844] Trial 47 finished with value: 0.7413916602711691 and parameters: {'max_depth': 9, 'learning_rate': 0.06891456762061535, 'subsample': 0.8124938593963296, 'colsample_bytree': 0.7797296436059715, 'reg_alpha': 2.9684226315219656e-05, 'reg_lambda': 0.0009518583830156428, 'min_child_weight': 6, 'gamma': 0.3906783103572563, 'num_boost_round': 562}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:22,020] Trial 48 finished with value: 0.7426451777948324 and parameters: {'max_depth': 8, 'learning_rate': 0.18941669461488408, 'subsample': 0.8480392112241684, 'colsample_bytree': 0.8946208260306895, 'reg_alpha': 0.0003763052914727413, 'reg_lambda': 0.0021055442538763605, 'min_child_weight': 8, 'gamma': 0.3700062600387211, 'num_boost_round': 727}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:28,530] Trial 49 finished with value: 0.7384241493988233 and parameters: {'max_depth': 9, 'learning_rate': 0.07638744617350413, 'subsample': 0.8843684323189335, 'colsample_bytree': 0.8059605174536333, 'reg_alpha': 0.0002898111639820575, 'reg_lambda': 5.0906341621140285e-06, 'min_child_weight': 3, 'gamma': 0.4552690523868219, 'num_boost_round': 644}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:38,916] Trial 50 finished with value: 0.7418521360961883 and parameters: {'max_depth': 9, 'learning_rate': 0.05379882267488343, 'subsample': 0.8284887237278099, 'colsample_bytree': 0.9074111894932599, 'reg_alpha': 6.924463405017299e-05, 'reg_lambda': 0.00019443066824372166, 'min_child_weight': 3, 'gamma': 0.4662112787468981, 'num_boost_round': 653}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:48,182] Trial 51 finished with value: 0.7427475057559478 and parameters: {'max_depth': 10, 'learning_rate': 0.06925923929632331, 'subsample': 0.7936900326883245, 'colsample_bytree': 0.9183089641535379, 'reg_alpha': 0.16173138467631676, 'reg_lambda': 9.837300097321885e-05, 'min_child_weight': 7, 'gamma': 0.3527082887339864, 'num_boost_round': 596}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:51:59,472] Trial 52 finished with value: 0.7451010488616014 and parameters: {'max_depth': 9, 'learning_rate': 0.07318122091055689, 'subsample': 0.8062734225151206, 'colsample_bytree': 0.8682572009297584, 'reg_alpha': 0.012791427113124983, 'reg_lambda': 0.002458094854046573, 'min_child_weight': 3, 'gamma': 0.3748041713988465, 'num_boost_round': 568}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:06,739] Trial 53 finished with value: 0.7395497569710924 and parameters: {'max_depth': 9, 'learning_rate': 0.07250818672688789, 'subsample': 0.7654234684908752, 'colsample_bytree': 0.8593917200470117, 'reg_alpha': 0.002000525003555335, 'reg_lambda': 0.02693954627372612, 'min_child_weight': 8, 'gamma': 0.35972652066293614, 'num_boost_round': 619}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:16,715] Trial 54 finished with value: 0.7452289588129957 and parameters: {'max_depth': 10, 'learning_rate': 0.07527935365760681, 'subsample': 0.8160336600759805, 'colsample_bytree': 0.8129118995383078, 'reg_alpha': 7.860625317211184e-05, 'reg_lambda': 5.763917495739232e-06, 'min_child_weight': 6, 'gamma': 0.4581645279065971, 'num_boost_round': 561}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:21,324] Trial 55 finished with value: 0.742542849833717 and parameters: {'max_depth': 9, 'learning_rate': 0.1458804068313849, 'subsample': 0.7958440238110347, 'colsample_bytree': 0.8882406216313506, 'reg_alpha': 0.0002768067187591692, 'reg_lambda': 0.0007282677642502511, 'min_child_weight': 6, 'gamma': 0.41042998696877436, 'num_boost_round': 618}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:29,954] Trial 56 finished with value: 0.7441800972115631 and parameters: {'max_depth': 10, 'learning_rate': 0.09137474650401241, 'subsample': 0.7708967236657053, 'colsample_bytree': 0.946688606830166, 'reg_alpha': 0.00088874697066633, 'reg_lambda': 3.8900550254009367e-05, 'min_child_weight': 3, 'gamma': 0.4414353043751407, 'num_boost_round': 491}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:36,920] Trial 57 finished with value: 0.7447684829879765 and parameters: {'max_depth': 9, 'learning_rate': 0.11957358296464463, 'subsample': 0.8354752063585174, 'colsample_bytree': 0.8542665109656781, 'reg_alpha': 1.4264931009168145e-05, 'reg_lambda': 0.002400455155773745, 'min_child_weight': 6, 'gamma': 0.36076062431915595, 'num_boost_round': 589}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:44,326] Trial 58 finished with value: 0.7433358915323612 and parameters: {'max_depth': 10, 'learning_rate': 0.08277687995071094, 'subsample': 0.8326469784623486, 'colsample_bytree': 0.9493143086287936, 'reg_alpha': 0.00048404767868838434, 'reg_lambda': 0.00084961776437249, 'min_child_weight': 9, 'gamma': 0.35122130994221157, 'num_boost_round': 575}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:50,110] Trial 59 finished with value: 0.7450498848810437 and parameters: {'max_depth': 10, 'learning_rate': 0.12294457341866351, 'subsample': 0.8061965780934077, 'colsample_bytree': 0.8979483822479464, 'reg_alpha': 9.365214912623351e-06, 'reg_lambda': 0.00012190294512100582, 'min_child_weight': 8, 'gamma': 0.3830864530652342, 'num_boost_round': 560}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:52:57,193] Trial 60 finished with value: 0.7398823228447173 and parameters: {'max_depth': 10, 'learning_rate': 0.07167253070701433, 'subsample': 0.7701916041639457, 'colsample_bytree': 0.8672548695228118, 'reg_alpha': 0.0001088885227646035, 'reg_lambda': 2.4158992840757207e-05, 'min_child_weight': 8, 'gamma': 0.36410094453773356, 'num_boost_round': 550}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:04,799] Trial 61 finished with value: 0.7429521616781786 and parameters: {'max_depth': 10, 'learning_rate': 0.10139719245752264, 'subsample': 0.8347041800487166, 'colsample_bytree': 0.8237835968117817, 'reg_alpha': 0.0009705662716558777, 'reg_lambda': 0.0002108678666437282, 'min_child_weight': 2, 'gamma': 0.40403651352192493, 'num_boost_round': 499}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:15,807] Trial 62 finished with value: 0.7317728319263239 and parameters: {'max_depth': 9, 'learning_rate': 0.032044827951119015, 'subsample': 0.9213632489318367, 'colsample_bytree': 0.8477462638143293, 'reg_alpha': 0.0014459568959768478, 'reg_lambda': 0.00018237650328028641, 'min_child_weight': 8, 'gamma': 0.24151546077706468, 'num_boost_round': 588}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:24,500] Trial 63 finished with value: 0.7442056792018419 and parameters: {'max_depth': 9, 'learning_rate': 0.07815558162068086, 'subsample': 0.8669094040879219, 'colsample_bytree': 0.8306469227196908, 'reg_alpha': 1.3210100167021703e-05, 'reg_lambda': 4.5390068600015646e-05, 'min_child_weight': 9, 'gamma': 0.32631138038650487, 'num_boost_round': 579}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:33,486] Trial 64 finished with value: 0.7453568687643899 and parameters: {'max_depth': 10, 'learning_rate': 0.07644342986899517, 'subsample': 0.8506508738995369, 'colsample_bytree': 0.8710168759287059, 'reg_alpha': 0.0005468380610379322, 'reg_lambda': 5.531746970766966e-05, 'min_child_weight': 5, 'gamma': 0.4403567956693106, 'num_boost_round': 517}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:41,156] Trial 65 finished with value: 0.7435149654643132 and parameters: {'max_depth': 9, 'learning_rate': 0.07896900001470358, 'subsample': 0.8079803456900445, 'colsample_bytree': 0.7825342608841732, 'reg_alpha': 0.0004766705762018292, 'reg_lambda': 0.005469277688009576, 'min_child_weight': 4, 'gamma': 0.34586234055616877, 'num_boost_round': 503}. Best is trial 33 with value: 0.7457661806088514.\n",
      "[I 2024-01-15 13:53:48,124] Trial 66 finished with value: 0.7467127142491685 and parameters: {'max_depth': 10, 'learning_rate': 0.1493026087521659, 'subsample': 0.8049465235314491, 'colsample_bytree': 0.8743707399080058, 'reg_alpha': 4.53033610299336e-05, 'reg_lambda': 1.7431411609118788e-05, 'min_child_weight': 2, 'gamma': 0.42034925289062564, 'num_boost_round': 622}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:53:59,273] Trial 67 finished with value: 0.7426963417753901 and parameters: {'max_depth': 10, 'learning_rate': 0.05708468068405911, 'subsample': 0.7901376011940326, 'colsample_bytree': 0.8528958305598583, 'reg_alpha': 7.043280899267867e-06, 'reg_lambda': 0.0010555048559840545, 'min_child_weight': 5, 'gamma': 0.38023178334579316, 'num_boost_round': 581}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:05,734] Trial 68 finished with value: 0.7418521360961883 and parameters: {'max_depth': 8, 'learning_rate': 0.08706124411472234, 'subsample': 0.7929947837504312, 'colsample_bytree': 0.8707508801469829, 'reg_alpha': 0.00016824621421318104, 'reg_lambda': 0.00018518242272255774, 'min_child_weight': 9, 'gamma': 0.39232749870030686, 'num_boost_round': 430}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:12,919] Trial 69 finished with value: 0.7435661294448708 and parameters: {'max_depth': 9, 'learning_rate': 0.11529299272016531, 'subsample': 0.7447074844202652, 'colsample_bytree': 0.8847196034567961, 'reg_alpha': 0.002716343861869283, 'reg_lambda': 8.592237289047956e-05, 'min_child_weight': 5, 'gamma': 0.4743698209772717, 'num_boost_round': 602}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:22,153] Trial 70 finished with value: 0.745842926579688 and parameters: {'max_depth': 9, 'learning_rate': 0.0863279881825238, 'subsample': 0.8196744330839221, 'colsample_bytree': 0.8101977409084093, 'reg_alpha': 5.551955451476282e-05, 'reg_lambda': 0.00350365411366698, 'min_child_weight': 5, 'gamma': 0.3775516832843118, 'num_boost_round': 589}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:26,293] Trial 71 finished with value: 0.7431056536198517 and parameters: {'max_depth': 10, 'learning_rate': 0.17951503764168492, 'subsample': 0.7883046104966326, 'colsample_bytree': 0.8394429732038988, 'reg_alpha': 0.00019567309683522213, 'reg_lambda': 6.286684322980791e-05, 'min_child_weight': 4, 'gamma': 0.43807574862234616, 'num_boost_round': 497}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:32,925] Trial 72 finished with value: 0.7392171910974674 and parameters: {'max_depth': 9, 'learning_rate': 0.07651301788859544, 'subsample': 0.7745753980104301, 'colsample_bytree': 0.8937848804143473, 'reg_alpha': 3.919740573956123e-05, 'reg_lambda': 4.458200367101985e-05, 'min_child_weight': 4, 'gamma': 0.47230462878253165, 'num_boost_round': 604}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:38,334] Trial 73 finished with value: 0.7459452545408033 and parameters: {'max_depth': 9, 'learning_rate': 0.16566096131591665, 'subsample': 0.8101451686736396, 'colsample_bytree': 0.9197544482554789, 'reg_alpha': 0.00029337914580551374, 'reg_lambda': 1.101435478967028e-05, 'min_child_weight': 5, 'gamma': 0.44712892855644093, 'num_boost_round': 680}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:45,366] Trial 74 finished with value: 0.743847531337938 and parameters: {'max_depth': 9, 'learning_rate': 0.11809792508903935, 'subsample': 0.7378199971055966, 'colsample_bytree': 0.8322443976477272, 'reg_alpha': 3.932782945345194e-05, 'reg_lambda': 8.403472492479576e-05, 'min_child_weight': 1, 'gamma': 0.3724116587079729, 'num_boost_round': 665}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:54:52,280] Trial 75 finished with value: 0.744538245075467 and parameters: {'max_depth': 10, 'learning_rate': 0.10654598177419013, 'subsample': 0.8338282566460672, 'colsample_bytree': 0.7916610563028657, 'reg_alpha': 0.00026815563225087466, 'reg_lambda': 3.346773546813554e-05, 'min_child_weight': 3, 'gamma': 0.4335985788129164, 'num_boost_round': 734}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:03,487] Trial 76 finished with value: 0.7438986953184958 and parameters: {'max_depth': 10, 'learning_rate': 0.05720689752501643, 'subsample': 0.7920148198008407, 'colsample_bytree': 0.7999554454307455, 'reg_alpha': 0.0001507176290836377, 'reg_lambda': 7.254927959085156e-05, 'min_child_weight': 6, 'gamma': 0.3813002914767571, 'num_boost_round': 516}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:08,787] Trial 77 finished with value: 0.7403427986697365 and parameters: {'max_depth': 9, 'learning_rate': 0.10233278156511406, 'subsample': 0.8227232829364708, 'colsample_bytree': 0.8699464716882311, 'reg_alpha': 3.619939777835674e-05, 'reg_lambda': 1.0795562847927727e-06, 'min_child_weight': 5, 'gamma': 0.353679000562761, 'num_boost_round': 727}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:21,848] Trial 78 finished with value: 0.744512663085188 and parameters: {'max_depth': 10, 'learning_rate': 0.050990924271171544, 'subsample': 0.7544812017030389, 'colsample_bytree': 0.8613018146238308, 'reg_alpha': 0.00020736887849433913, 'reg_lambda': 0.0016438455306017512, 'min_child_weight': 7, 'gamma': 0.4483624734051245, 'num_boost_round': 636}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:27,936] Trial 79 finished with value: 0.7439498592990534 and parameters: {'max_depth': 10, 'learning_rate': 0.11503318317678198, 'subsample': 0.7680729722757798, 'colsample_bytree': 0.8891273667440311, 'reg_alpha': 1.9819265476876886e-06, 'reg_lambda': 6.012935279007678e-05, 'min_child_weight': 6, 'gamma': 0.4909457219266538, 'num_boost_round': 627}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:34,419] Trial 80 finished with value: 0.7448708109490918 and parameters: {'max_depth': 10, 'learning_rate': 0.1248901328077639, 'subsample': 0.8496935166481094, 'colsample_bytree': 0.8370145651552188, 'reg_alpha': 0.0012845005501135305, 'reg_lambda': 0.00020610667064348186, 'min_child_weight': 8, 'gamma': 0.2777984044207995, 'num_boost_round': 674}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:41,025] Trial 81 finished with value: 0.7417753901253518 and parameters: {'max_depth': 10, 'learning_rate': 0.08957952617285382, 'subsample': 0.8202132401477921, 'colsample_bytree': 0.9204264351873647, 'reg_alpha': 9.325297216879774e-05, 'reg_lambda': 0.0009005820308315922, 'min_child_weight': 3, 'gamma': 0.4741804917507239, 'num_boost_round': 682}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:45,879] Trial 82 finished with value: 0.7438219493476592 and parameters: {'max_depth': 10, 'learning_rate': 0.1640262312441614, 'subsample': 0.8112609036558721, 'colsample_bytree': 0.8336216990380483, 'reg_alpha': 0.0010970516097484062, 'reg_lambda': 0.0007555045502926339, 'min_child_weight': 5, 'gamma': 0.43779732166700425, 'num_boost_round': 742}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:55:51,490] Trial 83 finished with value: 0.7442056792018419 and parameters: {'max_depth': 9, 'learning_rate': 0.16547412025338162, 'subsample': 0.7475473127753707, 'colsample_bytree': 0.906178920783774, 'reg_alpha': 0.00021229602316541582, 'reg_lambda': 0.00011713872649315668, 'min_child_weight': 6, 'gamma': 0.3704645240845324, 'num_boost_round': 711}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:00,213] Trial 84 finished with value: 0.7453568687643899 and parameters: {'max_depth': 10, 'learning_rate': 0.07799257394793674, 'subsample': 0.8177026963272142, 'colsample_bytree': 0.8811569892240604, 'reg_alpha': 9.776512338920458e-05, 'reg_lambda': 5.527876582431561e-05, 'min_child_weight': 6, 'gamma': 0.41090419717491333, 'num_boost_round': 638}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:05,540] Trial 85 finished with value: 0.7430289076490151 and parameters: {'max_depth': 10, 'learning_rate': 0.11911438808311876, 'subsample': 0.8848672355999735, 'colsample_bytree': 0.8459221920013864, 'reg_alpha': 0.00015357490383550125, 'reg_lambda': 3.925043122343926e-06, 'min_child_weight': 7, 'gamma': 0.32172299065162974, 'num_boost_round': 619}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:12,324] Trial 86 finished with value: 0.7462266564338705 and parameters: {'max_depth': 10, 'learning_rate': 0.14709873218757746, 'subsample': 0.9050316212039479, 'colsample_bytree': 0.8485909319110699, 'reg_alpha': 3.458048269602487e-05, 'reg_lambda': 2.1036850802509296e-05, 'min_child_weight': 5, 'gamma': 0.3221989105260381, 'num_boost_round': 649}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:17,461] Trial 87 finished with value: 0.7435661294448708 and parameters: {'max_depth': 9, 'learning_rate': 0.16285462106839638, 'subsample': 0.8215532408469017, 'colsample_bytree': 0.8633118909417471, 'reg_alpha': 0.00032696720916971665, 'reg_lambda': 4.050692640704534e-05, 'min_child_weight': 7, 'gamma': 0.3285029257537952, 'num_boost_round': 596}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:27,786] Trial 88 finished with value: 0.7437196213865438 and parameters: {'max_depth': 9, 'learning_rate': 0.06150742086583795, 'subsample': 0.8097803339806959, 'colsample_bytree': 0.8901241809001444, 'reg_alpha': 0.00012198789283867032, 'reg_lambda': 0.00020827114143054498, 'min_child_weight': 10, 'gamma': 0.4652616931647712, 'num_boost_round': 689}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:38,367] Trial 89 finished with value: 0.7421079559989767 and parameters: {'max_depth': 9, 'learning_rate': 0.05369978091246938, 'subsample': 0.8139642547386013, 'colsample_bytree': 0.8475782622886735, 'reg_alpha': 0.0006712977305452332, 'reg_lambda': 6.687543142964645e-06, 'min_child_weight': 8, 'gamma': 0.31358741197738554, 'num_boost_round': 604}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:42,371] Trial 90 finished with value: 0.7402404707086212 and parameters: {'max_depth': 9, 'learning_rate': 0.14624826601211074, 'subsample': 0.8039368029131633, 'colsample_bytree': 0.781578305330512, 'reg_alpha': 0.0008407578484671633, 'reg_lambda': 3.3328760397180767e-06, 'min_child_weight': 4, 'gamma': 0.391318781264222, 'num_boost_round': 718}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:56:51,864] Trial 91 finished with value: 0.7425684318239959 and parameters: {'max_depth': 9, 'learning_rate': 0.06608700679889273, 'subsample': 0.8445774094828439, 'colsample_bytree': 0.933751405513653, 'reg_alpha': 0.00019617591899886614, 'reg_lambda': 2.085323301154141e-05, 'min_child_weight': 7, 'gamma': 0.4390241626304724, 'num_boost_round': 646}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:00,329] Trial 92 finished with value: 0.7414428242517268 and parameters: {'max_depth': 8, 'learning_rate': 0.07110467705768123, 'subsample': 0.8568868916357905, 'colsample_bytree': 0.7985884274983276, 'reg_alpha': 1.0089802571842361e-05, 'reg_lambda': 0.0007034327660007006, 'min_child_weight': 9, 'gamma': 0.3901765431623509, 'num_boost_round': 641}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:06,116] Trial 93 finished with value: 0.7421847019698132 and parameters: {'max_depth': 10, 'learning_rate': 0.11142507936078604, 'subsample': 0.7901228042100792, 'colsample_bytree': 0.8773757995816674, 'reg_alpha': 0.00014087869014553248, 'reg_lambda': 9.110035584979372e-05, 'min_child_weight': 4, 'gamma': 0.3569842416105557, 'num_boost_round': 720}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:12,107] Trial 94 finished with value: 0.7444103351240727 and parameters: {'max_depth': 10, 'learning_rate': 0.1624752171406197, 'subsample': 0.8726106727504522, 'colsample_bytree': 0.923819189906544, 'reg_alpha': 0.00016538349882429254, 'reg_lambda': 0.00010484118895945137, 'min_child_weight': 8, 'gamma': 0.440436124581453, 'num_boost_round': 595}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:18,464] Trial 95 finished with value: 0.7465848042977743 and parameters: {'max_depth': 10, 'learning_rate': 0.17784435748894564, 'subsample': 0.8680630052243326, 'colsample_bytree': 0.830517515634701, 'reg_alpha': 2.453268463063212e-05, 'reg_lambda': 0.0006777681503303542, 'min_child_weight': 10, 'gamma': 0.3113076438839553, 'num_boost_round': 657}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:29,253] Trial 96 finished with value: 0.7436172934254285 and parameters: {'max_depth': 9, 'learning_rate': 0.05944782738524249, 'subsample': 0.886959944678594, 'colsample_bytree': 0.9095883155657084, 'reg_alpha': 0.0004158318138176942, 'reg_lambda': 8.281976500971174e-06, 'min_child_weight': 5, 'gamma': 0.4428711879646862, 'num_boost_round': 583}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:35,847] Trial 97 finished with value: 0.7453568687643899 and parameters: {'max_depth': 10, 'learning_rate': 0.11972644236486546, 'subsample': 0.8354519104529465, 'colsample_bytree': 0.8620785680138235, 'reg_alpha': 9.792825084728587e-05, 'reg_lambda': 0.0002687028022384847, 'min_child_weight': 9, 'gamma': 0.3310135381726302, 'num_boost_round': 690}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:46,664] Trial 98 finished with value: 0.7442312611921207 and parameters: {'max_depth': 9, 'learning_rate': 0.06925147201487919, 'subsample': 0.8531377247778958, 'colsample_bytree': 0.807157668149884, 'reg_alpha': 7.075613508682879e-06, 'reg_lambda': 0.00037266025749672547, 'min_child_weight': 3, 'gamma': 0.32407063084748905, 'num_boost_round': 625}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:53,257] Trial 99 finished with value: 0.7461243284727552 and parameters: {'max_depth': 9, 'learning_rate': 0.16241858141132695, 'subsample': 0.8197607427257181, 'colsample_bytree': 0.8492697223251239, 'reg_alpha': 3.555811527604125e-05, 'reg_lambda': 5.5730691356247936e-05, 'min_child_weight': 3, 'gamma': 0.3006867693493771, 'num_boost_round': 518}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:57:59,543] Trial 100 finished with value: 0.7393451010488616 and parameters: {'max_depth': 9, 'learning_rate': 0.08137700462189223, 'subsample': 0.8725605558157321, 'colsample_bytree': 0.8171945332581305, 'reg_alpha': 7.806726320546514e-05, 'reg_lambda': 6.18375807091815e-05, 'min_child_weight': 10, 'gamma': 0.3388111885950371, 'num_boost_round': 672}. Best is trial 66 with value: 0.7467127142491685.\n",
      "[I 2024-01-15 13:58:07,257] Trial 101 finished with value: 0.7468406242005627 and parameters: {'max_depth': 9, 'learning_rate': 0.1474304074389946, 'subsample': 0.8384232964277991, 'colsample_bytree': 0.8967026366041799, 'reg_alpha': 0.00027568890376973327, 'reg_lambda': 2.155484508003986e-05, 'min_child_weight': 7, 'gamma': 0.3671254628447375, 'num_boost_round': 617}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:13,355] Trial 102 finished with value: 0.7446149910463034 and parameters: {'max_depth': 10, 'learning_rate': 0.11251415932626209, 'subsample': 0.8565933969468282, 'colsample_bytree': 0.8051833311271522, 'reg_alpha': 4.9781803934112e-05, 'reg_lambda': 0.00021617864725341192, 'min_child_weight': 4, 'gamma': 0.3186131554265875, 'num_boost_round': 684}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:20,850] Trial 103 finished with value: 0.745152212842159 and parameters: {'max_depth': 10, 'learning_rate': 0.10601670714853194, 'subsample': 0.8821408606221407, 'colsample_bytree': 0.8548171952744281, 'reg_alpha': 9.947328833693124e-05, 'reg_lambda': 6.352916787000211e-05, 'min_child_weight': 5, 'gamma': 0.406584102330524, 'num_boost_round': 651}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:26,098] Trial 104 finished with value: 0.746149910463034 and parameters: {'max_depth': 10, 'learning_rate': 0.18363560042625532, 'subsample': 0.8590486944691224, 'colsample_bytree': 0.8763229394381049, 'reg_alpha': 7.275328784601827e-06, 'reg_lambda': 0.00793536737468168, 'min_child_weight': 6, 'gamma': 0.3302625764535078, 'num_boost_round': 554}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:31,200] Trial 105 finished with value: 0.745152212842159 and parameters: {'max_depth': 10, 'learning_rate': 0.1576813885386937, 'subsample': 0.8404353070304763, 'colsample_bytree': 0.9013485963563493, 'reg_alpha': 3.5592420976196816e-06, 'reg_lambda': 0.0002213891432666082, 'min_child_weight': 13, 'gamma': 0.41101163090378723, 'num_boost_round': 650}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:36,488] Trial 106 finished with value: 0.7450243028907648 and parameters: {'max_depth': 10, 'learning_rate': 0.17867647374937007, 'subsample': 0.837186435469286, 'colsample_bytree': 0.8730612175432713, 'reg_alpha': 4.656284653766412e-06, 'reg_lambda': 5.89830383786662e-06, 'min_child_weight': 6, 'gamma': 0.24107183472733315, 'num_boost_round': 611}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:45,997] Trial 107 finished with value: 0.745996418521361 and parameters: {'max_depth': 9, 'learning_rate': 0.07703859340966318, 'subsample': 0.8716277199827316, 'colsample_bytree': 0.8430401400641009, 'reg_alpha': 6.562979609406637e-05, 'reg_lambda': 0.0020806143049861337, 'min_child_weight': 9, 'gamma': 0.33551343898582464, 'num_boost_round': 738}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:51,137] Trial 108 finished with value: 0.743694039396265 and parameters: {'max_depth': 9, 'learning_rate': 0.1816721609388841, 'subsample': 0.8840578716018527, 'colsample_bytree': 0.8802949862764696, 'reg_alpha': 0.0008155595238650858, 'reg_lambda': 0.002380030541596428, 'min_child_weight': 11, 'gamma': 0.35167867799210467, 'num_boost_round': 648}. Best is trial 101 with value: 0.7468406242005627.\n",
      "[I 2024-01-15 13:58:56,971] Trial 109 finished with value: 0.7471731900741878 and parameters: {'max_depth': 10, 'learning_rate': 0.16361066367857321, 'subsample': 0.8073135963222022, 'colsample_bytree': 0.8882314153232878, 'reg_alpha': 0.0001240155675116328, 'reg_lambda': 0.00019569639241642954, 'min_child_weight': 6, 'gamma': 0.3351807578721107, 'num_boost_round': 563}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:02,078] Trial 110 finished with value: 0.7433358915323612 and parameters: {'max_depth': 10, 'learning_rate': 0.13625800757170406, 'subsample': 0.7934763368281721, 'colsample_bytree': 0.8492247716664787, 'reg_alpha': 1.633622447864834e-06, 'reg_lambda': 0.0004431494256017626, 'min_child_weight': 10, 'gamma': 0.2981604290246461, 'num_boost_round': 451}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:08,860] Trial 111 finished with value: 0.7458940905602456 and parameters: {'max_depth': 10, 'learning_rate': 0.13645659977805055, 'subsample': 0.804373237463591, 'colsample_bytree': 0.815063269249402, 'reg_alpha': 0.0001523818975139857, 'reg_lambda': 0.0001675605613332085, 'min_child_weight': 5, 'gamma': 0.36103491381322295, 'num_boost_round': 589}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:15,034] Trial 112 finished with value: 0.740752110514198 and parameters: {'max_depth': 10, 'learning_rate': 0.10987612100284631, 'subsample': 0.7826470887037305, 'colsample_bytree': 0.8667750734700287, 'reg_alpha': 0.00010450052043282766, 'reg_lambda': 0.0006514153431433546, 'min_child_weight': 1, 'gamma': 0.3902473259817383, 'num_boost_round': 642}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:20,024] Trial 113 finished with value: 0.7366845740598619 and parameters: {'max_depth': 10, 'learning_rate': 0.09482583467378473, 'subsample': 0.7897347684936733, 'colsample_bytree': 0.8351810325626118, 'reg_alpha': 0.0007987355249062403, 'reg_lambda': 0.0007861597734624379, 'min_child_weight': 3, 'gamma': 0.4853059371025308, 'num_boost_round': 609}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:24,539] Trial 114 finished with value: 0.7431312356101305 and parameters: {'max_depth': 9, 'learning_rate': 0.1793754314351609, 'subsample': 0.7809682585701941, 'colsample_bytree': 0.8912473068437257, 'reg_alpha': 7.840173100637163e-05, 'reg_lambda': 3.855301420143236e-05, 'min_child_weight': 7, 'gamma': 0.339124785133203, 'num_boost_round': 593}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:29,747] Trial 115 finished with value: 0.7438731133282169 and parameters: {'max_depth': 10, 'learning_rate': 0.1636401512711947, 'subsample': 0.8262628302658027, 'colsample_bytree': 0.877666030041347, 'reg_alpha': 4.057143620904411e-05, 'reg_lambda': 0.00018425688993990646, 'min_child_weight': 7, 'gamma': 0.30137278752949836, 'num_boost_round': 545}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:40,447] Trial 116 finished with value: 0.7433358915323612 and parameters: {'max_depth': 9, 'learning_rate': 0.05538988532243299, 'subsample': 0.8214534334385414, 'colsample_bytree': 0.8779640949944507, 'reg_alpha': 9.26288691306421e-05, 'reg_lambda': 0.0001774282121516758, 'min_child_weight': 10, 'gamma': 0.3783494519305856, 'num_boost_round': 607}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:48,256] Trial 117 finished with value: 0.7457405986185726 and parameters: {'max_depth': 9, 'learning_rate': 0.1125711756751916, 'subsample': 0.8208073423222408, 'colsample_bytree': 0.8930822268476157, 'reg_alpha': 0.0001989688960114433, 'reg_lambda': 0.0012569642902007549, 'min_child_weight': 7, 'gamma': 0.3545288648058922, 'num_boost_round': 671}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:52,230] Trial 118 finished with value: 0.7417498081350729 and parameters: {'max_depth': 10, 'learning_rate': 0.1675565763473233, 'subsample': 0.8213368549191697, 'colsample_bytree': 0.854234894547875, 'reg_alpha': 0.00012399587835529592, 'reg_lambda': 0.00012899607771720426, 'min_child_weight': 12, 'gamma': 0.3769465557080438, 'num_boost_round': 571}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 13:59:59,329] Trial 119 finished with value: 0.7448196469685342 and parameters: {'max_depth': 10, 'learning_rate': 0.11531230484945458, 'subsample': 0.8385073025593832, 'colsample_bytree': 0.9218149128452309, 'reg_alpha': 0.00010632154967146767, 'reg_lambda': 0.0015557672739148897, 'min_child_weight': 8, 'gamma': 0.3174845847527369, 'num_boost_round': 639}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:04,956] Trial 120 finished with value: 0.7431568176004093 and parameters: {'max_depth': 9, 'learning_rate': 0.14495800388033622, 'subsample': 0.8358578990404439, 'colsample_bytree': 0.8729995305117149, 'reg_alpha': 0.00018704002709535853, 'reg_lambda': 0.0003458882540691258, 'min_child_weight': 9, 'gamma': 0.35557618913244027, 'num_boost_round': 636}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:09,796] Trial 121 finished with value: 0.7434126375031977 and parameters: {'max_depth': 9, 'learning_rate': 0.1630719672481625, 'subsample': 0.7938478894680423, 'colsample_bytree': 0.8598383114576291, 'reg_alpha': 0.000289732306885363, 'reg_lambda': 0.00028958028255304693, 'min_child_weight': 6, 'gamma': 0.3111960775460529, 'num_boost_round': 661}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:15,762] Trial 122 finished with value: 0.741724226144794 and parameters: {'max_depth': 9, 'learning_rate': 0.09214214549089303, 'subsample': 0.8237582560207143, 'colsample_bytree': 0.8352699719307092, 'reg_alpha': 0.0002450939015677052, 'reg_lambda': 0.0025702292914534363, 'min_child_weight': 6, 'gamma': 0.4425732371474222, 'num_boost_round': 642}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:23,766] Trial 123 finished with value: 0.742056792018419 and parameters: {'max_depth': 9, 'learning_rate': 0.0714695231244654, 'subsample': 0.8440714022052042, 'colsample_bytree': 0.8335353202279224, 'reg_alpha': 0.0007802839288257179, 'reg_lambda': 0.00017352225732757506, 'min_child_weight': 5, 'gamma': 0.36286336906027206, 'num_boost_round': 608}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:29,448] Trial 124 finished with value: 0.7430800716295728 and parameters: {'max_depth': 10, 'learning_rate': 0.11554730581331202, 'subsample': 0.7910752007165233, 'colsample_bytree': 0.8941927666797533, 'reg_alpha': 0.00013604215090036762, 'reg_lambda': 3.425718740526343e-05, 'min_child_weight': 8, 'gamma': 0.35670612980309774, 'num_boost_round': 641}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:34,335] Trial 125 finished with value: 0.7433358915323612 and parameters: {'max_depth': 9, 'learning_rate': 0.1721146039391395, 'subsample': 0.8424143576098283, 'colsample_bytree': 0.8458527617552656, 'reg_alpha': 0.00014808862624121925, 'reg_lambda': 0.0007823381846926616, 'min_child_weight': 6, 'gamma': 0.34261158284834964, 'num_boost_round': 629}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:40,033] Trial 126 finished with value: 0.7400613967766692 and parameters: {'max_depth': 9, 'learning_rate': 0.0872757629830238, 'subsample': 0.8028232919755314, 'colsample_bytree': 0.8928945414072927, 'reg_alpha': 4.383256158132278e-05, 'reg_lambda': 0.0018884435473396325, 'min_child_weight': 11, 'gamma': 0.38688120417860244, 'num_boost_round': 547}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:47,377] Trial 127 finished with value: 0.7440777692504477 and parameters: {'max_depth': 10, 'learning_rate': 0.08910769222040384, 'subsample': 0.8289357042324154, 'colsample_bytree': 0.8530228154083814, 'reg_alpha': 0.0001428256114385621, 'reg_lambda': 0.00011404818908286765, 'min_child_weight': 7, 'gamma': 0.3659903410013026, 'num_boost_round': 596}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:00:56,601] Trial 128 finished with value: 0.7471731900741878 and parameters: {'max_depth': 10, 'learning_rate': 0.1108866159041365, 'subsample': 0.7877730041335349, 'colsample_bytree': 0.8935464512214046, 'reg_alpha': 3.834197417026058e-05, 'reg_lambda': 0.00039925250589750835, 'min_child_weight': 7, 'gamma': 0.32956718657243433, 'num_boost_round': 642}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:02,936] Trial 129 finished with value: 0.7465080583269379 and parameters: {'max_depth': 10, 'learning_rate': 0.17402944931469316, 'subsample': 0.8098855554313322, 'colsample_bytree': 0.7939654178009474, 'reg_alpha': 3.5361536823499745e-05, 'reg_lambda': 0.0016940575683641948, 'min_child_weight': 4, 'gamma': 0.3343023589084678, 'num_boost_round': 597}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:08,822] Trial 130 finished with value: 0.7455359426963417 and parameters: {'max_depth': 10, 'learning_rate': 0.13854897351022472, 'subsample': 0.8353137020446597, 'colsample_bytree': 0.8849937260886963, 'reg_alpha': 6.0880223682226875e-05, 'reg_lambda': 0.00021581952740826906, 'min_child_weight': 5, 'gamma': 0.3768027276786479, 'num_boost_round': 586}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:16,067] Trial 131 finished with value: 0.7441033512407265 and parameters: {'max_depth': 9, 'learning_rate': 0.09665329185211405, 'subsample': 0.8058489961438979, 'colsample_bytree': 0.8518597846626681, 'reg_alpha': 8.94030938203009e-05, 'reg_lambda': 0.0001655265747841907, 'min_child_weight': 7, 'gamma': 0.2994146144332398, 'num_boost_round': 684}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:23,542] Trial 132 finished with value: 0.7437963673573804 and parameters: {'max_depth': 9, 'learning_rate': 0.0819558995728788, 'subsample': 0.8209803132235349, 'colsample_bytree': 0.8509334203979917, 'reg_alpha': 7.774543624999373e-05, 'reg_lambda': 5.643653309673814e-05, 'min_child_weight': 5, 'gamma': 0.4774547429110605, 'num_boost_round': 550}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:30,731] Trial 133 finished with value: 0.7435149654643132 and parameters: {'max_depth': 9, 'learning_rate': 0.09347746915692928, 'subsample': 0.8239454608868749, 'colsample_bytree': 0.8721634470575772, 'reg_alpha': 0.00014395308563959188, 'reg_lambda': 6.026512142536153e-05, 'min_child_weight': 5, 'gamma': 0.393529248878946, 'num_boost_round': 639}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:37,940] Trial 134 finished with value: 0.7452545408032745 and parameters: {'max_depth': 10, 'learning_rate': 0.12691170247617697, 'subsample': 0.79007854513251, 'colsample_bytree': 0.8783705008554894, 'reg_alpha': 4.578277042395011e-05, 'reg_lambda': 0.0002366914525592905, 'min_child_weight': 4, 'gamma': 0.32399407278594505, 'num_boost_round': 611}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:47,500] Trial 135 finished with value: 0.7457405986185726 and parameters: {'max_depth': 10, 'learning_rate': 0.0769657214963469, 'subsample': 0.8232675042153988, 'colsample_bytree': 0.8812480513093665, 'reg_alpha': 3.839326530687277e-05, 'reg_lambda': 0.0007778312381135939, 'min_child_weight': 4, 'gamma': 0.43014495315206785, 'num_boost_round': 589}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:53,458] Trial 136 finished with value: 0.7428498337170633 and parameters: {'max_depth': 9, 'learning_rate': 0.11544933221565754, 'subsample': 0.7991184636296196, 'colsample_bytree': 0.8826178330453116, 'reg_alpha': 7.839738211725922e-05, 'reg_lambda': 0.0002424847341505914, 'min_child_weight': 5, 'gamma': 0.3640485841371809, 'num_boost_round': 585}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:01:57,413] Trial 137 finished with value: 0.7401381427475058 and parameters: {'max_depth': 8, 'learning_rate': 0.16337913842347518, 'subsample': 0.8456206526021337, 'colsample_bytree': 0.8905092749420003, 'reg_alpha': 3.765407890122521e-05, 'reg_lambda': 0.0005263504799698077, 'min_child_weight': 5, 'gamma': 0.28160315330346386, 'num_boost_round': 640}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:02,438] Trial 138 finished with value: 0.7363008442056792 and parameters: {'max_depth': 9, 'learning_rate': 0.0859713042195885, 'subsample': 0.8282898195794685, 'colsample_bytree': 0.8878377251791572, 'reg_alpha': 2.616033362462549e-05, 'reg_lambda': 0.00016487158343184177, 'min_child_weight': 8, 'gamma': 0.3820509369260867, 'num_boost_round': 644}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:14,314] Trial 139 finished with value: 0.7457917625991302 and parameters: {'max_depth': 10, 'learning_rate': 0.06451339397987414, 'subsample': 0.8114442860577719, 'colsample_bytree': 0.8614122067702344, 'reg_alpha': 0.0003267759763058392, 'reg_lambda': 0.0005834704863492188, 'min_child_weight': 6, 'gamma': 0.33001868760470365, 'num_boost_round': 599}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:20,565] Trial 140 finished with value: 0.7442312611921207 and parameters: {'max_depth': 10, 'learning_rate': 0.1259330794402201, 'subsample': 0.8125082135365466, 'colsample_bytree': 0.867804874294803, 'reg_alpha': 0.00020815755409241487, 'reg_lambda': 0.001423636724933593, 'min_child_weight': 6, 'gamma': 0.3262529057485546, 'num_boost_round': 542}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:27,444] Trial 141 finished with value: 0.7460220005116398 and parameters: {'max_depth': 10, 'learning_rate': 0.11495723165554039, 'subsample': 0.7793614387180465, 'colsample_bytree': 0.8969042860392482, 'reg_alpha': 0.0002911029244949185, 'reg_lambda': 0.004000630018537796, 'min_child_weight': 4, 'gamma': 0.3822419361770604, 'num_boost_round': 603}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:35,155] Trial 142 finished with value: 0.7438219493476592 and parameters: {'max_depth': 10, 'learning_rate': 0.08358977492360771, 'subsample': 0.8081320739256377, 'colsample_bytree': 0.8621924732714843, 'reg_alpha': 2.754862168394558e-05, 'reg_lambda': 0.000721919300769582, 'min_child_weight': 4, 'gamma': 0.32762001841370714, 'num_boost_round': 546}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:42,194] Trial 143 finished with value: 0.7421079559989767 and parameters: {'max_depth': 10, 'learning_rate': 0.08847785451384121, 'subsample': 0.8198748741177567, 'colsample_bytree': 0.8562233322943349, 'reg_alpha': 0.0002446056694163501, 'reg_lambda': 0.0002222021453378596, 'min_child_weight': 5, 'gamma': 0.38950079574212504, 'num_boost_round': 620}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:02:50,516] Trial 144 finished with value: 0.7414684062420056 and parameters: {'max_depth': 10, 'learning_rate': 0.06784512155237173, 'subsample': 0.8072499843578389, 'colsample_bytree': 0.8340320357611891, 'reg_alpha': 6.606218968696651e-05, 'reg_lambda': 0.0007803013323344303, 'min_child_weight': 8, 'gamma': 0.39164128530135506, 'num_boost_round': 560}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:01,001] Trial 145 finished with value: 0.7417498081350729 and parameters: {'max_depth': 10, 'learning_rate': 0.05151986152860239, 'subsample': 0.8372781718212545, 'colsample_bytree': 0.8694925847661401, 'reg_alpha': 5.667886884261924e-05, 'reg_lambda': 0.0005509942244007124, 'min_child_weight': 7, 'gamma': 0.378582543664805, 'num_boost_round': 550}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:07,275] Trial 146 finished with value: 0.7336914811972371 and parameters: {'max_depth': 10, 'learning_rate': 0.061938806604946606, 'subsample': 0.8256451175436594, 'colsample_bytree': 0.8699812688573201, 'reg_alpha': 7.153491513737993e-05, 'reg_lambda': 0.0010953921393859205, 'min_child_weight': 4, 'gamma': 0.2800775651848065, 'num_boost_round': 565}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:17,800] Trial 147 finished with value: 0.7409823484267075 and parameters: {'max_depth': 10, 'learning_rate': 0.05221415146729498, 'subsample': 0.7839454953776976, 'colsample_bytree': 0.8844674181970136, 'reg_alpha': 1.175973689914691e-05, 'reg_lambda': 0.0015250885531418476, 'min_child_weight': 4, 'gamma': 0.32808153411898294, 'num_boost_round': 544}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:23,840] Trial 148 finished with value: 0.7442312611921207 and parameters: {'max_depth': 10, 'learning_rate': 0.12483720546358577, 'subsample': 0.8408927632742729, 'colsample_bytree': 0.8958916824412828, 'reg_alpha': 6.754070973687343e-05, 'reg_lambda': 0.0006460220650468238, 'min_child_weight': 7, 'gamma': 0.33116857855721205, 'num_boost_round': 599}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:32,552] Trial 149 finished with value: 0.7421335379892555 and parameters: {'max_depth': 10, 'learning_rate': 0.06906673280317033, 'subsample': 0.7813283877655565, 'colsample_bytree': 0.8919334464553078, 'reg_alpha': 0.0001697261128103796, 'reg_lambda': 0.0006330120013808814, 'min_child_weight': 3, 'gamma': 0.313533964681878, 'num_boost_round': 635}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:44,400] Trial 150 finished with value: 0.7425684318239959 and parameters: {'max_depth': 10, 'learning_rate': 0.050191220432765686, 'subsample': 0.7988282734645076, 'colsample_bytree': 0.868780633924852, 'reg_alpha': 0.00029688422837535484, 'reg_lambda': 0.002787597539538684, 'min_child_weight': 3, 'gamma': 0.31262980269246937, 'num_boost_round': 650}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:52,180] Trial 151 finished with value: 0.7449219749296495 and parameters: {'max_depth': 10, 'learning_rate': 0.08602590392415983, 'subsample': 0.7845103112491255, 'colsample_bytree': 0.9505221866731743, 'reg_alpha': 0.0023877708667196357, 'reg_lambda': 0.0005227215486516635, 'min_child_weight': 3, 'gamma': 0.3529813502120024, 'num_boost_round': 536}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:03:58,329] Trial 152 finished with value: 0.7431568176004093 and parameters: {'max_depth': 10, 'learning_rate': 0.10705455941810553, 'subsample': 0.7941184109368115, 'colsample_bytree': 0.8558520977972387, 'reg_alpha': 0.00011194609044708433, 'reg_lambda': 0.017811783048115783, 'min_child_weight': 7, 'gamma': 0.3117472854204257, 'num_boost_round': 595}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:07,556] Trial 153 finished with value: 0.7444870810949091 and parameters: {'max_depth': 10, 'learning_rate': 0.08001483948168288, 'subsample': 0.8145064054189926, 'colsample_bytree': 0.8503892600222904, 'reg_alpha': 0.00011808927184943836, 'reg_lambda': 0.010279129259280245, 'min_child_weight': 5, 'gamma': 0.35700810409148176, 'num_boost_round': 586}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:18,512] Trial 154 finished with value: 0.7452289588129957 and parameters: {'max_depth': 10, 'learning_rate': 0.06779457468554499, 'subsample': 0.7912690822569467, 'colsample_bytree': 0.8881130674010573, 'reg_alpha': 0.0001964277826418005, 'reg_lambda': 0.0006335050237105585, 'min_child_weight': 6, 'gamma': 0.35031801279016833, 'num_boost_round': 566}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:24,256] Trial 155 finished with value: 0.7450498848810437 and parameters: {'max_depth': 10, 'learning_rate': 0.14403843960968798, 'subsample': 0.8144047953343473, 'colsample_bytree': 0.9267223706595481, 'reg_alpha': 0.0001796268158757609, 'reg_lambda': 0.00026363116961522455, 'min_child_weight': 5, 'gamma': 0.366205767431419, 'num_boost_round': 520}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:31,986] Trial 156 finished with value: 0.7426451777948324 and parameters: {'max_depth': 10, 'learning_rate': 0.07562800923980206, 'subsample': 0.814650471495691, 'colsample_bytree': 0.8699428419705667, 'reg_alpha': 0.0004414825327646199, 'reg_lambda': 0.0003571013406355971, 'min_child_weight': 6, 'gamma': 0.32558188744680866, 'num_boost_round': 638}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:39,905] Trial 157 finished with value: 0.7465080583269379 and parameters: {'max_depth': 10, 'learning_rate': 0.10681559452293819, 'subsample': 0.8118000926324738, 'colsample_bytree': 0.8612387687436744, 'reg_alpha': 0.0001855529338190218, 'reg_lambda': 0.0028814312670552124, 'min_child_weight': 7, 'gamma': 0.32515950937471877, 'num_boost_round': 559}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:47,553] Trial 158 finished with value: 0.746149910463034 and parameters: {'max_depth': 10, 'learning_rate': 0.1202179617010789, 'subsample': 0.8073205928295111, 'colsample_bytree': 0.9012008523869494, 'reg_alpha': 5.093379095120083e-05, 'reg_lambda': 0.0009782438425695067, 'min_child_weight': 4, 'gamma': 0.2989244920924054, 'num_boost_round': 596}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:04:53,038] Trial 159 finished with value: 0.74469173701714 and parameters: {'max_depth': 9, 'learning_rate': 0.15517738429026395, 'subsample': 0.7916909798736035, 'colsample_bytree': 0.9186510910932713, 'reg_alpha': 0.0001213376998543268, 'reg_lambda': 0.0017308526373278651, 'min_child_weight': 6, 'gamma': 0.3629277569096315, 'num_boost_round': 656}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:05:01,261] Trial 160 finished with value: 0.74469173701714 and parameters: {'max_depth': 10, 'learning_rate': 0.08505262390055812, 'subsample': 0.8033011172174105, 'colsample_bytree': 0.8949349680919134, 'reg_alpha': 1.4152292658391657e-05, 'reg_lambda': 0.0008046934092931674, 'min_child_weight': 4, 'gamma': 0.3625085087937887, 'num_boost_round': 589}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:05:06,645] Trial 161 finished with value: 0.7448196469685342 and parameters: {'max_depth': 10, 'learning_rate': 0.137031769158702, 'subsample': 0.8197820402246369, 'colsample_bytree': 0.8741003343722323, 'reg_alpha': 2.7109826632463317e-05, 'reg_lambda': 0.0038866045111731855, 'min_child_weight': 6, 'gamma': 0.29113472472215074, 'num_boost_round': 575}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:05:13,881] Trial 162 finished with value: 0.7468150422102839 and parameters: {'max_depth': 10, 'learning_rate': 0.1399472870348744, 'subsample': 0.8328870376873754, 'colsample_bytree': 0.8540534820977557, 'reg_alpha': 0.0002960956011498698, 'reg_lambda': 0.00042286625069414387, 'min_child_weight': 6, 'gamma': 0.2903662793866109, 'num_boost_round': 570}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:05:20,157] Trial 163 finished with value: 0.7456382706574571 and parameters: {'max_depth': 10, 'learning_rate': 0.11852122597306324, 'subsample': 0.82096070532961, 'colsample_bytree': 0.9051463538056796, 'reg_alpha': 0.0011444490384311737, 'reg_lambda': 0.0009891629269964406, 'min_child_weight': 5, 'gamma': 0.30699970837444485, 'num_boost_round': 585}. Best is trial 109 with value: 0.7471731900741878.\n",
      "[I 2024-01-15 14:05:27,066] Trial 164 finished with value: 0.7473266820158608 and parameters: {'max_depth': 10, 'learning_rate': 0.16600004757130238, 'subsample': 0.8338515850264939, 'colsample_bytree': 0.8350193167977603, 'reg_alpha': 5.89065889962893e-05, 'reg_lambda': 0.0018907516717618252, 'min_child_weight': 6, 'gamma': 0.28284008599691146, 'num_boost_round': 571}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:05:35,496] Trial 165 finished with value: 0.7432847275518035 and parameters: {'max_depth': 10, 'learning_rate': 0.08014740315479646, 'subsample': 0.8062510138782947, 'colsample_bytree': 0.9148575165637464, 'reg_alpha': 0.00026304664376535365, 'reg_lambda': 0.000857793051222099, 'min_child_weight': 5, 'gamma': 0.33766783476288487, 'num_boost_round': 567}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:05:43,492] Trial 166 finished with value: 0.7429521616781786 and parameters: {'max_depth': 10, 'learning_rate': 0.07925572870548561, 'subsample': 0.7845294179173506, 'colsample_bytree': 0.9053805637839694, 'reg_alpha': 5.5206256260754675e-05, 'reg_lambda': 0.0008729629313219714, 'min_child_weight': 7, 'gamma': 0.3381227295648021, 'num_boost_round': 594}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:05:52,088] Trial 167 finished with value: 0.7454847787157841 and parameters: {'max_depth': 10, 'learning_rate': 0.08428293815852725, 'subsample': 0.8287287524276862, 'colsample_bytree': 0.8613555347996305, 'reg_alpha': 6.106568655478518e-05, 'reg_lambda': 0.0009291982125760441, 'min_child_weight': 7, 'gamma': 0.32694105850921107, 'num_boost_round': 548}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:05:59,294] Trial 168 finished with value: 0.7448963929393707 and parameters: {'max_depth': 10, 'learning_rate': 0.10604211872309818, 'subsample': 0.7941641051377125, 'colsample_bytree': 0.9016349159633505, 'reg_alpha': 0.0005986708772286711, 'reg_lambda': 0.000729204515769135, 'min_child_weight': 5, 'gamma': 0.30083683706781594, 'num_boost_round': 459}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:05,808] Trial 169 finished with value: 0.7442568431823996 and parameters: {'max_depth': 10, 'learning_rate': 0.0984777678230098, 'subsample': 0.7999404219322249, 'colsample_bytree': 0.8863883952586513, 'reg_alpha': 0.0001453717097447907, 'reg_lambda': 0.004268366938740265, 'min_child_weight': 6, 'gamma': 0.296327691854113, 'num_boost_round': 557}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:11,944] Trial 170 finished with value: 0.7453312867741111 and parameters: {'max_depth': 10, 'learning_rate': 0.13385557853514077, 'subsample': 0.8040871488835422, 'colsample_bytree': 0.8734384493630473, 'reg_alpha': 0.00010386973435795378, 'reg_lambda': 0.00313498738170297, 'min_child_weight': 6, 'gamma': 0.34460908012564584, 'num_boost_round': 530}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:17,972] Trial 171 finished with value: 0.7409567664364287 and parameters: {'max_depth': 9, 'learning_rate': 0.10720907138740064, 'subsample': 0.8426869535633295, 'colsample_bytree': 0.8800698841397105, 'reg_alpha': 0.00024422993989794143, 'reg_lambda': 0.002311776167257319, 'min_child_weight': 6, 'gamma': 0.16462875861368326, 'num_boost_round': 555}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:24,186] Trial 172 finished with value: 0.7453824507546687 and parameters: {'max_depth': 10, 'learning_rate': 0.14773199485663235, 'subsample': 0.849100346942967, 'colsample_bytree': 0.8881744417022479, 'reg_alpha': 0.00028310194600656596, 'reg_lambda': 0.001706211008284849, 'min_child_weight': 6, 'gamma': 0.3332854272718934, 'num_boost_round': 523}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:35,058] Trial 173 finished with value: 0.7460987464824763 and parameters: {'max_depth': 10, 'learning_rate': 0.07052406305896097, 'subsample': 0.8318087005676388, 'colsample_bytree': 0.89354626842306, 'reg_alpha': 0.00011384104976741539, 'reg_lambda': 0.0011494002812677952, 'min_child_weight': 4, 'gamma': 0.2880318589781505, 'num_boost_round': 610}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:40,478] Trial 174 finished with value: 0.7440521872601689 and parameters: {'max_depth': 10, 'learning_rate': 0.1624875923218854, 'subsample': 0.8366907042015408, 'colsample_bytree': 0.8944075136818116, 'reg_alpha': 0.000148363986466991, 'reg_lambda': 0.0015177884505795233, 'min_child_weight': 6, 'gamma': 0.34512673124855897, 'num_boost_round': 538}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:45,434] Trial 175 finished with value: 0.7454591967255053 and parameters: {'max_depth': 10, 'learning_rate': 0.1792280090227239, 'subsample': 0.8157612632527786, 'colsample_bytree': 0.8320421943437029, 'reg_alpha': 0.0002748673630137184, 'reg_lambda': 0.0003317039281845842, 'min_child_weight': 5, 'gamma': 0.3134623802987846, 'num_boost_round': 540}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:50,212] Trial 176 finished with value: 0.7437452033768227 and parameters: {'max_depth': 10, 'learning_rate': 0.17341596792202313, 'subsample': 0.8172571499242306, 'colsample_bytree': 0.8062005071936378, 'reg_alpha': 4.873420250577543e-05, 'reg_lambda': 0.0007978507580642955, 'min_child_weight': 3, 'gamma': 0.35496583181074737, 'num_boost_round': 522}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:54,984] Trial 177 finished with value: 0.7391660271169097 and parameters: {'max_depth': 9, 'learning_rate': 0.11810602918713584, 'subsample': 0.824167686811588, 'colsample_bytree': 0.8175832167755577, 'reg_alpha': 0.000208239497967317, 'reg_lambda': 0.0007597314557264773, 'min_child_weight': 5, 'gamma': 0.3040368957848903, 'num_boost_round': 609}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:06:59,305] Trial 178 finished with value: 0.7429521616781786 and parameters: {'max_depth': 9, 'learning_rate': 0.16750083303510924, 'subsample': 0.8121894608491312, 'colsample_bytree': 0.8660137821997822, 'reg_alpha': 0.0002252215975076578, 'reg_lambda': 0.005825167833274174, 'min_child_weight': 5, 'gamma': 0.2726492571090909, 'num_boost_round': 600}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:03,817] Trial 179 finished with value: 0.7432335635712459 and parameters: {'max_depth': 9, 'learning_rate': 0.1781517410137455, 'subsample': 0.8216624700852354, 'colsample_bytree': 0.8461455962589923, 'reg_alpha': 0.0004096037546490196, 'reg_lambda': 0.0005818202232689487, 'min_child_weight': 5, 'gamma': 0.28561931858196105, 'num_boost_round': 514}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:13,080] Trial 180 finished with value: 0.7459708365310821 and parameters: {'max_depth': 10, 'learning_rate': 0.09209849478536998, 'subsample': 0.8453992980617325, 'colsample_bytree': 0.8577219033850509, 'reg_alpha': 0.00019321053673795706, 'reg_lambda': 0.0013677172639589151, 'min_child_weight': 6, 'gamma': 0.3085984730610442, 'num_boost_round': 609}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:22,389] Trial 181 finished with value: 0.7473266820158608 and parameters: {'max_depth': 10, 'learning_rate': 0.09970340382541809, 'subsample': 0.8438091781498112, 'colsample_bytree': 0.9030177843659862, 'reg_alpha': 0.00023620852085782417, 'reg_lambda': 0.0035201684330327695, 'min_child_weight': 6, 'gamma': 0.29014345794343466, 'num_boost_round': 678}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:30,045] Trial 182 finished with value: 0.7451266308518802 and parameters: {'max_depth': 10, 'learning_rate': 0.10648502920688913, 'subsample': 0.834074934665885, 'colsample_bytree': 0.8324996923016159, 'reg_alpha': 0.0008871864904621067, 'reg_lambda': 0.004450637068133582, 'min_child_weight': 5, 'gamma': 0.28198029805499614, 'num_boost_round': 599}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:35,737] Trial 183 finished with value: 0.7413916602711691 and parameters: {'max_depth': 10, 'learning_rate': 0.09895969310675594, 'subsample': 0.8168887227133143, 'colsample_bytree': 0.8335822361751644, 'reg_alpha': 0.0003465830270649773, 'reg_lambda': 0.0032382374911530576, 'min_child_weight': 4, 'gamma': 0.268214469242801, 'num_boost_round': 580}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:46,351] Trial 184 finished with value: 0.7464568943463801 and parameters: {'max_depth': 10, 'learning_rate': 0.07677904299859864, 'subsample': 0.8271556542544335, 'colsample_bytree': 0.8967002224940331, 'reg_alpha': 3.892793276191773e-05, 'reg_lambda': 0.0031820656653697627, 'min_child_weight': 3, 'gamma': 0.351921037989267, 'num_boost_round': 632}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:07:54,364] Trial 185 finished with value: 0.7446149910463034 and parameters: {'max_depth': 10, 'learning_rate': 0.10053257517144809, 'subsample': 0.8461579775796437, 'colsample_bytree': 0.8707404636643323, 'reg_alpha': 0.000239320722353393, 'reg_lambda': 0.0005777150572751278, 'min_child_weight': 4, 'gamma': 0.2652884990781798, 'num_boost_round': 508}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:04,774] Trial 186 finished with value: 0.7447173190074188 and parameters: {'max_depth': 10, 'learning_rate': 0.06738363267530371, 'subsample': 0.8153792776625454, 'colsample_bytree': 0.8870876197135507, 'reg_alpha': 0.0004281162055463078, 'reg_lambda': 0.0012723947178689798, 'min_child_weight': 4, 'gamma': 0.2813947074947823, 'num_boost_round': 592}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:11,023] Trial 187 finished with value: 0.7423381939114864 and parameters: {'max_depth': 10, 'learning_rate': 0.09672427363671346, 'subsample': 0.8246974549084337, 'colsample_bytree': 0.8589325091556298, 'reg_alpha': 8.492603109370168e-05, 'reg_lambda': 0.0052337929693913885, 'min_child_weight': 6, 'gamma': 0.2436333833905645, 'num_boost_round': 613}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:18,139] Trial 188 finished with value: 0.74469173701714 and parameters: {'max_depth': 10, 'learning_rate': 0.1169728262456335, 'subsample': 0.8380318489906795, 'colsample_bytree': 0.8785316789241046, 'reg_alpha': 0.0010535678431947862, 'reg_lambda': 0.0011555670956290697, 'min_child_weight': 4, 'gamma': 0.34355013894290115, 'num_boost_round': 532}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:26,622] Trial 189 finished with value: 0.7470708621130724 and parameters: {'max_depth': 10, 'learning_rate': 0.11003199488681953, 'subsample': 0.8761114245285806, 'colsample_bytree': 0.8396839102108716, 'reg_alpha': 0.00013712722942569943, 'reg_lambda': 0.00029891724028448366, 'min_child_weight': 4, 'gamma': 0.27407200176322927, 'num_boost_round': 536}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:36,076] Trial 190 finished with value: 0.7454336147352264 and parameters: {'max_depth': 10, 'learning_rate': 0.08400360172690954, 'subsample': 0.8151220704976918, 'colsample_bytree': 0.882849518448231, 'reg_alpha': 0.00021327335732478005, 'reg_lambda': 0.00043147030175231626, 'min_child_weight': 3, 'gamma': 0.33438539602178874, 'num_boost_round': 601}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:41,931] Trial 191 finished with value: 0.7465080583269379 and parameters: {'max_depth': 10, 'learning_rate': 0.17280081452791887, 'subsample': 0.8305960273337384, 'colsample_bytree': 0.845061089266356, 'reg_alpha': 0.00011245968388608042, 'reg_lambda': 0.0010542250479694273, 'min_child_weight': 6, 'gamma': 0.2766128029602382, 'num_boost_round': 622}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:48,846] Trial 192 finished with value: 0.7426195958045536 and parameters: {'max_depth': 10, 'learning_rate': 0.09482423956907245, 'subsample': 0.8509369099316523, 'colsample_bytree': 0.8582419098655654, 'reg_alpha': 8.182105756833705e-05, 'reg_lambda': 0.0016856240324028542, 'min_child_weight': 5, 'gamma': 0.2311143612361625, 'num_boost_round': 582}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:08:56,944] Trial 193 finished with value: 0.7415963161933998 and parameters: {'max_depth': 9, 'learning_rate': 0.07087534686743527, 'subsample': 0.8316120307479326, 'colsample_bytree': 0.9023219369639639, 'reg_alpha': 0.00026069474109427683, 'reg_lambda': 0.0013722139760098458, 'min_child_weight': 5, 'gamma': 0.28866697466808816, 'num_boost_round': 662}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:06,268] Trial 194 finished with value: 0.745177794832438 and parameters: {'max_depth': 10, 'learning_rate': 0.07927963227769484, 'subsample': 0.8531519286615764, 'colsample_bytree': 0.8968867745885224, 'reg_alpha': 5.534351934923964e-05, 'reg_lambda': 0.0016532525104086325, 'min_child_weight': 2, 'gamma': 0.2987903289495073, 'num_boost_round': 605}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:13,552] Trial 195 finished with value: 0.7432591455615247 and parameters: {'max_depth': 9, 'learning_rate': 0.09737880823116911, 'subsample': 0.8545461130172629, 'colsample_bytree': 0.8975752185453729, 'reg_alpha': 8.027464697701397e-05, 'reg_lambda': 0.0010767071581331694, 'min_child_weight': 4, 'gamma': 0.3440466600653741, 'num_boost_round': 663}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:21,977] Trial 196 finished with value: 0.7472243540547454 and parameters: {'max_depth': 10, 'learning_rate': 0.12432192684280914, 'subsample': 0.8517521264911335, 'colsample_bytree': 0.8839893554017728, 'reg_alpha': 0.00010359780638190863, 'reg_lambda': 0.0005056726176113019, 'min_child_weight': 4, 'gamma': 0.28013984818806464, 'num_boost_round': 653}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:30,784] Trial 197 finished with value: 0.7448963929393707 and parameters: {'max_depth': 10, 'learning_rate': 0.08857201569922878, 'subsample': 0.8171307811504527, 'colsample_bytree': 0.9090800794955005, 'reg_alpha': 6.426830061133368e-05, 'reg_lambda': 0.006412441850720493, 'min_child_weight': 3, 'gamma': 0.3096043108737726, 'num_boost_round': 727}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:38,211] Trial 198 finished with value: 0.7454591967255053 and parameters: {'max_depth': 10, 'learning_rate': 0.13329357367798533, 'subsample': 0.8501223850382009, 'colsample_bytree': 0.8740236104848179, 'reg_alpha': 5.7390515298840386e-05, 'reg_lambda': 0.00044280522003429184, 'min_child_weight': 5, 'gamma': 0.31496702483849354, 'num_boost_round': 596}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:46,948] Trial 199 finished with value: 0.7464568943463801 and parameters: {'max_depth': 10, 'learning_rate': 0.09495590706606895, 'subsample': 0.8326064021899612, 'colsample_bytree': 0.8502957680013558, 'reg_alpha': 0.00013332360960047166, 'reg_lambda': 0.001487831524530508, 'min_child_weight': 5, 'gamma': 0.3115615526098671, 'num_boost_round': 600}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:09:56,779] Trial 200 finished with value: 0.7441800972115631 and parameters: {'max_depth': 10, 'learning_rate': 0.0719981213565799, 'subsample': 0.838782998584228, 'colsample_bytree': 0.8977647284137138, 'reg_alpha': 4.840470269344046e-05, 'reg_lambda': 0.01315413188032895, 'min_child_weight': 4, 'gamma': 0.2810547315656531, 'num_boost_round': 597}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:03,729] Trial 201 finished with value: 0.745996418521361 and parameters: {'max_depth': 10, 'learning_rate': 0.11842074011840448, 'subsample': 0.8249447528562817, 'colsample_bytree': 0.842975434188314, 'reg_alpha': 6.400977301266389e-05, 'reg_lambda': 0.00019010999910656096, 'min_child_weight': 4, 'gamma': 0.24942557648773644, 'num_boost_round': 639}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:10,764] Trial 202 finished with value: 0.7457405986185726 and parameters: {'max_depth': 10, 'learning_rate': 0.10802900355172163, 'subsample': 0.8606259511061343, 'colsample_bytree': 0.8734915016482583, 'reg_alpha': 8.554373623078028e-05, 'reg_lambda': 0.0008948316435659821, 'min_child_weight': 6, 'gamma': 0.29189695724685244, 'num_boost_round': 584}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:20,266] Trial 203 finished with value: 0.747147608083909 and parameters: {'max_depth': 10, 'learning_rate': 0.09294180828243015, 'subsample': 0.8867983339742893, 'colsample_bytree': 0.8473104908293092, 'reg_alpha': 8.28085239502714e-05, 'reg_lambda': 0.00012539261157949093, 'min_child_weight': 4, 'gamma': 0.249766926567628, 'num_boost_round': 645}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:27,107] Trial 204 finished with value: 0.7456894346380148 and parameters: {'max_depth': 10, 'learning_rate': 0.14019046451219452, 'subsample': 0.8538947262012384, 'colsample_bytree': 0.8661455194107262, 'reg_alpha': 2.592578890024321e-05, 'reg_lambda': 0.0011942914153405617, 'min_child_weight': 3, 'gamma': 0.252200144227845, 'num_boost_round': 658}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:34,646] Trial 205 finished with value: 0.7450498848810437 and parameters: {'max_depth': 10, 'learning_rate': 0.11201039524042379, 'subsample': 0.839805827723382, 'colsample_bytree': 0.8711542634260373, 'reg_alpha': 3.693224380123566e-05, 'reg_lambda': 0.0009408855691225151, 'min_child_weight': 5, 'gamma': 0.2711003128162379, 'num_boost_round': 625}. Best is trial 164 with value: 0.7473266820158608.\n",
      "[I 2024-01-15 14:10:41,044] Trial 206 finished with value: 0.7478383218214377 and parameters: {'max_depth': 10, 'learning_rate': 0.1784948845361474, 'subsample': 0.84827097216949, 'colsample_bytree': 0.8393718130015214, 'reg_alpha': 5.303063954324498e-05, 'reg_lambda': 0.0009577826629438424, 'min_child_weight': 4, 'gamma': 0.24483074531891544, 'num_boost_round': 592}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:10:48,195] Trial 207 finished with value: 0.7458940905602456 and parameters: {'max_depth': 10, 'learning_rate': 0.1265545891648695, 'subsample': 0.826064287901568, 'colsample_bytree': 0.875943640627661, 'reg_alpha': 9.522514638561593e-05, 'reg_lambda': 0.0003311335085805238, 'min_child_weight': 5, 'gamma': 0.3233025069902785, 'num_boost_round': 651}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:10:54,418] Trial 208 finished with value: 0.7438986953184958 and parameters: {'max_depth': 10, 'learning_rate': 0.1289989714042549, 'subsample': 0.8606081630485399, 'colsample_bytree': 0.8911682792805118, 'reg_alpha': 6.249290195409573e-05, 'reg_lambda': 0.001632909389281137, 'min_child_weight': 3, 'gamma': 0.32818519567548, 'num_boost_round': 654}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:02,836] Trial 209 finished with value: 0.7472755180353031 and parameters: {'max_depth': 10, 'learning_rate': 0.12739175118528645, 'subsample': 0.801460453537293, 'colsample_bytree': 0.8853942537071902, 'reg_alpha': 0.0001619273821994779, 'reg_lambda': 0.0005674249545271117, 'min_child_weight': 5, 'gamma': 0.3261337303358531, 'num_boost_round': 653}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:11,273] Trial 210 finished with value: 0.7470708621130724 and parameters: {'max_depth': 10, 'learning_rate': 0.10741770880049904, 'subsample': 0.853593916594558, 'colsample_bytree': 0.8819854032225046, 'reg_alpha': 7.724817775329122e-05, 'reg_lambda': 0.0015417968551527795, 'min_child_weight': 4, 'gamma': 0.3127004150016936, 'num_boost_round': 593}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:17,652] Trial 211 finished with value: 0.7457917625991302 and parameters: {'max_depth': 10, 'learning_rate': 0.1636892511206398, 'subsample': 0.8574198680413819, 'colsample_bytree': 0.8543755676321164, 'reg_alpha': 0.0001285139123171401, 'reg_lambda': 0.0002556585954575986, 'min_child_weight': 4, 'gamma': 0.23668569601133832, 'num_boost_round': 572}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:26,324] Trial 212 finished with value: 0.7458173445894091 and parameters: {'max_depth': 10, 'learning_rate': 0.12200250846792934, 'subsample': 0.8356203463582692, 'colsample_bytree': 0.868339188602736, 'reg_alpha': 0.0001482306444181117, 'reg_lambda': 0.0006069473542551369, 'min_child_weight': 3, 'gamma': 0.22628007147268905, 'num_boost_round': 594}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:33,736] Trial 213 finished with value: 0.7475569199283705 and parameters: {'max_depth': 10, 'learning_rate': 0.14951979919916653, 'subsample': 0.8355515369121345, 'colsample_bytree': 0.8305831419238753, 'reg_alpha': 7.529398418634e-05, 'reg_lambda': 0.00022328207214536284, 'min_child_weight': 3, 'gamma': 0.28764314410618025, 'num_boost_round': 616}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:39,794] Trial 214 finished with value: 0.742721923765669 and parameters: {'max_depth': 10, 'learning_rate': 0.11311707733827815, 'subsample': 0.8335716500282808, 'colsample_bytree': 0.8990027006696888, 'reg_alpha': 0.00011725201240780201, 'reg_lambda': 0.001529800379504191, 'min_child_weight': 4, 'gamma': 0.2748807707224983, 'num_boost_round': 655}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:46,258] Trial 215 finished with value: 0.7461243284727552 and parameters: {'max_depth': 10, 'learning_rate': 0.13839732591275752, 'subsample': 0.8337066154215498, 'colsample_bytree': 0.8648505559985262, 'reg_alpha': 0.00010382107152145569, 'reg_lambda': 0.0012298842445137418, 'min_child_weight': 5, 'gamma': 0.2983598952610001, 'num_boost_round': 650}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:11:54,038] Trial 216 finished with value: 0.7456382706574571 and parameters: {'max_depth': 10, 'learning_rate': 0.11088292220686316, 'subsample': 0.8387379948243909, 'colsample_bytree': 0.8855985621318332, 'reg_alpha': 7.061051818117117e-05, 'reg_lambda': 0.00046118803015576056, 'min_child_weight': 4, 'gamma': 0.2729950944675533, 'num_boost_round': 624}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:02,338] Trial 217 finished with value: 0.7467382962394474 and parameters: {'max_depth': 10, 'learning_rate': 0.12197732912333036, 'subsample': 0.85469995405795, 'colsample_bytree': 0.8771443781629611, 'reg_alpha': 3.262765460702562e-05, 'reg_lambda': 0.0012480919421436112, 'min_child_weight': 4, 'gamma': 0.271741590717011, 'num_boost_round': 639}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:10,763] Trial 218 finished with value: 0.7462522384241494 and parameters: {'max_depth': 10, 'learning_rate': 0.1228947550739303, 'subsample': 0.8573213727145237, 'colsample_bytree': 0.869322301048808, 'reg_alpha': 6.606292090724085e-05, 'reg_lambda': 0.00012737705475051128, 'min_child_weight': 3, 'gamma': 0.2628320387335503, 'num_boost_round': 605}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:19,636] Trial 219 finished with value: 0.7454591967255053 and parameters: {'max_depth': 10, 'learning_rate': 0.11484586948342997, 'subsample': 0.8290121012210783, 'colsample_bytree': 0.8485312161552905, 'reg_alpha': 0.0001737007899431922, 'reg_lambda': 0.001809025121749834, 'min_child_weight': 3, 'gamma': 0.24172243852865838, 'num_boost_round': 618}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:25,322] Trial 220 finished with value: 0.7411358403683806 and parameters: {'max_depth': 9, 'learning_rate': 0.10556742691418429, 'subsample': 0.8782096957859626, 'colsample_bytree': 0.8302199598139468, 'reg_alpha': 9.821969264287436e-05, 'reg_lambda': 0.0003266188861214278, 'min_child_weight': 3, 'gamma': 0.21780503627486486, 'num_boost_round': 623}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:31,494] Trial 221 finished with value: 0.7463545663852648 and parameters: {'max_depth': 10, 'learning_rate': 0.14235665082778567, 'subsample': 0.8527003975766074, 'colsample_bytree': 0.8315309311340978, 'reg_alpha': 2.339864210075227e-05, 'reg_lambda': 0.0016168540310787188, 'min_child_weight': 5, 'gamma': 0.3072938501588739, 'num_boost_round': 614}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:38,453] Trial 222 finished with value: 0.7440777692504477 and parameters: {'max_depth': 10, 'learning_rate': 0.12536453334497907, 'subsample': 0.8264558788313798, 'colsample_bytree': 0.8531794586501624, 'reg_alpha': 3.729578561943137e-05, 'reg_lambda': 0.0004162178130627352, 'min_child_weight': 2, 'gamma': 0.3117063231797651, 'num_boost_round': 598}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:45,440] Trial 223 finished with value: 0.7457661806088514 and parameters: {'max_depth': 10, 'learning_rate': 0.1311235109412183, 'subsample': 0.8814793983487635, 'colsample_bytree': 0.8501371701191556, 'reg_alpha': 7.063095060484532e-05, 'reg_lambda': 0.0005356880038169199, 'min_child_weight': 4, 'gamma': 0.26359067487072757, 'num_boost_round': 584}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:12:54,665] Trial 224 finished with value: 0.7468662061908417 and parameters: {'max_depth': 10, 'learning_rate': 0.09591191919897847, 'subsample': 0.8270880656506425, 'colsample_bytree': 0.8464719581753313, 'reg_alpha': 8.839363506923734e-05, 'reg_lambda': 0.00038051564048945753, 'min_child_weight': 4, 'gamma': 0.28015352482009476, 'num_boost_round': 608}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:13:00,345] Trial 225 finished with value: 0.7431823995906881 and parameters: {'max_depth': 10, 'learning_rate': 0.1352082311904539, 'subsample': 0.8288167260597143, 'colsample_bytree': 0.8551079147369738, 'reg_alpha': 5.859545742392479e-05, 'reg_lambda': 0.00026558896322756773, 'min_child_weight': 6, 'gamma': 0.24472090395457785, 'num_boost_round': 669}. Best is trial 206 with value: 0.7478383218214377.\n",
      "[I 2024-01-15 14:13:10,589] Trial 226 finished with value: 0.748273215656178 and parameters: {'max_depth': 10, 'learning_rate': 0.1134302900721548, 'subsample': 0.8523196347582844, 'colsample_bytree': 0.8534662673611759, 'reg_alpha': 8.318105525623995e-05, 'reg_lambda': 0.00033301172354458896, 'min_child_weight': 3, 'gamma': 0.28573247758481346, 'num_boost_round': 576}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:17,502] Trial 227 finished with value: 0.746482476336659 and parameters: {'max_depth': 10, 'learning_rate': 0.13412122076624672, 'subsample': 0.853545176762346, 'colsample_bytree': 0.851667407283863, 'reg_alpha': 6.308262024062446e-05, 'reg_lambda': 0.00020550507808639196, 'min_child_weight': 4, 'gamma': 0.2933103232617618, 'num_boost_round': 586}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:25,321] Trial 228 finished with value: 0.7454591967255053 and parameters: {'max_depth': 10, 'learning_rate': 0.11562119725747594, 'subsample': 0.8891793945953514, 'colsample_bytree': 0.8412787061823236, 'reg_alpha': 8.782527785304585e-05, 'reg_lambda': 6.339070529215313e-05, 'min_child_weight': 2, 'gamma': 0.27588971524575306, 'num_boost_round': 627}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:32,569] Trial 229 finished with value: 0.7464313123561013 and parameters: {'max_depth': 10, 'learning_rate': 0.1455914928729888, 'subsample': 0.8379530208208197, 'colsample_bytree': 0.8550908804400152, 'reg_alpha': 0.00010750465875292563, 'reg_lambda': 0.00016972670982073337, 'min_child_weight': 3, 'gamma': 0.2657104388104628, 'num_boost_round': 612}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:40,002] Trial 230 finished with value: 0.7471987720644666 and parameters: {'max_depth': 10, 'learning_rate': 0.1336985815802012, 'subsample': 0.8557241145210495, 'colsample_bytree': 0.8446414845061951, 'reg_alpha': 0.00029893268920183136, 'reg_lambda': 0.00048452362915564414, 'min_child_weight': 4, 'gamma': 0.2509944444676056, 'num_boost_round': 636}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:44,687] Trial 231 finished with value: 0.7439754412893322 and parameters: {'max_depth': 10, 'learning_rate': 0.18559833873746065, 'subsample': 0.8508522494325644, 'colsample_bytree': 0.8305837456190257, 'reg_alpha': 5.987803932647828e-05, 'reg_lambda': 0.00011038146112886659, 'min_child_weight': 3, 'gamma': 0.2477841500428316, 'num_boost_round': 569}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:50,997] Trial 232 finished with value: 0.7449731389102072 and parameters: {'max_depth': 10, 'learning_rate': 0.16330988130342847, 'subsample': 0.854211397918846, 'colsample_bytree': 0.855517174316008, 'reg_alpha': 0.00017065914338989393, 'reg_lambda': 0.00025759936954232644, 'min_child_weight': 4, 'gamma': 0.2910231076007984, 'num_boost_round': 617}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:13:56,090] Trial 233 finished with value: 0.7437963673573804 and parameters: {'max_depth': 10, 'learning_rate': 0.15264964206326587, 'subsample': 0.874443317594763, 'colsample_bytree': 0.8350738416880775, 'reg_alpha': 0.00023415412991981808, 'reg_lambda': 0.00041784948049518805, 'min_child_weight': 4, 'gamma': 0.20799820708507, 'num_boost_round': 622}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:03,581] Trial 234 finished with value: 0.7459452545408033 and parameters: {'max_depth': 10, 'learning_rate': 0.11735180699546949, 'subsample': 0.8670147859065026, 'colsample_bytree': 0.8306946248605718, 'reg_alpha': 0.00010949149180433929, 'reg_lambda': 0.00010609601365373294, 'min_child_weight': 4, 'gamma': 0.26275296468251014, 'num_boost_round': 603}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:09,487] Trial 235 finished with value: 0.7434893834740343 and parameters: {'max_depth': 10, 'learning_rate': 0.13457085664849702, 'subsample': 0.8291784482398572, 'colsample_bytree': 0.8548333746399153, 'reg_alpha': 7.350315979874039e-05, 'reg_lambda': 9.136728692928661e-05, 'min_child_weight': 4, 'gamma': 0.3034457161316619, 'num_boost_round': 597}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:17,951] Trial 236 finished with value: 0.7468406242005627 and parameters: {'max_depth': 10, 'learning_rate': 0.10423697028598664, 'subsample': 0.865433592033244, 'colsample_bytree': 0.8282333437849665, 'reg_alpha': 0.00034845851592702615, 'reg_lambda': 0.0003363497430888997, 'min_child_weight': 3, 'gamma': 0.2486597399090191, 'num_boost_round': 599}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:25,648] Trial 237 finished with value: 0.7431056536198517 and parameters: {'max_depth': 10, 'learning_rate': 0.08557366610379044, 'subsample': 0.8568696152222013, 'colsample_bytree': 0.8560550959584289, 'reg_alpha': 0.00018135966323110903, 'reg_lambda': 0.0005315037336347161, 'min_child_weight': 4, 'gamma': 0.2742092579497701, 'num_boost_round': 607}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:31,823] Trial 238 finished with value: 0.7416986441545153 and parameters: {'max_depth': 10, 'learning_rate': 0.09473438707027074, 'subsample': 0.8583895414396743, 'colsample_bytree': 0.8710506696499081, 'reg_alpha': 0.0001259098623188731, 'reg_lambda': 0.0005555377549065883, 'min_child_weight': 3, 'gamma': 0.247386656466768, 'num_boost_round': 629}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:39,263] Trial 239 finished with value: 0.7457150166282938 and parameters: {'max_depth': 10, 'learning_rate': 0.1331487765329955, 'subsample': 0.8604753963926184, 'colsample_bytree': 0.8191991726683053, 'reg_alpha': 0.00010519115482817838, 'reg_lambda': 0.0003903091288236898, 'min_child_weight': 2, 'gamma': 0.23644563401042326, 'num_boost_round': 626}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:45,450] Trial 240 finished with value: 0.7465080583269379 and parameters: {'max_depth': 10, 'learning_rate': 0.16100338922687094, 'subsample': 0.8419660774450239, 'colsample_bytree': 0.8573620130916919, 'reg_alpha': 0.0003821923258316627, 'reg_lambda': 0.0015141419326329805, 'min_child_weight': 5, 'gamma': 0.2438024154984689, 'num_boost_round': 641}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:52,969] Trial 241 finished with value: 0.7464057303658225 and parameters: {'max_depth': 10, 'learning_rate': 0.1266048491854092, 'subsample': 0.8594726929716503, 'colsample_bytree': 0.8193724657042548, 'reg_alpha': 0.00019754970061720318, 'reg_lambda': 0.00018359869947617833, 'min_child_weight': 4, 'gamma': 0.24604434289020516, 'num_boost_round': 591}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:14:58,611] Trial 242 finished with value: 0.7449475569199284 and parameters: {'max_depth': 10, 'learning_rate': 0.1558100616878555, 'subsample': 0.8786927182320181, 'colsample_bytree': 0.8323500083970208, 'reg_alpha': 0.0003252005639652383, 'reg_lambda': 0.0005602507318194732, 'min_child_weight': 4, 'gamma': 0.2266208146528673, 'num_boost_round': 617}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:06,494] Trial 243 finished with value: 0.7468406242005627 and parameters: {'max_depth': 10, 'learning_rate': 0.13524138967749122, 'subsample': 0.8506248733394866, 'colsample_bytree': 0.8327796064662155, 'reg_alpha': 0.0005884859429246192, 'reg_lambda': 0.0002088475739274754, 'min_child_weight': 3, 'gamma': 0.2388586513884885, 'num_boost_round': 655}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:13,801] Trial 244 finished with value: 0.7455615246866206 and parameters: {'max_depth': 10, 'learning_rate': 0.13507481416055978, 'subsample': 0.852918369429695, 'colsample_bytree': 0.8014328992893882, 'reg_alpha': 5.220427660183394e-05, 'reg_lambda': 0.00021537138718120697, 'min_child_weight': 3, 'gamma': 0.22840423465906584, 'num_boost_round': 638}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:24,407] Trial 245 finished with value: 0.7461754924533128 and parameters: {'max_depth': 10, 'learning_rate': 0.08248741613273274, 'subsample': 0.8508635039683288, 'colsample_bytree': 0.8520395175365244, 'reg_alpha': 0.0002793913490619919, 'reg_lambda': 8.53793379855574e-05, 'min_child_weight': 3, 'gamma': 0.27359300134235537, 'num_boost_round': 585}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:32,294] Trial 246 finished with value: 0.7470708621130724 and parameters: {'max_depth': 10, 'learning_rate': 0.1399613486606047, 'subsample': 0.8868778058664035, 'colsample_bytree': 0.8164213289313613, 'reg_alpha': 0.00016742031784619022, 'reg_lambda': 0.00029965603945475964, 'min_child_weight': 2, 'gamma': 0.25936040676694044, 'num_boost_round': 639}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:38,982] Trial 247 finished with value: 0.745177794832438 and parameters: {'max_depth': 10, 'learning_rate': 0.12440036704636763, 'subsample': 0.8598591576461037, 'colsample_bytree': 0.8715420337451198, 'reg_alpha': 0.00017353598265736016, 'reg_lambda': 0.0005145312477530975, 'min_child_weight': 4, 'gamma': 0.26372167988887546, 'num_boost_round': 624}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:48,403] Trial 248 finished with value: 0.7467382962394474 and parameters: {'max_depth': 10, 'learning_rate': 0.10584173000050617, 'subsample': 0.8292932167594945, 'colsample_bytree': 0.8518075992806475, 'reg_alpha': 0.00036560612524622944, 'reg_lambda': 0.0006559889298871003, 'min_child_weight': 1, 'gamma': 0.24171477434242705, 'num_boost_round': 608}. Best is trial 226 with value: 0.748273215656178.\n",
      "[I 2024-01-15 14:15:57,239] Trial 249 finished with value: 0.7482987976464569 and parameters: {'max_depth': 10, 'learning_rate': 0.124668346163416, 'subsample': 0.852784934119911, 'colsample_bytree': 0.8189124554933165, 'reg_alpha': 0.00016546733732631587, 'reg_lambda': 0.0003901929562223549, 'min_child_weight': 6, 'gamma': 0.2116895674539605, 'num_boost_round': 623}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:03,865] Trial 250 finished with value: 0.7450243028907648 and parameters: {'max_depth': 10, 'learning_rate': 0.13138079331050806, 'subsample': 0.8813780543558744, 'colsample_bytree': 0.8146511936772699, 'reg_alpha': 0.0002465917008769359, 'reg_lambda': 0.0005264647842039599, 'min_child_weight': 5, 'gamma': 0.19241437459865024, 'num_boost_round': 604}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:09,729] Trial 251 finished with value: 0.745152212842159 and parameters: {'max_depth': 10, 'learning_rate': 0.13489964365609097, 'subsample': 0.8721087778180246, 'colsample_bytree': 0.8534815769782188, 'reg_alpha': 2.7323414434533745e-05, 'reg_lambda': 0.000248457984392597, 'min_child_weight': 7, 'gamma': 0.29580390051472805, 'num_boost_round': 621}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:15,044] Trial 252 finished with value: 0.7454080327449475 and parameters: {'max_depth': 10, 'learning_rate': 0.14272689500933336, 'subsample': 0.8318112453740244, 'colsample_bytree': 0.8546018009674007, 'reg_alpha': 0.00011881980302584949, 'reg_lambda': 0.00027158595606309823, 'min_child_weight': 7, 'gamma': 0.24629782336699507, 'num_boost_round': 661}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:20,937] Trial 253 finished with value: 0.7456638526477359 and parameters: {'max_depth': 10, 'learning_rate': 0.184573780186566, 'subsample': 0.8442947573011769, 'colsample_bytree': 0.8372262035956983, 'reg_alpha': 0.00013727193334508534, 'reg_lambda': 0.0003032724528232147, 'min_child_weight': 3, 'gamma': 0.19554534895673475, 'num_boost_round': 663}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:27,357] Trial 254 finished with value: 0.7443335891532361 and parameters: {'max_depth': 10, 'learning_rate': 0.12036334650525249, 'subsample': 0.857313180214431, 'colsample_bytree': 0.840997344470007, 'reg_alpha': 0.00011343549581353644, 'reg_lambda': 0.0004585285779420736, 'min_child_weight': 4, 'gamma': 0.2530962246782711, 'num_boost_round': 650}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:34,147] Trial 255 finished with value: 0.7462010744435917 and parameters: {'max_depth': 10, 'learning_rate': 0.14287342260489885, 'subsample': 0.8531160841043738, 'colsample_bytree': 0.8444592867918844, 'reg_alpha': 0.0002768824764909106, 'reg_lambda': 0.0004065069391580843, 'min_child_weight': 5, 'gamma': 0.2249180783365534, 'num_boost_round': 632}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:41,460] Trial 256 finished with value: 0.7436172934254285 and parameters: {'max_depth': 10, 'learning_rate': 0.10562386763225542, 'subsample': 0.8461391143551074, 'colsample_bytree': 0.8229803511371654, 'reg_alpha': 0.0003030195164694493, 'reg_lambda': 0.0005136806434819732, 'min_child_weight': 3, 'gamma': 0.2227226813289172, 'num_boost_round': 649}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:48,516] Trial 257 finished with value: 0.7455103607060629 and parameters: {'max_depth': 10, 'learning_rate': 0.11951426807298518, 'subsample': 0.8552122765625678, 'colsample_bytree': 0.8484042266537554, 'reg_alpha': 0.0004353653683912309, 'reg_lambda': 0.0006556567730097924, 'min_child_weight': 4, 'gamma': 0.27292852591546424, 'num_boost_round': 638}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:52,852] Trial 258 finished with value: 0.7428242517267843 and parameters: {'max_depth': 10, 'learning_rate': 0.1853085777332337, 'subsample': 0.8460683285707379, 'colsample_bytree': 0.833478243813785, 'reg_alpha': 9.924631092579441e-05, 'reg_lambda': 0.0009569596517933007, 'min_child_weight': 6, 'gamma': 0.221469217786758, 'num_boost_round': 622}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:16:59,357] Trial 259 finished with value: 0.7455871066768995 and parameters: {'max_depth': 10, 'learning_rate': 0.17794205099223628, 'subsample': 0.8551626675640461, 'colsample_bytree': 0.8133759371781955, 'reg_alpha': 0.00042597432789561124, 'reg_lambda': 0.0006249540469808556, 'min_child_weight': 4, 'gamma': 0.2598734371904558, 'num_boost_round': 648}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:05,783] Trial 260 finished with value: 0.7444103351240727 and parameters: {'max_depth': 10, 'learning_rate': 0.12073098641745583, 'subsample': 0.824746015232569, 'colsample_bytree': 0.8413542497957186, 'reg_alpha': 0.0004978348608790702, 'reg_lambda': 0.0007619585610845813, 'min_child_weight': 5, 'gamma': 0.24305517618707403, 'num_boost_round': 640}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:12,679] Trial 261 finished with value: 0.7464568943463801 and parameters: {'max_depth': 10, 'learning_rate': 0.15681051752792569, 'subsample': 0.8490789397822772, 'colsample_bytree': 0.8266480922168572, 'reg_alpha': 0.0006602803931664523, 'reg_lambda': 0.0002859566067025091, 'min_child_weight': 4, 'gamma': 0.19983827610061589, 'num_boost_round': 618}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:18,425] Trial 262 finished with value: 0.7450498848810437 and parameters: {'max_depth': 10, 'learning_rate': 0.15472063555065924, 'subsample': 0.8536829272133467, 'colsample_bytree': 0.8662574264841303, 'reg_alpha': 0.0002832853676795547, 'reg_lambda': 0.0006098754260290362, 'min_child_weight': 6, 'gamma': 0.2464395763996258, 'num_boost_round': 591}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:25,292] Trial 263 finished with value: 0.745177794832438 and parameters: {'max_depth': 10, 'learning_rate': 0.11115277407897238, 'subsample': 0.8500471372094625, 'colsample_bytree': 0.8456792121418987, 'reg_alpha': 0.0002943607617524283, 'reg_lambda': 0.0002600040372804855, 'min_child_weight': 5, 'gamma': 0.22601909845366555, 'num_boost_round': 593}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:32,731] Trial 264 finished with value: 0.7445894090560246 and parameters: {'max_depth': 10, 'learning_rate': 0.09465781599769274, 'subsample': 0.8632955486546692, 'colsample_bytree': 0.8489586724925728, 'reg_alpha': 0.0003030963706494279, 'reg_lambda': 0.0008767338977672002, 'min_child_weight': 5, 'gamma': 0.2333735119438345, 'num_boost_round': 632}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:42,061] Trial 265 finished with value: 0.747966231772832 and parameters: {'max_depth': 10, 'learning_rate': 0.11523048261775172, 'subsample': 0.8602419933907007, 'colsample_bytree': 0.8309181959227434, 'reg_alpha': 0.0004300511081967723, 'reg_lambda': 0.0010130967084978945, 'min_child_weight': 5, 'gamma': 0.22879849720894738, 'num_boost_round': 641}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:48,540] Trial 266 finished with value: 0.7456382706574571 and parameters: {'max_depth': 10, 'learning_rate': 0.128900188855792, 'subsample': 0.8768949679352576, 'colsample_bytree': 0.8314766863816117, 'reg_alpha': 0.00039093462308022637, 'reg_lambda': 0.0007008584036141256, 'min_child_weight': 6, 'gamma': 0.20600079316486958, 'num_boost_round': 698}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:54,033] Trial 267 finished with value: 0.7456894346380148 and parameters: {'max_depth': 10, 'learning_rate': 0.148616765088463, 'subsample': 0.8601879134176426, 'colsample_bytree': 0.8232000013455829, 'reg_alpha': 0.00019498261200522784, 'reg_lambda': 0.0007147540588426085, 'min_child_weight': 4, 'gamma': 0.25833387981198597, 'num_boost_round': 684}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:17:57,878] Trial 268 finished with value: 0.7403683806600154 and parameters: {'max_depth': 10, 'learning_rate': 0.17902273394359144, 'subsample': 0.8313019962653135, 'colsample_bytree': 0.8653367707740391, 'reg_alpha': 0.00013914556804042839, 'reg_lambda': 0.00030776926836584414, 'min_child_weight': 4, 'gamma': 0.26008185746228957, 'num_boost_round': 659}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:04,539] Trial 269 finished with value: 0.7464568943463801 and parameters: {'max_depth': 10, 'learning_rate': 0.16313505645647391, 'subsample': 0.8622684360631226, 'colsample_bytree': 0.8251930117209348, 'reg_alpha': 0.00017580272145541827, 'reg_lambda': 0.0006392788116151188, 'min_child_weight': 4, 'gamma': 0.21372905144088455, 'num_boost_round': 613}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:09,344] Trial 270 finished with value: 0.744845228958813 and parameters: {'max_depth': 10, 'learning_rate': 0.19455757510176316, 'subsample': 0.8552797816140744, 'colsample_bytree': 0.812559577029433, 'reg_alpha': 0.0003124292617788829, 'reg_lambda': 0.001068996389046903, 'min_child_weight': 6, 'gamma': 0.2627809723062127, 'num_boost_round': 660}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:14,571] Trial 271 finished with value: 0.7447173190074188 and parameters: {'max_depth': 10, 'learning_rate': 0.170943900102414, 'subsample': 0.8693016008845174, 'colsample_bytree': 0.8205440704859883, 'reg_alpha': 0.00011483638266235814, 'reg_lambda': 0.00048378288158935115, 'min_child_weight': 7, 'gamma': 0.22682142385128773, 'num_boost_round': 626}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:19,341] Trial 272 finished with value: 0.7423126119212075 and parameters: {'max_depth': 10, 'learning_rate': 0.14967425907458706, 'subsample': 0.8515777498602795, 'colsample_bytree': 0.843780984408893, 'reg_alpha': 0.00019108198442921993, 'reg_lambda': 0.0003975700591107423, 'min_child_weight': 6, 'gamma': 0.22859179391505144, 'num_boost_round': 644}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:26,066] Trial 273 finished with value: 0.7457150166282938 and parameters: {'max_depth': 10, 'learning_rate': 0.13551878160819536, 'subsample': 0.8455958191804266, 'colsample_bytree': 0.8495105530191913, 'reg_alpha': 0.001197395646805157, 'reg_lambda': 0.0009699717930657153, 'min_child_weight': 4, 'gamma': 0.23722592344791768, 'num_boost_round': 589}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:31,493] Trial 274 finished with value: 0.7430544896392939 and parameters: {'max_depth': 10, 'learning_rate': 0.136181962943561, 'subsample': 0.8510585950761336, 'colsample_bytree': 0.7978914165489, 'reg_alpha': 0.00027697433024736936, 'reg_lambda': 0.00101067957700268, 'min_child_weight': 6, 'gamma': 0.21063171097976724, 'num_boost_round': 640}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:40,033] Trial 275 finished with value: 0.7472755180353031 and parameters: {'max_depth': 10, 'learning_rate': 0.11160007976280331, 'subsample': 0.8445518502628839, 'colsample_bytree': 0.8159151759967332, 'reg_alpha': 0.00015062330243118915, 'reg_lambda': 0.0008505655942141448, 'min_child_weight': 5, 'gamma': 0.19138494120405128, 'num_boost_round': 652}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:45,887] Trial 276 finished with value: 0.7453312867741111 and parameters: {'max_depth': 10, 'learning_rate': 0.13115218307999288, 'subsample': 0.8704390508560376, 'colsample_bytree': 0.8294772875153825, 'reg_alpha': 0.001013857322234439, 'reg_lambda': 0.00026379887814975944, 'min_child_weight': 4, 'gamma': 0.21374628702979, 'num_boost_round': 635}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:52,509] Trial 277 finished with value: 0.7464313123561013 and parameters: {'max_depth': 10, 'learning_rate': 0.13746141180065238, 'subsample': 0.8590002828181447, 'colsample_bytree': 0.8436783419160993, 'reg_alpha': 0.0004515357211194374, 'reg_lambda': 0.0006061696221962273, 'min_child_weight': 5, 'gamma': 0.2065608392821817, 'num_boost_round': 595}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:18:57,896] Trial 278 finished with value: 0.7441289332310054 and parameters: {'max_depth': 10, 'learning_rate': 0.1342529746233914, 'subsample': 0.8512102376019953, 'colsample_bytree': 0.838937146468665, 'reg_alpha': 0.0003079374128847748, 'reg_lambda': 0.00122000221921324, 'min_child_weight': 6, 'gamma': 0.18189342982468387, 'num_boost_round': 647}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:03,977] Trial 279 finished with value: 0.7456126886671783 and parameters: {'max_depth': 10, 'learning_rate': 0.14613873961723958, 'subsample': 0.8521719324122252, 'colsample_bytree': 0.7911340345115425, 'reg_alpha': 0.00010606464020771817, 'reg_lambda': 0.00018603759789313353, 'min_child_weight': 4, 'gamma': 0.19638574644414702, 'num_boost_round': 640}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:09,063] Trial 280 finished with value: 0.7448196469685342 and parameters: {'max_depth': 10, 'learning_rate': 0.1716463544094706, 'subsample': 0.8646088707775477, 'colsample_bytree': 0.8483172397464886, 'reg_alpha': 0.0007000860719297449, 'reg_lambda': 0.00024388990168322693, 'min_child_weight': 3, 'gamma': 0.2146622725832201, 'num_boost_round': 623}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:17,767] Trial 281 finished with value: 0.7475313379380916 and parameters: {'max_depth': 10, 'learning_rate': 0.14438940412384943, 'subsample': 0.8666544831267635, 'colsample_bytree': 0.8087620209701234, 'reg_alpha': 0.000359012245066331, 'reg_lambda': 0.00043192225615385784, 'min_child_weight': 3, 'gamma': 0.22107750013833685, 'num_boost_round': 612}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:23,651] Trial 282 finished with value: 0.7468150422102839 and parameters: {'max_depth': 10, 'learning_rate': 0.15778803337981875, 'subsample': 0.8461825561888323, 'colsample_bytree': 0.8305983530873595, 'reg_alpha': 0.0004662676217368829, 'reg_lambda': 0.0009248029834977207, 'min_child_weight': 6, 'gamma': 0.1788927086334478, 'num_boost_round': 691}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:30,232] Trial 283 finished with value: 0.7454847787157841 and parameters: {'max_depth': 10, 'learning_rate': 0.13218272471117187, 'subsample': 0.8368058935101271, 'colsample_bytree': 0.8549322619154592, 'reg_alpha': 0.0006932386470457578, 'reg_lambda': 0.0010600266429904367, 'min_child_weight': 5, 'gamma': 0.21870844074796894, 'num_boost_round': 647}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:37,425] Trial 284 finished with value: 0.7424661038628806 and parameters: {'max_depth': 10, 'learning_rate': 0.08436069337010299, 'subsample': 0.8626517465297957, 'colsample_bytree': 0.8338509277864964, 'reg_alpha': 0.0005753951684796398, 'reg_lambda': 0.0005954759921771328, 'min_child_weight': 6, 'gamma': 0.24336449592604972, 'num_boost_round': 546}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:42,807] Trial 285 finished with value: 0.7453312867741111 and parameters: {'max_depth': 10, 'learning_rate': 0.1954244574870346, 'subsample': 0.8281840856716713, 'colsample_bytree': 0.857888135692577, 'reg_alpha': 0.00023004359196005487, 'reg_lambda': 0.0004773694111010171, 'min_child_weight': 4, 'gamma': 0.2067589197249951, 'num_boost_round': 668}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:48,713] Trial 286 finished with value: 0.744845228958813 and parameters: {'max_depth': 10, 'learning_rate': 0.13644802981210488, 'subsample': 0.865632401773986, 'colsample_bytree': 0.8099724990036913, 'reg_alpha': 0.0003357968633243672, 'reg_lambda': 0.0005297441252344459, 'min_child_weight': 5, 'gamma': 0.20523630508871582, 'num_boost_round': 617}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:19:57,036] Trial 287 finished with value: 0.7469429521616782 and parameters: {'max_depth': 10, 'learning_rate': 0.1115652257746142, 'subsample': 0.852594776585376, 'colsample_bytree': 0.8341908725260276, 'reg_alpha': 0.00040588127459733215, 'reg_lambda': 0.00040965059934543034, 'min_child_weight': 5, 'gamma': 0.1964078428784808, 'num_boost_round': 567}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:02,954] Trial 288 finished with value: 0.7470964441033512 and parameters: {'max_depth': 10, 'learning_rate': 0.14541057978857624, 'subsample': 0.8817497589681803, 'colsample_bytree': 0.8259262744149776, 'reg_alpha': 0.0003135865750096503, 'reg_lambda': 0.0008639005943652659, 'min_child_weight': 6, 'gamma': 0.21151646073175362, 'num_boost_round': 608}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:09,183] Trial 289 finished with value: 0.744538245075467 and parameters: {'max_depth': 10, 'learning_rate': 0.12686795209470986, 'subsample': 0.8587423706467405, 'colsample_bytree': 0.8355620535532845, 'reg_alpha': 0.00021240230499810226, 'reg_lambda': 0.0001622857854630684, 'min_child_weight': 5, 'gamma': 0.2277365300334573, 'num_boost_round': 606}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:15,603] Trial 290 finished with value: 0.7412637503197749 and parameters: {'max_depth': 10, 'learning_rate': 0.09773747437392409, 'subsample': 0.8452537258607143, 'colsample_bytree': 0.8203891855418094, 'reg_alpha': 0.00039299659142352756, 'reg_lambda': 0.002343386201453177, 'min_child_weight': 5, 'gamma': 0.22323835369025938, 'num_boost_round': 631}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:24,323] Trial 291 finished with value: 0.7468406242005627 and parameters: {'max_depth': 10, 'learning_rate': 0.09592158830812984, 'subsample': 0.8681887981344986, 'colsample_bytree': 0.826225673375778, 'reg_alpha': 0.0003498957144226944, 'reg_lambda': 0.0015288863075490593, 'min_child_weight': 5, 'gamma': 0.1865609613179054, 'num_boost_round': 594}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:30,400] Trial 292 finished with value: 0.7448196469685342 and parameters: {'max_depth': 10, 'learning_rate': 0.1651999194220285, 'subsample': 0.8597815992960911, 'colsample_bytree': 0.8020411391873149, 'reg_alpha': 0.00011199531863700487, 'reg_lambda': 0.0002844429205752308, 'min_child_weight': 5, 'gamma': 0.18916596659788615, 'num_boost_round': 607}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:37,572] Trial 293 finished with value: 0.7463801483755437 and parameters: {'max_depth': 10, 'learning_rate': 0.14177609019004234, 'subsample': 0.873232138730872, 'colsample_bytree': 0.8206004759634169, 'reg_alpha': 0.0006940815457714414, 'reg_lambda': 0.00036419302611258845, 'min_child_weight': 4, 'gamma': 0.23169760224099392, 'num_boost_round': 597}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:43,106] Trial 294 finished with value: 0.7444870810949091 and parameters: {'max_depth': 10, 'learning_rate': 0.13548571595162728, 'subsample': 0.8607276307803786, 'colsample_bytree': 0.8069636547327416, 'reg_alpha': 0.000319524714701589, 'reg_lambda': 0.00024518348863992914, 'min_child_weight': 5, 'gamma': 0.22409216472356103, 'num_boost_round': 591}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:49,288] Trial 295 finished with value: 0.7447173190074188 and parameters: {'max_depth': 10, 'learning_rate': 0.12444479088445182, 'subsample': 0.871696903989746, 'colsample_bytree': 0.8358359633975156, 'reg_alpha': 0.00022066186275848733, 'reg_lambda': 0.0016588992227808098, 'min_child_weight': 5, 'gamma': 0.22481682621937657, 'num_boost_round': 645}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:20:58,115] Trial 296 finished with value: 0.7464313123561013 and parameters: {'max_depth': 10, 'learning_rate': 0.10254499702751689, 'subsample': 0.8458656851190508, 'colsample_bytree': 0.8351319114932891, 'reg_alpha': 0.00015335367132211428, 'reg_lambda': 0.0009814039010213095, 'min_child_weight': 6, 'gamma': 0.25097379732098035, 'num_boost_round': 635}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:21:03,995] Trial 297 finished with value: 0.7434638014837555 and parameters: {'max_depth': 10, 'learning_rate': 0.11954830068515672, 'subsample': 0.8847010356117949, 'colsample_bytree': 0.8201823300896558, 'reg_alpha': 0.0002977661924195985, 'reg_lambda': 0.0005624315959433976, 'min_child_weight': 5, 'gamma': 0.21287491557344762, 'num_boost_round': 574}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:21:09,505] Trial 298 finished with value: 0.7444359171143515 and parameters: {'max_depth': 10, 'learning_rate': 0.13074482613750604, 'subsample': 0.8556892856344835, 'colsample_bytree': 0.8252112627470007, 'reg_alpha': 0.0003077704144380935, 'reg_lambda': 0.00018939632131611736, 'min_child_weight': 5, 'gamma': 0.1528769501120578, 'num_boost_round': 595}. Best is trial 249 with value: 0.7482987976464569.\n",
      "[I 2024-01-15 14:21:16,797] Trial 299 finished with value: 0.7455359426963417 and parameters: {'max_depth': 10, 'learning_rate': 0.11033336398548978, 'subsample': 0.8649323540003293, 'colsample_bytree': 0.8450563547605424, 'reg_alpha': 0.001398111140222866, 'reg_lambda': 0.0022807697870628257, 'min_child_weight': 7, 'gamma': 0.21456559626242847, 'num_boost_round': 633}. Best is trial 249 with value: 0.7482987976464569.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: score 0.7482987976464569, params {'max_depth': 10, 'learning_rate': 0.124668346163416, 'subsample': 0.852784934119911, 'colsample_bytree': 0.8189124554933165, 'reg_alpha': 0.00016546733732631587, 'reg_lambda': 0.0003901929562223549, 'min_child_weight': 6, 'gamma': 0.2116895674539605, 'num_boost_round': 623}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from optuna.samplers import CmaEsSampler\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "\n",
    "train_X1_full = pd.concat([train_X1, test_X1])\n",
    "train_Y1_temp_full = pd.concat([train_Y1_temp, test_Y1_temp])\n",
    "\n",
    "def objective(trial):\n",
    "    # params = {\n",
    "       \n",
    "    #     'max_depth': trial.suggest_int('max_depth', 9, 20),\n",
    "    #     'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "    #     'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "    #     'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1),\n",
    "    #     'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "    #     'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "    #     'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1),\n",
    "    #     'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1),\n",
    "    # }\n",
    "    \n",
    "    params = {\n",
    "        'tree_method': 'hist',  # Use histogram-based algorithm\n",
    "        'device': 'cuda',  # Use cuda for GPU acceleration\n",
    "        #'predictor': 'gpu_predictor',\n",
    "        #'gpu_id' : 0,\n",
    "        #'predictor': 'cpu_predictor', # This is to make the GPU acceleration more smooth. Making the CPU as the predictor will lighten the load for the GPU\n",
    "\n",
    "        #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "        'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "        'num_class': 3, # Default is 1 (for binary classification)\n",
    "        'random_state': 42,  # Default is 0\n",
    "        'eval_metric': 'merror',\n",
    "        #'eval_metric': 'mlogloss',\n",
    "      \n",
    "      \n",
    "        #'n_estimators': trial.suggest_int('n_estimators', 400, 850), # Default is 100\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10), # Default is 6\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True), # Default is 0.3\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0), # Default is 1\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0), # Default is 1\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-6, 1, log=True), # Default is 0\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-6, 2, log=True),  # Default is 1\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), # Default is 1\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "\n",
    "        \n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 400, 850) # This adds num_boost_round into hyper parameter tuning\n",
    "\n",
    "    dtrain = xgb.DMatrix(train_X1_full, label=train_Y1_temp_full) # This is change the pandas data into something our GPU can handle better (DMatrix)\n",
    "    dvalid = xgb.DMatrix(valid_X1, label=valid_Y1_temp)\n",
    "\n",
    "    #pruning_callback = XGBoostPruningCallback(trial, \"validation-merror\")  # This is what makes this code \"Optuna's integration with XGBoost\"\n",
    "  \n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'validation')]  # Specify what data to evaluate and when\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=10,\n",
    "        #callbacks=[pruning_callback],\n",
    "        verbose_eval=False  # verbose\n",
    "    )\n",
    "\n",
    "    # Use validation data for evaluation\n",
    "    preds = model.predict(dvalid)\n",
    "    error = (preds != valid_Y1_temp).sum() / len(valid_Y1_temp)\n",
    "\n",
    "    return 1 - error\n",
    "\n",
    "#pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=10)\n",
    "pruner = HyperbandPruner(min_resource=10) # Pruning won't be considered until after 10 iterations for each trial\n",
    "\n",
    "sampler = CmaEsSampler() # Think about using CmaEsSampler, especially when you use ALL the features, since it's good at handling that. Also good when we are trying to tune 10+ parameters at the same time\n",
    "#sampler = TPESampler()\n",
    "\n",
    "study = optuna.create_study(direction='maximize', \n",
    "                            #pruner=pruner, \n",
    "                            sampler=sampler)\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "print(f'Best trial: trial number {study.best_trial.number}, score {study.best_trial.value}, params {study.best_trial.params}')\n",
    "\n",
    "# Took 11 minutes for 300 trials. XGBoostPruningCallBack and CmaEsSampler\n",
    "\n",
    "# Took 37 min for 300 trials. Hyperband pruning and TPESampler\n",
    "\n",
    "# Took 37 min for 300 trials. Hyperbad pruning and CmaEsSampler\n",
    "\n",
    "# All logs can be found in the \"Optuna_Logs_XGBoost_Model19.txt\" file. The 11 minute one is on the bottom of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from optuna.integration import XGBoostPruningCallback\n",
    "# from optuna.samplers import CmaEsSampler\n",
    "# from optuna.samplers import TPESampler\n",
    "# from optuna.pruners import HyperbandPruner\n",
    "# from optuna.pruners import MedianPruner\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation and testt sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# def objective(trial):\n",
    "#     # params = {\n",
    "       \n",
    "#     #     'max_depth': trial.suggest_int('max_depth', 9, 20),\n",
    "#     #     'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "#     #     'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "#     #     'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1),\n",
    "#     #     'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "#     #     'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "#     #     'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1),\n",
    "#     #     'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1),\n",
    "#     # }\n",
    "    \n",
    "#     params = {\n",
    "#         'tree_method': 'hist',  # Use histogram-based algorithm\n",
    "#         'device': 'cuda',  # Use cuda for GPU acceleration\n",
    "#         #'predictor': 'gpu_predictor',\n",
    "#         #'gpu_id' : 0,\n",
    "#         #'predictor': 'cpu_predictor', # This is to make the GPU acceleration more smooth. Making the CPU as the predictor will lighten the load for the GPU\n",
    "\n",
    "#         #These below are the hyperparameters from Model 5, which has the highest score (74.75) so far\n",
    "#         'objective': 'multi:softmax',  # Default is 'binary:logistic'\n",
    "#         'num_class': 3, # Default is 1 (for binary classification)\n",
    "#         'random_state': 42,  # Default is 0\n",
    "#         'eval_metric': 'merror',\n",
    "#         #'eval_metric': 'mlogloss',\n",
    "      \n",
    "      \n",
    "#         #'n_estimators': trial.suggest_int('n_estimators', 400, 850), # Default is 100\n",
    "#         'max_depth': trial.suggest_int('max_depth', 5, 10), # Default is 6\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True), # Default is 0.3\n",
    "#         'subsample': trial.suggest_float('subsample', 0.7, 1.0), # Default is 1\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0), # Default is 1\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 1e-6, 1, log=True), # Default is 0\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-6, 2, log=True),  # Default is 1\n",
    "#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), # Default is 1\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 0.5), # Default is 0 \n",
    "\n",
    "        \n",
    "#     }\n",
    "\n",
    "#     num_boost_round = trial.suggest_int('num_boost_round', 400, 850) # This adds num_boost_round into hyper parameter tuning\n",
    "\n",
    "#     dtrain = xgb.DMatrix(train_X1_full, label=train_Y1_temp_full) # This is change the pandas data into something our GPU can handle better (DMatrix)\n",
    "\n",
    "#     pruning_callback = XGBoostPruningCallback(trial, \"test-merror-mean\") # This is what makes this code \"Optuna's integration with XGBoost\"\n",
    "  \n",
    "\n",
    "#     cv_results = xgb.cv(\n",
    "#         params,\n",
    "#         dtrain,\n",
    "#         num_boost_round = num_boost_round,\n",
    "#         nfold=3,\n",
    "#         metrics={'merror'},\n",
    "#         early_stopping_rounds=10,\n",
    "#         callbacks=[pruning_callback]\n",
    "#     )\n",
    "\n",
    "#     return 1 - cv_results['test-merror-mean'].min()\n",
    "\n",
    "# #pruner = MedianPruner()\n",
    "# pruner = HyperbandPruner()\n",
    "\n",
    "# sampler = CmaEsSampler() # Think about using CmaEsSampler, especially when you use ALL the features, since it's good at handling that. Also good when we are trying to tune 10+ parameters at the same time\n",
    "# #sampler = TPESampler()\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', pruner=pruner, sampler=sampler)\n",
    "# study.optimize(objective, n_trials=200)\n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:21:28] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Submitted!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [I 2024-01-14 22:04:05,626] Trial 1 finished with value: 0.737477615758506 and parameters: {'max_depth': 8, 'learning_rate': 0.0493605952014681, 'subsample': 0.8050435191563892, 'colsample_bytree': 0.854112103631072, 'reg_alpha': 0.00020118715081702792, 'reg_lambda': 0.006404389225491458, 'min_child_weight': 14, 'gamma': 0.29607599793557965, 'num_boost_round': 549}. Best is trial 1 with value: 0.737477615758506.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "params = {\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'merror',\n",
    "    'random_state': 42,\n",
    "\n",
    "\n",
    "\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.0493605952014681,\n",
    "    'subsample': 0.8050435191563892,\n",
    "    'colsample_bytree': 0.854112103631072,\n",
    "    'reg_alpha': 0.00020118715081702792,\n",
    "    'reg_lambda': 0.006404389225491458,\n",
    "    'min_child_weight': 14,\n",
    "    'gamma': 0.29607599793557965,\n",
    "    \n",
    "}\n",
    "\n",
    "# Number of boosting rounds\n",
    "num_boost_round = 549\n",
    "\n",
    "\n",
    "# Concatenate the datasets\n",
    "full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "dfull = xgb.DMatrix(full_X, label=full_Y_temp)\n",
    "\n",
    "model19 = xgb.train(params, dfull, num_boost_round)\n",
    "\n",
    "\n",
    "# Save the model to a file\n",
    "model19.save_model('saved_XGBoost_model19.model')\n",
    "\n",
    "\n",
    "# # This is how to load the model from joblib\n",
    "# from joblib import load\n",
    "# model19 = load('model19.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# Separate building_ids and features in the test data\n",
    "competition_test_building_ids = test_data1['building_id']\n",
    "competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "dcompetition_test_X = xgb.DMatrix(competition_test_X)\n",
    "\n",
    "# Predict on the competition test data\n",
    "competition_y_pred = model19.predict(dcompetition_test_X)\n",
    "competition_y_pred = np.round(competition_y_pred).astype(int)\n",
    "\n",
    "# Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'building_id': competition_test_building_ids,\n",
    "    'damage_grade': competition_y_pred\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file for submission\n",
    "submission.to_csv('submission_XGBoost_19.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "print(\"Successfully Submitted!\")\n",
    "\n",
    "# Successfully Submitted!\n",
    "# /home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "# Potential solutions:\n",
    "# - Use a data structure that matches the device ordinal in the booster.\n",
    "# - Set the device for booster before call to inplace_predict.\n",
    "\n",
    "# This warning will only be shown once.\n",
    "\n",
    "#   warnings.warn(smsg, UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # [I 2024-01-14 22:04:05,626] Trial 1 finished with value: 0.737477615758506 and parameters: {'max_depth': 8, 'learning_rate': 0.0493605952014681, 'subsample': 0.8050435191563892, 'colsample_bytree': 0.854112103631072, 'reg_alpha': 0.00020118715081702792, 'reg_lambda': 0.006404389225491458, 'min_child_weight': 14, 'gamma': 0.29607599793557965, 'num_boost_round': 549}. Best is trial 1 with value: 0.737477615758506.\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "\n",
    "\n",
    "# # XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# params = {\n",
    "#     'tree_method': 'hist',\n",
    "#     'device': 'cuda',\n",
    "#     'objective': 'multi:softmax',\n",
    "#     'num_class': 3,\n",
    "#     'eval_metric': 'merror',\n",
    "#     'random_state': 42,\n",
    "\n",
    "\n",
    "\n",
    "#     'max_depth': 8,\n",
    "#     'learning_rate': 0.0493605952014681,\n",
    "#     'subsample': 0.8050435191563892,\n",
    "#     'colsample_bytree': 0.854112103631072,\n",
    "#     'reg_alpha': 0.00020118715081702792,\n",
    "#     'reg_lambda': 0.006404389225491458,\n",
    "#     'min_child_weight': 14,\n",
    "#     'gamma': 0.29607599793557965,\n",
    "    \n",
    "# }\n",
    "\n",
    "# # Number of boosting rounds\n",
    "# num_boost_round = 549\n",
    "\n",
    "\n",
    "# model19 = XGBClassifier(**params)\n",
    "\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model19.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model19, 'saved_XGBoost_model19.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model19 = load('model19.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model19.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission_XGBoost_19.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")\n",
    "\n",
    "# # Successfully Submitted!\n",
    "# # /home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "# # Potential solutions:\n",
    "# # - Use a data structure that matches the device ordinal in the booster.\n",
    "# # - Set the device for booster before call to inplace_predict.\n",
    "\n",
    "# # This warning will only be shown once.\n",
    "\n",
    "# #   warnings.warn(smsg, UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Model 20, using first round of Optuna (on the bottom of the \"Optuna_Logs_XGBoost_Model19.txt\" file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [I 2024-01-14 22:04:05,626] Trial 1 finished with value: 0.737477615758506 and parameters: {'max_depth': 8, 'learning_rate': 0.0493605952014681, 'subsample': 0.8050435191563892, 'colsample_bytree': 0.854112103631072, 'reg_alpha': 0.00020118715081702792, 'reg_lambda': 0.006404389225491458, 'min_child_weight': 14, 'gamma': 0.29607599793557965, 'num_boost_round': 549}. Best is trial 1 with value: 0.737477615758506.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# XGBoost expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since XGBoost wants to start at 0\n",
    "train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "params = {\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'merror',\n",
    "    'random_state': 42,\n",
    "\n",
    "\n",
    "\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.0493605952014681,\n",
    "    'subsample': 0.8050435191563892,\n",
    "    'colsample_bytree': 0.854112103631072,\n",
    "    'reg_alpha': 0.00020118715081702792,\n",
    "    'reg_lambda': 0.006404389225491458,\n",
    "    'min_child_weight': 14,\n",
    "    'gamma': 0.29607599793557965,\n",
    "    \n",
    "}\n",
    "\n",
    "# Number of boosting rounds\n",
    "num_boost_round = 549\n",
    "\n",
    "\n",
    "# Concatenate the datasets\n",
    "full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "dfull = xgb.DMatrix(full_X, label=full_Y_temp)\n",
    "\n",
    "model19 = xgb.train(params, dfull, num_boost_round)\n",
    "\n",
    "\n",
    "# Save the model to a file\n",
    "model19.save_model('saved_XGBoost_model19.model')\n",
    "\n",
    "\n",
    "# # This is how to load the model from joblib\n",
    "# from joblib import load\n",
    "# model19 = load('model19.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# Separate building_ids and features in the test data\n",
    "competition_test_building_ids = test_data1['building_id']\n",
    "competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "dcompetition_test_X = xgb.DMatrix(competition_test_X)\n",
    "\n",
    "# Predict on the competition test data\n",
    "competition_y_pred = model19.predict(dcompetition_test_X)\n",
    "competition_y_pred = np.round(competition_y_pred).astype(int)\n",
    "\n",
    "# Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'building_id': competition_test_building_ids,\n",
    "    'damage_grade': competition_y_pred\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file for submission\n",
    "submission.to_csv('submission_XGBoost_19.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "print(\"Successfully Submitted!\")\n",
    "\n",
    "# Successfully Submitted!\n",
    "# /home/michaelye22/.local/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:19:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
    "# Potential solutions:\n",
    "# - Use a data structure that matches the device ordinal in the booster.\n",
    "# - Set the device for booster before call to inplace_predict.\n",
    "\n",
    "# This warning will only be shown once.\n",
    "\n",
    "#   warnings.warn(smsg, UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe try using ALL features in another model and then doing Optuna on it\n",
    "\n",
    "# Maybe try using GPU for XGBoost (GPU Acceleration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
