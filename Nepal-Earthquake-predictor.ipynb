{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my Richter's Predictor Nepal Earthquake Damage Predictor Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default code from Kaggle Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "# import kerastuner as kt\n",
    "# print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "# import keras_tuner as kt2\n",
    "# print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "# import platform\n",
    "# print(\"Python:\", platform.python_version())\n",
    "\n",
    "# import numpy as np\n",
    "# print(\"numpy:\", np.__version__)\n",
    "\n",
    "# import pandas as pd\n",
    "# print(\"pandas:\", pd.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "# import matplotlib\n",
    "# print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "# import seaborn as sns\n",
    "# print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# # WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
    "\n",
    "# # Tensorflow: 2.15.0\n",
    "# # C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_6936\\1753711907.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
    "# #   import kerastuner as kt\n",
    "# # kerastuner: 1.0.5\n",
    "# # keras_tuner: 1.3.5\n",
    "# # Python: 3.10.11\n",
    "# # numpy: 1.24.3\n",
    "# # pandas: 2.1.4\n",
    "# # sklearn version: 1.2.2\n",
    "# # sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
    "# # matplotlib: 3.8.2\n",
    "# # seaborn: 0.13.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_values are the features (X), and train_labels is the target/label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv(\"train_values.csv\")\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "test_values = pd.read_csv(\"test_values.csv\")\n",
    "\n",
    "# print(\"train labels:\\n\", train_labels.head())\n",
    "\n",
    "# print(\"train_values:\\n\", train_values.head())\n",
    "      \n",
    "# print(\"test_values:\\n\", test_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I want to find out which features to use since there are so many. Here are some common data science techniques:\n",
    "\n",
    "1. **Correlation Matrix with Heatmap**: Correlation states how the features are related to each other or the target variable. You can use a heatmap to visualize the correlation matrix.\n",
    "\n",
    "2. **Univariate Selection**: Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "3. **Recursive Feature Elimination (RFE)**: RFE is a popular feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached.\n",
    "\n",
    "4. **Feature Importance**: You can get the feature importance of each feature of your dataset by using the feature importance property of the model. For example, Decision Trees models in the scikit-learn library offer an importance property that can be accessed directly.\n",
    "\n",
    "For categorical features, you can convert them into numerical values using techniques like One-Hot Encoding or Label Encoding before applying these feature selection techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False  True  True False  True False  True\n",
      "  True False  True  True  True  True False  True  True False False  True\n",
      " False False False False  True False False False False False False False\n",
      " False False False  True False False False False]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2 18 10 31\n",
      " 27 33 28 25 30  6  1  1  3  1  8  1  1  5  1  1  1  1 34  1  1 16  9  1\n",
      " 23  7  4 14  1 29 15 20 17 35 32 24 21 13 26  1 11 22 12 19]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMRegressor #Lightgbm is a great gradient boosting model for large amount of data\n",
    "\n",
    "# Assuming X is your feature set and y is the target value\n",
    "X = train_values.drop('building_id', axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "y = train_labels.drop('building_id', axis=1)\n",
    "y = np.ravel(y) # converting dataframe to a one-dimensional array using the ravel function from numpy\n",
    "\n",
    "estimator = LGBMRegressor(verbose = 0, random_state = 42)  # It's best to find the best model for you\n",
    "selector = RFE(estimator, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# The mask of selected features.\n",
    "print(selector.support_)\n",
    "\n",
    "# The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. \n",
    "# Selected features are assigned rank 1.\n",
    "print(selector.ranking_)\n",
    "\n",
    "# [ True  True  True  True  True  True  True  True  True  True  True  True\n",
    "#   True  True  True  True  True  True  True  True False False False False\n",
    "#  False False False False False False  True  True False  True False  True\n",
    "#   True False  True  True  True  True False  True  True False False  True\n",
    "#  False False False False  True False False False False False False False\n",
    "#  False False False  True False False False False]\n",
    "# [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2 18 10 31\n",
    "#  27 33 28 25 30  6  1  1  3  1  8  1  1  5  1  1  1  1 34  1  1 16  9  1\n",
    "#  23  7  4 14  1 29 15 20 17 35 32 24 21 13 26  1 11 22 12 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis from ChatGPT-4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables in the features dataset have been successfully encoded. Now, let's look at the correlation of these features with the `damage_grade`:\n",
    "\n",
    "### Correlation with `damage_grade`\n",
    "The correlation values range between -1 and 1. A value closer to 1 indicates a strong positive correlation, meaning that as the feature increases, the `damage_grade` tends to increase. Conversely, a value closer to -1 indicates a strong negative correlation, where an increase in the feature leads to a decrease in `damage_grade`. Values around 0 imply weak or no linear correlation.\n",
    "\n",
    "#### Top Positively Correlated Features:\n",
    "- `has_superstructure_mud_mortar_stone`\n",
    "- `count_floors_pre_eq`\n",
    "- Other features like `legal_ownership_status`, `has_superstructure_stone_flag`, etc., also show positive correlation but to a lesser extent.\n",
    "\n",
    "#### Top Negatively Correlated Features:\n",
    "- `has_superstructure_cement_mortar_brick`\n",
    "- `ground_floor_type`\n",
    "- `has_superstructure_rc_engineered`\n",
    "- Other features like `roof_type`, `has_superstructure_rc_non_engineered`, etc., also show negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the selected features based on the correlation threshold (of 0.05):\n",
    "\n",
    "1. `has_superstructure_mud_mortar_stone`\n",
    "2. `count_floors_pre_eq`\n",
    "3. `legal_ownership_status`\n",
    "4. `has_superstructure_stone_flag`\n",
    "5. `count_families`\n",
    "6. `has_superstructure_adobe_mud`\n",
    "7. `position`\n",
    "8. `has_superstructure_cement_mortar_stone`\n",
    "9. `has_superstructure_bamboo`\n",
    "10. `has_superstructure_timber`\n",
    "11. `geo_level_1_id`\n",
    "12. `has_secondary_use`\n",
    "13. `has_secondary_use_rental`\n",
    "14. `has_secondary_use_hotel`\n",
    "15. `foundation_type`\n",
    "16. `area_percentage`\n",
    "17. `has_superstructure_rc_non_engineered`\n",
    "18. `roof_type`\n",
    "19. `has_superstructure_rc_engineered`\n",
    "20. `ground_floor_type`\n",
    "21. `has_superstructure_cement_mortar_brick`\n",
    "\n",
    "These features were chosen because they have a correlation with the target variable `damage_grade` greater than the specified threshold of 0.05 (in absolute value). You can use these features for building your predictive model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TRY USING lightgbm! It's SO FAST. And maybe try it on the titanic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
