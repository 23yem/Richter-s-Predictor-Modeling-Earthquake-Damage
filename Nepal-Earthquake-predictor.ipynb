{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my Richter's Predictor Nepal Earthquake Damage Predictor Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default code from Kaggle Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "# import kerastuner as kt\n",
    "# print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "# import keras_tuner as kt2\n",
    "# print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "# import platform\n",
    "# print(\"Python:\", platform.python_version())\n",
    "\n",
    "# import numpy as np\n",
    "# print(\"numpy:\", np.__version__)\n",
    "\n",
    "# import pandas as pd\n",
    "# print(\"pandas:\", pd.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "# import sklearn\n",
    "# print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "# import matplotlib\n",
    "# print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "# import seaborn as sns\n",
    "# print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# # WARNING:tensorflow:From c:\\Users\\Micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
    "\n",
    "# # Tensorflow: 2.15.0\n",
    "# # C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_6936\\1753711907.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
    "# #   import kerastuner as kt\n",
    "# # kerastuner: 1.0.5\n",
    "# # keras_tuner: 1.3.5\n",
    "# # Python: 3.10.11\n",
    "# # numpy: 1.24.3\n",
    "# # pandas: 2.1.4\n",
    "# # sklearn version: 1.2.2\n",
    "# # sklearn path: ['c:\\\\Users\\\\Micha\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\sklearn']\n",
    "# # matplotlib: 3.8.2\n",
    "# # seaborn: 0.13.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_values are the features (X), and train_labels is the target/label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"train_values.csv\")\n",
    "train_Y = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "test_values = pd.read_csv(\"test_values.csv\")\n",
    "\n",
    "# print(\"train labels:\\n\", train_Y.head())\n",
    "\n",
    "# print(\"train values:\\n\", train_X.head())\n",
    "      \n",
    "# print(\"test_values:\\n\", test_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I want to find out which features to use since there are so many. Here are some common data science techniques:\n",
    "\n",
    "1. **Correlation Matrix with Heatmap**: Correlation states how the features are related to each other or the target variable. You can use a heatmap to visualize the correlation matrix.\n",
    "\n",
    "2. **Univariate Selection**: Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "3. **Recursive Feature Elimination (RFE)**: RFE is a popular feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached.\n",
    "\n",
    "4. **Feature Importance**: You can get the feature importance of each feature of your dataset by using the feature importance property of the model. For example, Decision Trees models in the scikit-learn library offer an importance property that can be accessed directly.\n",
    "\n",
    "For categorical features, you can convert them into numerical values using techniques like One-Hot Encoding or Label Encoding before applying these feature selection techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, I will try RFE (Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from lightgbm import LGBMRegressor #Lightgbm is a great gradient boosting model for large amount of data\n",
    "\n",
    "# # Assuming X is your feature set and y is the target value\n",
    "# X = train_X.drop('building_id', axis=1)\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# y = train_Y.drop('building_id', axis=1)\n",
    "# y = np.ravel(y) # converting dataframe to a one-dimensional array using the ravel function from numpy\n",
    "\n",
    "# estimator = LGBMRegressor(verbose = 0, random_state = 42)  # It's best to find the best model for you\n",
    "# selector = RFE(estimator, step=1)\n",
    "# selector = selector.fit(X, y)\n",
    "\n",
    "# # Assuming 'X' is your DataFrame with the feature data\n",
    "# feature_names = X.columns\n",
    "\n",
    "# # Map the feature names to the support array, which tells you which features were selected\n",
    "# support_dict = dict(zip(feature_names, selector.support_))\n",
    "\n",
    "# # Get the selected features\n",
    "# selected_features = [feature for feature, support in support_dict.items() if support]\n",
    "\n",
    "# # Print the selected features\n",
    "# print(\"Selected features:\\n\", selected_features)\n",
    "\n",
    "# # ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "# # 'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "# # 'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "# # 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "# # 'land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', \n",
    "# # 'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s',\n",
    "# # 'plan_configuration_u']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. \n",
    "# # Selected features are assigned rank 1.\n",
    "# # Map the feature names to the ranking array\n",
    "# ranking_dict = dict(zip(feature_names, selector.ranking_))\n",
    "# print(ranking_dict)\n",
    "\n",
    "\n",
    "# # [ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2 18 10 31\n",
    "# #  27 33 28 25 30  6  1  1  3  1  8  1  1  5  1  1  1  1 34  1  1 16  9  1\n",
    "# #  23  7  4 14  1 29 15 20 17 35 32 24 21 13 26  1 11 22 12 19]\n",
    "\n",
    "\n",
    "# # {'geo_level_1_id': 1, 'geo_level_2_id': 1, 'geo_level_3_id': 1, 'count_floors_pre_eq': 1, 'age': 1, 'area_percentage': 1, \n",
    "# #  'height_percentage': 1, 'has_superstructure_adobe_mud': 1, 'has_superstructure_mud_mortar_stone': 1, 'has_superstructure_stone_flag': 1, \n",
    "# #  'has_superstructure_cement_mortar_stone': 1, 'has_superstructure_mud_mortar_brick': 1, 'has_superstructure_cement_mortar_brick': 1, \n",
    "# #  'has_superstructure_timber': 1, 'has_superstructure_bamboo': 1, 'has_superstructure_rc_non_engineered': 1, 'has_superstructure_rc_engineered': 1, \n",
    "# #  'has_superstructure_other': 1, 'count_families': 1, 'has_secondary_use': 1, 'has_secondary_use_agriculture': 2, 'has_secondary_use_hotel': 18, \n",
    "# #  'has_secondary_use_rental': 10, 'has_secondary_use_institution': 31, 'has_secondary_use_school': 27, 'has_secondary_use_industry': 33, \n",
    "# #  'has_secondary_use_health_post': 28, 'has_secondary_use_gov_office': 25, 'has_secondary_use_use_police': 30, 'has_secondary_use_other': 6, \n",
    "# #  'land_surface_condition_n': 1, 'land_surface_condition_o': 1, 'land_surface_condition_t': 3, 'foundation_type_h': 1, 'foundation_type_i': 8, \n",
    "# #  'foundation_type_r': 1, 'foundation_type_u': 1, 'foundation_type_w': 5, 'roof_type_n': 1, 'roof_type_q': 1, 'roof_type_x': 1, \n",
    "# #  'ground_floor_type_f': 1, 'ground_floor_type_m': 34, 'ground_floor_type_v': 1, 'ground_floor_type_x': 1, 'ground_floor_type_z': 16, \n",
    "# #  'other_floor_type_j': 9, 'other_floor_type_q': 1, 'other_floor_type_s': 23, 'other_floor_type_x': 7, 'position_j': 4, 'position_o': 14, \n",
    "# #  'position_s': 1, 'position_t': 29, 'plan_configuration_a': 15, 'plan_configuration_c': 20, 'plan_configuration_d': 17, 'plan_configuration_f': 35, \n",
    "# #  'plan_configuration_m': 32, 'plan_configuration_n': 24, 'plan_configuration_o': 21, 'plan_configuration_q': 13, 'plan_configuration_s': 26, \n",
    "# #  'plan_configuration_u': 1, 'legal_ownership_status_a': 11, 'legal_ownership_status_r': 22, 'legal_ownership_status_v': 12, \n",
    "# #  'legal_ownership_status_w': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "New features:\n",
      " ['land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', 'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s', 'plan_configuration_u']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "'land_surface_condition_n', 'land_surface_condition_o', 'foundation_type_h', 'foundation_type_r', 'foundation_type_u', 'roof_type_n', \n",
    "'roof_type_q', 'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_v', 'ground_floor_type_x', 'other_floor_type_q', 'position_s',\n",
    "'plan_configuration_u']\n",
    "\n",
    "print(len(features))\n",
    "\n",
    "\n",
    "# Find out which features are created through one-hot-encoding\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original data\n",
    "original_data = pd.read_csv('train_values.csv')\n",
    "\n",
    "# Get the original feature names\n",
    "original_features = original_data.columns\n",
    "\n",
    "# Check which features are not in the original data\n",
    "new_features = [feature for feature in features if feature not in original_features]\n",
    "\n",
    "# Print the new features\n",
    "print(\"New features:\\n\", new_features)\n",
    "\n",
    "\n",
    "#Manually remove the one-hot-encoding that pd.get_dummies() used on categorial \n",
    "features_before_dummies = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage',\n",
    "'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', \n",
    "'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'count_families', 'has_secondary_use', \n",
    "'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position','plan_configuration']\n",
    "\n",
    "print(len(features_before_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis from ChatGPT-4 (second way to find best features):\n",
    "\n",
    "The categorical variables in the features dataset have been successfully encoded. Now, let's look at the correlation of these features with the `damage_grade`:\n",
    "\n",
    "#### Correlation with `damage_grade`\n",
    "The correlation values range between -1 and 1. A value closer to 1 indicates a strong positive correlation, meaning that as the feature increases, the `damage_grade` tends to increase. Conversely, a value closer to -1 indicates a strong negative correlation, where an increase in the feature leads to a decrease in `damage_grade`. Values around 0 imply weak or no linear correlation.\n",
    "\n",
    "#### Top Positively Correlated Features:\n",
    "- `has_superstructure_mud_mortar_stone`\n",
    "- `count_floors_pre_eq`\n",
    "- Other features like `legal_ownership_status`, `has_superstructure_stone_flag`, etc., also show positive correlation but to a lesser extent.\n",
    "\n",
    "#### Top Negatively Correlated Features:\n",
    "- `has_superstructure_cement_mortar_brick`\n",
    "- `ground_floor_type`\n",
    "- `has_superstructure_rc_engineered`\n",
    "- Other features like `roof_type`, `has_superstructure_rc_non_engineered`, etc., also show negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the selected features based on the correlation threshold (of 0.05):\n",
    "\n",
    "1. `has_superstructure_mud_mortar_stone`\n",
    "2. `count_floors_pre_eq`\n",
    "3. `legal_ownership_status`\n",
    "4. `has_superstructure_stone_flag`\n",
    "5. `count_families`\n",
    "6. `has_superstructure_adobe_mud`\n",
    "7. `position`\n",
    "8. `has_superstructure_cement_mortar_stone`\n",
    "9. `has_superstructure_bamboo`\n",
    "10. `has_superstructure_timber`\n",
    "11. `geo_level_1_id`\n",
    "12. `has_secondary_use`\n",
    "13. `has_secondary_use_rental`\n",
    "14. `has_secondary_use_hotel`\n",
    "15. `foundation_type`\n",
    "16. `area_percentage`\n",
    "17. `has_superstructure_rc_non_engineered`\n",
    "18. `roof_type`\n",
    "19. `has_superstructure_rc_engineered`\n",
    "20. `ground_floor_type`\n",
    "21. `has_superstructure_cement_mortar_brick`\n",
    "\n",
    "These features were chosen because they have a correlation with the target variable `damage_grade` greater than the specified threshold of 0.05 (in absolute value). You can use these features for building your predictive model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [\n",
    "    \"has_superstructure_mud_mortar_stone\",\n",
    "    \"count_floors_pre_eq\",\n",
    "    \"legal_ownership_status\",\n",
    "    \"has_superstructure_stone_flag\",\n",
    "    \"count_families\",\n",
    "    \"has_superstructure_adobe_mud\",\n",
    "    \"position\",\n",
    "    \"has_superstructure_cement_mortar_stone\",\n",
    "    \"has_superstructure_bamboo\",\n",
    "    \"has_superstructure_timber\",\n",
    "    \"geo_level_1_id\",\n",
    "    \"has_secondary_use\",\n",
    "    \"has_secondary_use_rental\",\n",
    "    \"has_secondary_use_hotel\",\n",
    "    \"foundation_type\",\n",
    "    \"area_percentage\",\n",
    "    \"has_superstructure_rc_non_engineered\",\n",
    "    \"roof_type\",\n",
    "    \"has_superstructure_rc_engineered\",\n",
    "    \"ground_floor_type\",\n",
    "    \"has_superstructure_cement_mortar_brick\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, I will try SelectKBest to find best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "# # Create the SelectKBest with the f_classif function. You can set the parameter \"k\" equal to a number if you want to limit the amount of features\n",
    "# selector = SelectKBest(f_classif, k = 20) # Getting the 20 best features\n",
    "\n",
    "# # Assuming X is your feature set and y is the target value\n",
    "# X = train_X.drop('building_id', axis=1)\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# y = train_Y.drop('building_id', axis=1)\n",
    "# y = np.ravel(y) # converting dataframe to a one-dimensional array using the ravel function from numpy\n",
    "\n",
    "\n",
    "# # Fit the selector to the data\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the boolean mask of the selected features\n",
    "# mask = selector.get_support()\n",
    "\n",
    "# # Get the names of the selected features\n",
    "# selected_features = X.columns[mask]\n",
    "\n",
    "# # Convert the Index object to a list\n",
    "# features3 = selected_features.tolist()\n",
    "\n",
    "# print(features3)\n",
    "\n",
    "# # ['geo_level_1_id', 'count_floors_pre_eq', 'area_percentage', 'has_superstructure_mud_mortar_stone', 'has_superstructure_cement_mortar_brick', \n",
    "# #  'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_secondary_use_hotel', 'has_secondary_use_rental', \n",
    "# #  'foundation_type_i', 'foundation_type_r', 'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_x', 'ground_floor_type_f', \n",
    "# #  'ground_floor_type_v', 'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s']\n",
    "\n",
    "\n",
    "# # Get the scores\n",
    "# scores = selector.scores_\n",
    "\n",
    "# # Create a DataFrame with the scores\n",
    "# features_scores = pd.DataFrame({'Feature': X.columns, 'Score': scores})\n",
    "\n",
    "# # Sort the DataFrame by score in descending order\n",
    "# features_scores = features_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# # print the best 20 features\n",
    "# print(features_scores[0:19])\n",
    "\n",
    "# #                                    Feature         Score\n",
    "# # 35                       foundation_type_r  23787.275036\n",
    "# # 43                     ground_floor_type_v  20782.933584\n",
    "# # 40                             roof_type_x  16891.038184\n",
    "# # 8      has_superstructure_mud_mortar_stone  16490.386507\n",
    "# # 34                       foundation_type_i  16385.772905\n",
    "# # 12  has_superstructure_cement_mortar_brick  11120.193268\n",
    "# # 48                      other_floor_type_s  10507.484572\n",
    "# # 41                     ground_floor_type_f  10151.525359\n",
    "# # 16        has_superstructure_rc_engineered   7757.593854\n",
    "# # 47                      other_floor_type_q   7378.599061\n",
    "# # 15    has_superstructure_rc_non_engineered   4721.916051\n",
    "# # 37                       foundation_type_w   4568.674306\n",
    "# # 46                      other_floor_type_j   4533.708398\n",
    "# # 36                       foundation_type_u   2972.409108\n",
    "# # 0                           geo_level_1_id   2657.791274\n",
    "# # 3                      count_floors_pre_eq   2544.836052\n",
    "# # 5                          area_percentage   2529.046730\n",
    "# # 38                             roof_type_n   1776.396178\n",
    "# # 21                 has_secondary_use_hotel   1537.672773\n",
    "\n",
    "# # ...\n",
    "\n",
    "# # 22                has_secondary_use_rental   1342.099336\n",
    "# # 64                legal_ownership_status_a   1166.606551\n",
    "# # 19                       has_secondary_use    841.802928\n",
    "# # 39                             roof_type_q    761.885856\n",
    "# # 7             has_superstructure_adobe_mud    739.412821\n",
    "# # 13               has_superstructure_timber    659.199014\n",
    "# # 9            has_superstructure_stone_flag    576.438023\n",
    "# # 14               has_superstructure_bamboo    538.551492\n",
    "# # 66                legal_ownership_status_v    536.308634\n",
    "# # 11     has_superstructure_mud_mortar_brick    531.784659\n",
    "# # 63                    plan_configuration_u    515.087147\n",
    "# # 10  has_superstructure_cement_mortar_stone    478.844199\n",
    "# # 18                          count_families    476.562914\n",
    "# # 56                    plan_configuration_d    378.234531\n",
    "# # 53                              position_t    373.594539\n",
    "# # 6                        height_percentage    370.173817\n",
    "# # 20           has_secondary_use_agriculture    289.462856\n",
    "# # 1                           geo_level_2_id    264.447807\n",
    "# # 49                      other_floor_type_x    244.432657\n",
    "# # 4                                      age    219.626253\n",
    "# # 33                       foundation_type_h    209.425818\n",
    "# # 32                land_surface_condition_t    201.698101\n",
    "# # 30                land_surface_condition_n    182.152148\n",
    "# # 61                    plan_configuration_q    165.370472\n",
    "# # 23           has_secondary_use_institution    146.731486\n",
    "# # 17                has_superstructure_other    142.014204\n",
    "# # 50                              position_j    136.783490\n",
    "# # 67                legal_ownership_status_w    116.039402\n",
    "# # 52                              position_s    110.038831\n",
    "# # 55                    plan_configuration_c     75.423022\n",
    "# # 51                              position_o     75.104323\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = ['geo_level_1_id', 'count_floors_pre_eq', 'area_percentage', 'has_superstructure_mud_mortar_stone', 'has_superstructure_cement_mortar_brick', \n",
    " 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_secondary_use_hotel', 'has_secondary_use_rental', \n",
    " 'foundation_type_i', 'foundation_type_r', 'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_x', 'ground_floor_type_f', \n",
    " 'ground_floor_type_v', 'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if there are any missing values in the data. If so, we have to do imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in train_X: 0\n",
      "Number of missing values in train_Y: 0\n",
      "Number of missing values in test_values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_train_X = train_X.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_X:\", missing_train_X)\n",
    "\n",
    "missing_train_Y = train_Y.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_Y:\", missing_train_Y)\n",
    "\n",
    "missing_test_values = test_values.isnull().sum().sum()\n",
    "print(\"Number of missing values in test_values:\", missing_test_values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 0 missing values in each dataframe, we don't have to do imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have 3 different list of features (features, features2, and features3), which I found using RFE. Data Analysis ChatGPT-4, and SelectKBest respectively\n",
    "\n",
    "#### Now, I have to turn one-hot-encode the data using pd.get_dummies, and I'll be creating 3 seperate train_X, one for each list of possible best features. And also on the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. testX1 for the RFE features\n",
    "# Since the features from RFE are the one-hot-encoded features, we have to apply features after doing pd.get_dummies()\n",
    "\n",
    "trainX1 = pd.get_dummies(train_X)\n",
    "trainX1 = trainX1[features]\n",
    "\n",
    "\n",
    "# 2. testX2 for the Data Analysis ChatGPT-4\n",
    "# Since the features from Data Analysis are from the original feature set, we have to apply the features before doing pd.get_dummies()\n",
    "\n",
    "trainX2 = train_X[features2]\n",
    "trainX2 = pd.get_dummies(trainX2)\n",
    "\n",
    "\n",
    "\n",
    "# 3. testX3 for the SelectKBest\n",
    "# Since the features from RFE are the one-hot-encoded features, we have to apply features after doing pd.get_dummies()\n",
    "\n",
    "trainX3 = pd.get_dummies(train_X)\n",
    "trainX3 = trainX3[features3]\n",
    "\n",
    "\n",
    "\n",
    "# 4. Do pd.get_dummies() on test data. I will create a seperate test_data for each feature selection, since each test_data needs to have a certain set of features\n",
    "\n",
    "test_data1 = pd.get_dummies(test_values)\n",
    "test_data1 = test_data1[features + ['building_id'] ]\n",
    "\n",
    "test_data2 = test_values[features2 + ['building_id'] ]\n",
    "test_data2 = pd.get_dummies(test_data2)\n",
    "\n",
    "test_data3 = pd.get_dummies(test_values)\n",
    "test_data3 = test_data3[features3 + ['building_id'] ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's important to do pd.get_dummies() before doing the train_valid_test split. Now we can do the split\n",
    "I have to do train_valid_test split three times, one for each different train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. train_valid_test split for train_X1\n",
    "train_X1, test_X1, train_Y1, test_Y1 = train_test_split(trainX1, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X1, test_X1, valid_Y1, test_Y1 = train_test_split(test_X1, test_Y1, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)\n",
    "\n",
    "\n",
    "# 2. train_valid_test split for train_X2\n",
    "train_X2, test_X2, train_Y2, test_Y2 = train_test_split(trainX2, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X2, test_X2, valid_Y2, test_Y2 = train_test_split(test_X2, test_Y2, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)\n",
    "\n",
    "\n",
    "# 3. train_valid_test split for train_X3\n",
    "train_X3, test_X3, train_Y3, test_Y3 = train_test_split(trainX3, train_Y, test_size=0.3, random_state = 42) # split into training (70%) and a test set (30%)\n",
    "\n",
    "valid_X3, test_X3, valid_Y3, test_Y3 = train_test_split(test_X3, test_Y3, test_size = 0.5, random_state = 42) # split test set into a validation (15%) and test set (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Normalizing Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different Normalization Scalers:\n",
    "\n",
    "1. **MinMaxScaler**: This scaler scales and translates each feature individually such that it is in the given range on the training set, e.g., between zero and one.\n",
    "\n",
    "\n",
    "2. **StandardScaler**: This scaler standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "\n",
    "3. **RobustScaler**: This scaler scales features using statistics that are robust to outliers. It uses the Interquartile Range (IQR) to scale the data, making it a better choice for when the data has outliers.\n",
    "\n",
    "\n",
    "4. **Normalizer**: This scaler scales individual samples to have unit norm. This scaler works on the rows, not the columns!\n",
    "\n",
    "\n",
    "5. **MaxAbsScaler**: This scaler scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "\n",
    "\n",
    "Remember, the choice of scaler can depend on your specific dataset and the machine learning algorithm that you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've actually decided to not do normalization since almost all the columns are categorial columns, and the non-categorial columns are mostly normalized already in the dataset, so there's no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've actually decided to not do normalization since almost all the columns are categorial columns, and the non-categorial columns are mostly normalized already in the dataset, so there's no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LightGBMClassifier for Model 1 (Model 1 uses features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Define the model\n",
    "# model1 = LGBMClassifier(boosting_type='gbdt', \n",
    "#                        objective='multiclass', \n",
    "#                        num_class=3, \n",
    "#                        metric='multi_logloss', \n",
    "#                        seed=42,   # Control randomness\n",
    "#                     #    bagging_fraction=1, # Control randomness\n",
    "#                     #    bagging_freq=0,  # Control randomness\n",
    "#                     #    feature_fraction=1,  # Control randomness\n",
    "#                        deterministic=True,  # Control randomness\n",
    "#                        verbose=0)\n",
    "\n",
    "# # Train the model\n",
    "# model1.fit(train_X1, train_Y1_temp, eval_set=[(valid_X1, valid_Y1_temp)])\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model1.predict(test_X1)\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "# # Print the F1 score, since this is what the competition uses as its scoring system\n",
    "# print('The micro-averaged F1 score of prediction is:', f1_score(test_Y1_temp, y_pred, average='micro'))\n",
    "\n",
    "# # Print the accuracy\n",
    "# print('The accuracy of prediction is:', accuracy_score(test_Y1_temp, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "# params = model1.get_params()\n",
    "\n",
    "# print(\"Hyperparamters: \")\n",
    "# for key, value in params.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "\n",
    "# # Save hyperparameters in a JSON\n",
    "# import json\n",
    "\n",
    "# with open('hyperparameters_model1.json', 'w') as f:\n",
    "#     json.dump(params, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model1)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model1.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 1. Score: 0.7112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model1.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model1, 'model1.joblib')\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model1 = load('model1.joblib')\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model1.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission1.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model 2, which is the exact same thing as Model 1 but with features2 instead of features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 2 1 1]\n",
      "The micro-averaged F1 score of prediction is: 0.6803867897981632\n",
      "The accuracy of prediction is: 0.6803867897981632\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "train_Y2_temp = train_Y2['damage_grade'] - 1  \n",
    "valid_Y2_temp = valid_Y2['damage_grade'] - 1\n",
    "test_Y2_temp = test_Y2['damage_grade'] - 1\n",
    "\n",
    "# Define the model\n",
    "model2 = LGBMClassifier(boosting_type='gbdt', \n",
    "                       objective='multiclass', \n",
    "                       num_class=3, \n",
    "                       metric='multi_logloss', \n",
    "                       seed=42,   # Control randomness\n",
    "                    #    bagging_fraction=1, # Control randomness\n",
    "                    #    bagging_freq=0,  # Control randomness\n",
    "                    #    feature_fraction=1,  # Control randomness\n",
    "                       deterministic=True,  # Control randomness\n",
    "                       verbose=0)\n",
    "\n",
    "# Train the model\n",
    "model2.fit(train_X2, train_Y2_temp, eval_set=[(valid_X2, valid_Y2_temp)])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model2.predict(test_X2)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Print the F1 score, since this is what the competition uses as its scoring system\n",
    "print('The micro-averaged F1 score of prediction is:', f1_score(test_Y2_temp, y_pred, average='micro'))\n",
    "\n",
    "# Print the accuracy\n",
    "print('The accuracy of prediction is:', accuracy_score(test_Y2_temp, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "# params = model2.get_params()\n",
    "\n",
    "# print(\"Hyperparamters: \")\n",
    "# for key, value in params.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "\n",
    "# # Save hyperparameters in a JSON\n",
    "# import json\n",
    "\n",
    "# with open('hyperparameters_model2.json', 'w') as f: # Make sure to change the name to match whatever model you are using\n",
    "#     json.dump(params, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model2)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model2.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 2. Score: 68.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X2, valid_X2, test_X2])\n",
    "# full_Y_temp = pd.concat([train_Y2_temp, valid_Y2_temp, test_Y2_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model2.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model2, 'model2.joblib')\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model2 = load('model2.joblib')\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data2['building_id']\n",
    "# competition_test_X = test_data2.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model2.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission2.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model 3, which is the exact same thing as Model 1 but with features3 instead of features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "# train_Y3_temp = train_Y3['damage_grade'] - 1  \n",
    "# valid_Y3_temp = valid_Y3['damage_grade'] - 1\n",
    "# test_Y3_temp = test_Y3['damage_grade'] - 1\n",
    "\n",
    "# # Define the model\n",
    "# model3 = LGBMClassifier(boosting_type='gbdt', \n",
    "#                        objective='multiclass', \n",
    "#                        num_class=3, \n",
    "#                        metric='multi_logloss', \n",
    "#                        seed=42,   # Control randomness\n",
    "#                     #    bagging_fraction=1, # Control randomness\n",
    "#                     #    bagging_freq=0,  # Control randomness\n",
    "#                     #    feature_fraction=1,  # Control randomness\n",
    "#                        deterministic=True,  # Control randomness\n",
    "#                        verbose=0)\n",
    "\n",
    "# # Train the model\n",
    "# model3.fit(train_X3, train_Y3_temp, eval_set=[(valid_X3, valid_Y3_temp)])\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model3.predict(test_X3)\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "# # Print the F1 score, since this is what the competition uses as its scoring system\n",
    "# print('The micro-averaged F1 score of prediction is:', f1_score(test_Y3_temp, y_pred, average='micro'))\n",
    "\n",
    "# # Print the accuracy\n",
    "# print('The accuracy of prediction is:', accuracy_score(test_Y3_temp, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "# params = model3.get_params()\n",
    "\n",
    "# print(\"Hyperparamters: \")\n",
    "# for key, value in params.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "\n",
    "# # Save hyperparameters in a JSON\n",
    "# import json\n",
    "\n",
    "# with open('hyperparameters_model3.json', 'w') as f: # Make sure to change the name to match whatever model you are using\n",
    "#     json.dump(params, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model3)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model3.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 3. Score: 67.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X3, valid_X3, test_X3])\n",
    "# full_Y_temp = pd.concat([train_Y3_temp, valid_Y3_temp, test_Y3_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model3.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model3, 'model3.joblib')\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model3 = load('model3.joblib')\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data3['building_id']\n",
    "# competition_test_X = test_data3.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model3.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission3.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 will use \"features\" (same as Model 1) but with Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning hyperparameters for the `LGBMClassifier`, there are several key hyperparameters that typically have the biggest impact on performance:\n",
    "\n",
    "1. `num_leaves`: This is the main parameter to control the complexity of the tree model. Theoretically, having more leaves can capture more information about the data, but it can also lead to overfitting. \n",
    "\n",
    "2. `min_data_in_leaf`: This is a very important parameter to prevent overfitting in a leaf-wise tree. Its optimal value depends on the number of training samples and `num_leaves`. Setting it to a large value can avoid growing too deep a tree, but may cause underfitting.\n",
    "\n",
    "3. `max_depth`: This parameter is used to handle model overfitting. If you feel that your model is overfitted, you can try to limit `max_depth`.\n",
    "\n",
    "4. `learning_rate`: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate and improving it gradually, and the learning rate controls how much each tree contributes to this.\n",
    "\n",
    "5. `n_estimators`: This is the number of boosted trees to fit. More trees can model more complex patterns, but can also lead to overfitting.\n",
    "\n",
    "6. `subsample`: This is the fraction of samples to be used for fitting the individual base learners. If it's less than 1.0, that leads to Stochastic Gradient Boosting.\n",
    "\n",
    "7. `colsample_bytree`: This is the subsample ratio of columns when constructing each tree.\n",
    "\n",
    "8. `reg_alpha` and `reg_lambda`: These are regularization parameters. They can help prevent overfitting.\n",
    "\n",
    "Remember, the best hyperparameters can depend on your specific dataset, so it's always a good idea to use something like grid search or random search to find the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# # LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp])\n",
    "\n",
    "# # Define the model\n",
    "# model4 = LGBMClassifier(boosting_type='gbdt', \n",
    "#                        objective='multiclass', \n",
    "#                        num_class=3, \n",
    "#                        metric='multi_logloss', \n",
    "#                        random_state=42)\n",
    "\n",
    "# # Define the parameter grid focusing on most important hyperparameters\n",
    "# param_grid = {\n",
    "#     'num_leaves': list(range(150, 300, 50)), # 75 folds took 2 min 43 sec\n",
    "#     'max_depth': [-1], # -1 is the default\n",
    "#     'min_data_in_leaf': [20], # 20 is the default \n",
    "#     'n_estimators': list(range(50, 301, 50)), #default value is 100\n",
    "#     'min_child_samples': list(range(5, 20, 5)), # default value is 20\n",
    "#     # 'learning_rate': [i/100 for i in range(1, 11)], # default is 0.1\n",
    "#     # 'subsample': [i/10 for i in range(1, 11)], # default value is 1.0\n",
    "#     # 'colsample_bytree': [i/10 for i in range(1, 5)], # default value is 1.0\n",
    "# }\n",
    "\n",
    "# # Define the grid search\n",
    "# grid = GridSearchCV(model4, \n",
    "#                     param_grid, \n",
    "#                     verbose=3, \n",
    "#                     cv=2, \n",
    "#                     n_jobs=-1, \n",
    "#                     scoring=make_scorer(f1_score, average='micro'))\n",
    "\n",
    "# # Fit the grid search on the full data\n",
    "# grid.fit(train_X1_full, train_Y1_temp_full)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(f'Best parameters: {grid.best_params_}')\n",
    "# print(f'Best score: {grid.best_score_}')\n",
    "\n",
    "# model4 = grid.best_estimator_\n",
    "\n",
    "# # Use the best estimator to make predictions\n",
    "# y_pred = model4.predict(test_X1)\n",
    "\n",
    "# # Print the F1 score and the accuracy\n",
    "# print('The micro-averaged F1 score of prediction is:', f1_score(test_Y1_temp, y_pred, average='micro'))\n",
    "# print('The accuracy of prediction is:', accuracy_score(test_Y1_temp, y_pred))\n",
    "\n",
    "# # 75 fits. Took 2 min and 43 sec. THIS GOT 0.7405\n",
    "# # Best parameters: {'num_leaves': 252}\n",
    "# # Best score: 0.7361067271692862\n",
    "# # The micro-averaged F1 score of prediction is: 0.7364866593333503\n",
    "# # The accuracy of prediction is: 0.7364866593333503\n",
    "\n",
    "# # 240 fits. Took 6 min and 44 sec. This got 0.7358. \n",
    "# # Best parameters: {'max_depth': 10, 'min_data_in_leaf': 20, 'num_leaves': 250}\n",
    "# # Best score: 0.7282244593923525\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7317541121997391\n",
    "# # The accuracy of prediction is: 0.7317541121997391\n",
    "\n",
    "# # 108 fits. Took 14 min and 15 seconds, probably because max_depth is -1 so that means it's unlimited. This got a 0.7449\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 200}\n",
    "# # Best score: 0.7348516996975305\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7434703640224093\n",
    "# # The accuracy of prediction is: 0.7434703640224093\n",
    "\n",
    "# # Hyperparamters: \n",
    "# # boosting_type : gbdt\n",
    "# # class_weight : None\n",
    "# # colsample_bytree : 1.0\n",
    "# # importance_type : split\n",
    "# # learning_rate : 0.1\n",
    "# # max_depth : -1\n",
    "# # min_child_samples : 5\n",
    "# # min_child_weight : 0.001\n",
    "# # min_split_gain : 0.0\n",
    "# # n_estimators : 300\n",
    "# # n_jobs : None\n",
    "# # num_leaves : 200\n",
    "# # objective : multiclass\n",
    "# # random_state : 42\n",
    "# # reg_alpha : 0.0\n",
    "# # reg_lambda : 0.0\n",
    "# # subsample : 1.0\n",
    "# # subsample_for_bin : 200000\n",
    "# # subsample_freq : 0\n",
    "# # num_class : 3\n",
    "# # metric : multi_logloss\n",
    "# # min_data_in_leaf : 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "# params = model4.get_params()\n",
    "\n",
    "# print(\"Hyperparamters: \")\n",
    "# for key, value in params.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "\n",
    "# # Save hyperparameters in a JSON\n",
    "# import json\n",
    "\n",
    "# with open('hyperparameters_model4.json', 'w') as f:\n",
    "#     json.dump(params, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model4)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model4.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model4.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model4, 'model4.joblib')\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model4 = load('model4.joblib')\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model4.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission4.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 5, we do the same thing as Model 4 but fix the imbalance class problem, which there are a lot more damage_grade of 2 than 3, and a lot more 3 than 1. \n",
    "\n",
    "Use class_weight hyperparameter and use label_analysis.py to find class weights and look into other things as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# # LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp])\n",
    "\n",
    "# # Define the model\n",
    "# model5 = LGBMClassifier(boosting_type='gbdt', \n",
    "#                        objective='multiclass', \n",
    "#                        num_class=3, \n",
    "#                        metric='multi_logloss', \n",
    "#                        # class_weight={0: 1, 1: 3.5, 2: 2.5}, # I got these values through label_analysis.py\n",
    "#                        random_state=42)\n",
    "\n",
    "# # Define the parameter grid focusing on most important hyperparameters\n",
    "# param_grid = {\n",
    "#     'num_leaves': list(range(200, 400, 50)), # 75 folds took 2 min 43 sec\n",
    "#     'max_depth': [-1], # -1 is the default\n",
    "#     'min_data_in_leaf': [20], # 20 is the default \n",
    "#     'n_estimators': list(range(300, 500, 50)), #default value is 100\n",
    "#     'min_child_samples': list(range(3, 6, 1)), # default value is 20\n",
    "#     # 'learning_rate': [i/100 for i in range(1, 11)], # default is 0.1\n",
    "#     # 'subsample': [i/10 for i in range(1, 11)], # default value is 1.0\n",
    "#     # 'colsample_bytree': [i/10 for i in range(1, 5)], # default value is 1.0\n",
    "# }\n",
    "\n",
    "# # Define the grid search\n",
    "# grid = GridSearchCV(model5, \n",
    "#                     param_grid, \n",
    "#                     verbose=3, \n",
    "#                     cv=2, \n",
    "#                     n_jobs=-1, \n",
    "#                     scoring=make_scorer(f1_score, average='micro'))\n",
    "\n",
    "# # Fit the grid search on the full data\n",
    "# grid.fit(train_X1_full, train_Y1_temp_full)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(f'Best parameters: {grid.best_params_}')\n",
    "# print(f'Best score: {grid.best_score_}')\n",
    "\n",
    "# model5 = grid.best_estimator_\n",
    "\n",
    "# # Use the best estimator to make predictions\n",
    "# y_pred = model5.predict(test_X1)\n",
    "\n",
    "# # Print the F1 score and the accuracy\n",
    "# print('The micro-averaged F1 score of prediction is:', f1_score(test_Y1_temp, y_pred, average='micro'))\n",
    "# print('The accuracy of prediction is:', accuracy_score(test_Y1_temp, y_pred))\n",
    "\n",
    "# # 75 fits. Took 2 min and 43 sec. THIS GOT 0.7405\n",
    "# # Best parameters: {'num_leaves': 252}\n",
    "# # Best score: 0.7361067271692862\n",
    "# # The micro-averaged F1 score of prediction is: 0.7364866593333503\n",
    "# # The accuracy of prediction is: 0.7364866593333503\n",
    "\n",
    "# # 240 fits. Took 6 min and 44 sec. This got 0.7358. \n",
    "# # Best parameters: {'max_depth': 10, 'min_data_in_leaf': 20, 'num_leaves': 250}\n",
    "# # Best score: 0.7282244593923525\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7317541121997391\n",
    "# # The accuracy of prediction is: 0.7317541121997391\n",
    "\n",
    "# # 108 fits. Took 14 min and 15 seconds, probably because max_depth is -1 so that means it's unlimited. This got a 0.7449\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 200}\n",
    "# # Best score: 0.7348516996975305\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7434703640224093\n",
    "# # The accuracy of prediction is: 0.7434703640224093\n",
    "\n",
    "# # 108 fits, with class weights {1.5, 10, 3}. Took 6 minutes and 44 sec. Got a score of 0.7057\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7091463139361655\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7054820802742319\n",
    "# # The accuracy of prediction is: 0.7054820802742319\n",
    "\n",
    "# # 108 fits, version 3 with class weights {1, 3, 2}. Took 5 min and 50 sec. Got a score of 0.7348\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7283192632386799\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7331866670077511\n",
    "# # The accuracy of prediction is: 0.7331866670077511\n",
    "\n",
    "# # 108 fits. with class weights {1, 4, 2}. Took 5 min and 24 sec. Got a score of 0.7260\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7218726016884114\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7242843621293905\n",
    "# # The accuracy of prediction is: 0.7242843621293904\n",
    "\n",
    "# # 108 fits. with class weights {1, 3.5, 2.5}. Took 5 min and 24 sec. Got a score of 0.7355\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7282244593923526\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7317285308638818\n",
    "# # The accuracy of prediction is: 0.7317285308638817\n",
    "\n",
    "# # 108 fits. with class weights {0.5, 3.5, 2.5}. Took 5 min and 43 sec.\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7282244593923526\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7317285308638818\n",
    "# # The accuracy of prediction is: 0.7317285308638817\n",
    "\n",
    "# # 216 fits. with no weights. 11 min and 27 sec. Got a score of 0.7446. \n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 5, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 250}\n",
    "# # Best score: 0.7282244593923526\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=5 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7317285308638818\n",
    "# # The accuracy of prediction is: 0.7317285308638817\n",
    "\n",
    "# #96 fits, Took 26 minutes. Got a score of 0.7449.\n",
    "# # Best parameters: {'max_depth': -1, 'min_child_samples': 3, 'min_data_in_leaf': 20, 'n_estimators': 300, 'num_leaves': 200}\n",
    "# # Best score: 0.7348516996975305\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=3 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7434703640224093\n",
    "# # The accuracy of prediction is: 0.7434703640224093\n",
    "\n",
    "\n",
    "# #96 fits, Took 31 minutes. Got a score of 0.7444.\n",
    "# # Best parameters: {'learning_rate': 0.03, 'max_depth': -1, 'min_child_samples': 3, 'min_data_in_leaf': 20, 'n_estimators': 450, 'num_leaves': 400}\n",
    "# # Best score: 0.7347884971333123\n",
    "# # [LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=3 will be ignored. Current value: min_data_in_leaf=20\n",
    "# # The micro-averaged F1 score of prediction is: 0.7416540891765367\n",
    "# # The accuracy of prediction is: 0.7416540891765367\n",
    "\n",
    "\n",
    "# # Hyperparamters: \n",
    "# # boosting_type : gbdt\n",
    "# # class_weight : None\n",
    "# # colsample_bytree : 1.0\n",
    "# # importance_type : split\n",
    "# # learning_rate : 0.1\n",
    "# # max_depth : -1\n",
    "# # min_child_samples : 5\n",
    "# # min_child_weight : 0.001\n",
    "# # min_split_gain : 0.0\n",
    "# # n_estimators : 300\n",
    "# # n_jobs : None\n",
    "# # num_leaves : 200\n",
    "# # objective : multiclass\n",
    "# # random_state : 42\n",
    "# # reg_alpha : 0.0\n",
    "# # reg_lambda : 0.0\n",
    "# # subsample : 1.0\n",
    "# # subsample_for_bin : 200000\n",
    "# # subsample_freq : 0\n",
    "# # num_class : 3\n",
    "# # metric : multi_logloss\n",
    "# # min_data_in_leaf : 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "# params = model5.get_params()\n",
    "\n",
    "# print(\"Hyperparamters: \")\n",
    "# for key, value in params.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "\n",
    "# # Save hyperparameters in a JSON\n",
    "# import json\n",
    "\n",
    "# with open('hyperparameters_model5.json', 'w') as f: # Make sure to change the name to match whatever model you are using\n",
    "#     json.dump(params, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model5)\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model5.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model5.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model5, 'model5.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model5 = load('model5.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model5.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission5.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 6, we will use another hyper parameter tuning library: Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# # LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# # So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "# train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "# valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "# test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# # Concatenate training and validation sets so that I have more data to fit the model on during GridSearchCV\n",
    "# train_X1_full = pd.concat([train_X1, valid_X1])\n",
    "# train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp])\n",
    "\n",
    "# import optuna\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'objective': 'multiclass',\n",
    "#         'num_class': 3,\n",
    "#         'metric': 'multi_logloss',\n",
    "#         'random_state': 42,\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 200, 400),\n",
    "#         #'max_depth': trial.suggest_int('max_depth', -1, 50),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 30),\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 200, 500),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 2, 10),\n",
    "#     }\n",
    "\n",
    "#     model = LGBMClassifier(**params)\n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#     score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
    "#     f1 = score.mean()\n",
    "#     return f1\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "# # Best trial: score 0.7399033919859633, params {'num_leaves': 234, 'min_data_in_leaf': 17, 'n_estimators': 313, 'min_child_samples': 2}. Score: 0.7448\n",
    "\n",
    "\n",
    "# # [I 2024-01-07 18:08:11,590] A new study created in memory with name: no-name-f4210ae5-1edd-4ba0-ba88-c79abd4e15a5\n",
    "# # [I 2024-01-07 18:08:30,544] Trial 0 finished with value: 0.7395286911714135 and parameters: {'num_leaves': 206, 'min_data_in_leaf': 18, 'n_estimators': 312, 'min_child_samples': 2}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:08:55,049] Trial 1 finished with value: 0.7392397644502283 and parameters: {'num_leaves': 330, 'min_data_in_leaf': 14, 'n_estimators': 241, 'min_child_samples': 5}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:09:19,086] Trial 2 finished with value: 0.7391449609707464 and parameters: {'num_leaves': 222, 'min_data_in_leaf': 26, 'n_estimators': 386, 'min_child_samples': 3}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:09:44,856] Trial 3 finished with value: 0.7381111478026526 and parameters: {'num_leaves': 237, 'min_data_in_leaf': 28, 'n_estimators': 401, 'min_child_samples': 8}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:10:03,141] Trial 4 finished with value: 0.7393300564160129 and parameters: {'num_leaves': 228, 'min_data_in_leaf': 18, 'n_estimators': 310, 'min_child_samples': 10}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:10:29,937] Trial 5 finished with value: 0.7383729906453952 and parameters: {'num_leaves': 209, 'min_data_in_leaf': 20, 'n_estimators': 469, 'min_child_samples': 7}. Best is trial 0 with value: 0.7395286911714135.\n",
    "# # [I 2024-01-07 18:10:50,266] Trial 6 finished with value: 0.7396641266438571 and parameters: {'num_leaves': 293, 'min_data_in_leaf': 15, 'n_estimators': 260, 'min_child_samples': 7}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:11:17,779] Trial 7 finished with value: 0.7385309951809397 and parameters: {'num_leaves': 250, 'min_data_in_leaf': 25, 'n_estimators': 385, 'min_child_samples': 4}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:11:51,407] Trial 8 finished with value: 0.7362647280974784 and parameters: {'num_leaves': 311, 'min_data_in_leaf': 16, 'n_estimators': 445, 'min_child_samples': 6}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:12:25,125] Trial 9 finished with value: 0.7369373828182345 and parameters: {'num_leaves': 354, 'min_data_in_leaf': 30, 'n_estimators': 380, 'min_child_samples': 6}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:12:40,132] Trial 10 finished with value: 0.7393029637943268 and parameters: {'num_leaves': 278, 'min_data_in_leaf': 10, 'n_estimators': 216, 'min_child_samples': 9}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:13:06,741] Trial 11 finished with value: 0.7372624252496419 and parameters: {'num_leaves': 378, 'min_data_in_leaf': 11, 'n_estimators': 293, 'min_child_samples': 3}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:13:31,783] Trial 12 finished with value: 0.7385987112357685 and parameters: {'num_leaves': 282, 'min_data_in_leaf': 23, 'n_estimators': 287, 'min_child_samples': 8}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:13:57,092] Trial 13 finished with value: 0.7392397677518728 and parameters: {'num_leaves': 265, 'min_data_in_leaf': 14, 'n_estimators': 328, 'min_child_samples': 2}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:14:20,797] Trial 14 finished with value: 0.7384993925740954 and parameters: {'num_leaves': 391, 'min_data_in_leaf': 21, 'n_estimators': 257, 'min_child_samples': 5}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:14:36,322] Trial 15 finished with value: 0.738810888236098 and parameters: {'num_leaves': 306, 'min_data_in_leaf': 18, 'n_estimators': 206, 'min_child_samples': 2}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:14:59,843] Trial 16 finished with value: 0.7390682190148693 and parameters: {'num_leaves': 344, 'min_data_in_leaf': 13, 'n_estimators': 263, 'min_child_samples': 7}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:15:21,777] Trial 17 finished with value: 0.7391043310565978 and parameters: {'num_leaves': 203, 'min_data_in_leaf': 17, 'n_estimators': 344, 'min_child_samples': 4}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:15:50,008] Trial 18 finished with value: 0.7389102109331143 and parameters: {'num_leaves': 256, 'min_data_in_leaf': 21, 'n_estimators': 350, 'min_child_samples': 10}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:16:11,087] Trial 19 finished with value: 0.7386980272072128 and parameters: {'num_leaves': 284, 'min_data_in_leaf': 15, 'n_estimators': 279, 'min_child_samples': 7}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:16:32,058] Trial 20 finished with value: 0.7384587615593986 and parameters: {'num_leaves': 322, 'min_data_in_leaf': 12, 'n_estimators': 240, 'min_child_samples': 8}. Best is trial 6 with value: 0.7396641266438571.\n",
    "# # [I 2024-01-07 18:16:52,813] Trial 21 finished with value: 0.7397273280056272 and parameters: {'num_leaves': 236, 'min_data_in_leaf': 18, 'n_estimators': 315, 'min_child_samples': 10}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:17:11,802] Trial 22 finished with value: 0.7388605516939902 and parameters: {'num_leaves': 200, 'min_data_in_leaf': 19, 'n_estimators': 317, 'min_child_samples': 9}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:17:32,825] Trial 23 finished with value: 0.7393887419841763 and parameters: {'num_leaves': 244, 'min_data_in_leaf': 16, 'n_estimators': 303, 'min_child_samples': 9}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:17:52,295] Trial 24 finished with value: 0.7394113146546292 and parameters: {'num_leaves': 219, 'min_data_in_leaf': 21, 'n_estimators': 328, 'min_child_samples': 5}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:18:11,909] Trial 25 finished with value: 0.7387567149153309 and parameters: {'num_leaves': 270, 'min_data_in_leaf': 23, 'n_estimators': 268, 'min_child_samples': 10}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:18:43,309] Trial 26 finished with value: 0.7371450511785201 and parameters: {'num_leaves': 292, 'min_data_in_leaf': 17, 'n_estimators': 419, 'min_child_samples': 4}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:19:07,403] Trial 27 finished with value: 0.7388470073091064 and parameters: {'num_leaves': 229, 'min_data_in_leaf': 19, 'n_estimators': 363, 'min_child_samples': 3}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:19:23,872] Trial 28 finished with value: 0.7392442799992688 and parameters: {'num_leaves': 259, 'min_data_in_leaf': 15, 'n_estimators': 239, 'min_child_samples': 7}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:19:37,297] Trial 29 finished with value: 0.738273669904909 and parameters: {'num_leaves': 213, 'min_data_in_leaf': 13, 'n_estimators': 244, 'min_child_samples': 6}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:19:56,964] Trial 30 finished with value: 0.7386438564543911 and parameters: {'num_leaves': 342, 'min_data_in_leaf': 14, 'n_estimators': 228, 'min_child_samples': 5}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:20:16,141] Trial 31 finished with value: 0.739108841469747 and parameters: {'num_leaves': 214, 'min_data_in_leaf': 22, 'n_estimators': 335, 'min_child_samples': 5}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:20:37,877] Trial 32 finished with value: 0.7393481136597083 and parameters: {'num_leaves': 232, 'min_data_in_leaf': 20, 'n_estimators': 320, 'min_child_samples': 2}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:21:01,415] Trial 33 finished with value: 0.7393977724097001 and parameters: {'num_leaves': 218, 'min_data_in_leaf': 24, 'n_estimators': 363, 'min_child_samples': 4}. Best is trial 21 with value: 0.7397273280056272.\n",
    "# # [I 2024-01-07 18:21:21,869] Trial 34 finished with value: 0.7398040757088111 and parameters: {'num_leaves': 236, 'min_data_in_leaf': 18, 'n_estimators': 296, 'min_child_samples': 5}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:21:45,576] Trial 35 finished with value: 0.7392217062282677 and parameters: {'num_leaves': 243, 'min_data_in_leaf': 18, 'n_estimators': 277, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:22:03,788] Trial 36 finished with value: 0.739257823772737 and parameters: {'num_leaves': 237, 'min_data_in_leaf': 17, 'n_estimators': 300, 'min_child_samples': 6}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:22:21,188] Trial 37 finished with value: 0.7397860118618267 and parameters: {'num_leaves': 225, 'min_data_in_leaf': 19, 'n_estimators': 256, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:22:42,253] Trial 38 finished with value: 0.7396370370792492 and parameters: {'num_leaves': 296, 'min_data_in_leaf': 19, 'n_estimators': 257, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:23:01,084] Trial 39 finished with value: 0.7393752031631747 and parameters: {'num_leaves': 249, 'min_data_in_leaf': 16, 'n_estimators': 282, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:23:21,776] Trial 40 finished with value: 0.7391855885003737 and parameters: {'num_leaves': 229, 'min_data_in_leaf': 26, 'n_estimators': 309, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:23:41,618] Trial 41 finished with value: 0.73897341094977 and parameters: {'num_leaves': 302, 'min_data_in_leaf': 19, 'n_estimators': 256, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:24:01,502] Trial 42 finished with value: 0.738819920740435 and parameters: {'num_leaves': 317, 'min_data_in_leaf': 20, 'n_estimators': 223, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:24:16,773] Trial 43 finished with value: 0.7390953014259142 and parameters: {'num_leaves': 273, 'min_data_in_leaf': 18, 'n_estimators': 202, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:24:34,954] Trial 44 finished with value: 0.7385084196979746 and parameters: {'num_leaves': 294, 'min_data_in_leaf': 15, 'n_estimators': 250, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:24:53,217] Trial 45 finished with value: 0.7393661685188831 and parameters: {'num_leaves': 254, 'min_data_in_leaf': 17, 'n_estimators': 273, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:25:30,059] Trial 46 finished with value: 0.7350458255371327 and parameters: {'num_leaves': 329, 'min_data_in_leaf': 19, 'n_estimators': 483, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:25:51,635] Trial 47 finished with value: 0.7388921523443045 and parameters: {'num_leaves': 265, 'min_data_in_leaf': 18, 'n_estimators': 291, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:26:14,754] Trial 48 finished with value: 0.7382691577186544 and parameters: {'num_leaves': 371, 'min_data_in_leaf': 16, 'n_estimators': 228, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:26:35,012] Trial 49 finished with value: 0.73902306688725 and parameters: {'num_leaves': 289, 'min_data_in_leaf': 20, 'n_estimators': 264, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:26:59,826] Trial 50 finished with value: 0.7391314206823473 and parameters: {'num_leaves': 312, 'min_data_in_leaf': 22, 'n_estimators': 295, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:27:18,858] Trial 51 finished with value: 0.739528692088537 and parameters: {'num_leaves': 222, 'min_data_in_leaf': 17, 'n_estimators': 312, 'min_child_samples': 3}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:27:38,811] Trial 52 finished with value: 0.7392803984608616 and parameters: {'num_leaves': 221, 'min_data_in_leaf': 17, 'n_estimators': 310, 'min_child_samples': 6}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:27:58,586] Trial 53 finished with value: 0.739068213450987 and parameters: {'num_leaves': 240, 'min_data_in_leaf': 30, 'n_estimators': 288, 'min_child_samples': 3}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:28:20,843] Trial 54 finished with value: 0.7395918922886174 and parameters: {'num_leaves': 209, 'min_data_in_leaf': 15, 'n_estimators': 341, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:28:44,833] Trial 55 finished with value: 0.7388424894366864 and parameters: {'num_leaves': 211, 'min_data_in_leaf': 13, 'n_estimators': 395, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:29:08,785] Trial 56 finished with value: 0.7391133626438114 and parameters: {'num_leaves': 233, 'min_data_in_leaf': 15, 'n_estimators': 366, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:29:34,592] Trial 57 finished with value: 0.7382194945664701 and parameters: {'num_leaves': 276, 'min_data_in_leaf': 21, 'n_estimators': 344, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:29:47,331] Trial 58 finished with value: 0.7381562903310467 and parameters: {'num_leaves': 206, 'min_data_in_leaf': 10, 'n_estimators': 214, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:30:11,807] Trial 59 finished with value: 0.7396641254821673 and parameters: {'num_leaves': 264, 'min_data_in_leaf': 19, 'n_estimators': 328, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:30:30,511] Trial 60 finished with value: 0.7386303098072694 and parameters: {'num_leaves': 262, 'min_data_in_leaf': 19, 'n_estimators': 256, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:30:54,593] Trial 61 finished with value: 0.7384316781089472 and parameters: {'num_leaves': 249, 'min_data_in_leaf': 12, 'n_estimators': 340, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:31:22,290] Trial 62 finished with value: 0.7385761389321649 and parameters: {'num_leaves': 283, 'min_data_in_leaf': 14, 'n_estimators': 356, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:31:48,936] Trial 63 finished with value: 0.7383775004471289 and parameters: {'num_leaves': 298, 'min_data_in_leaf': 20, 'n_estimators': 324, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:32:12,371] Trial 64 finished with value: 0.7389237534837512 and parameters: {'num_leaves': 226, 'min_data_in_leaf': 18, 'n_estimators': 381, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:32:32,286] Trial 65 finished with value: 0.7391720491290981 and parameters: {'num_leaves': 206, 'min_data_in_leaf': 16, 'n_estimators': 329, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:32:50,534] Trial 66 finished with value: 0.7381698369781682 and parameters: {'num_leaves': 200, 'min_data_in_leaf': 22, 'n_estimators': 303, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:33:10,799] Trial 67 finished with value: 0.7393842236837654 and parameters: {'num_leaves': 268, 'min_data_in_leaf': 18, 'n_estimators': 270, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:33:32,051] Trial 68 finished with value: 0.7388786069811556 and parameters: {'num_leaves': 306, 'min_data_in_leaf': 19, 'n_estimators': 237, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:33:59,494] Trial 69 finished with value: 0.7379982851840866 and parameters: {'num_leaves': 236, 'min_data_in_leaf': 16, 'n_estimators': 420, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:34:20,447] Trial 70 finished with value: 0.738883121551931 and parameters: {'num_leaves': 249, 'min_data_in_leaf': 28, 'n_estimators': 333, 'min_child_samples': 5}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:34:40,447] Trial 71 finished with value: 0.7393435984775173 and parameters: {'num_leaves': 222, 'min_data_in_leaf': 17, 'n_estimators': 316, 'min_child_samples': 3}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:34:58,141] Trial 72 finished with value: 0.7396370390357792 and parameters: {'num_leaves': 216, 'min_data_in_leaf': 15, 'n_estimators': 280, 'min_child_samples': 2}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:35:15,822] Trial 73 finished with value: 0.7388379804909347 and parameters: {'num_leaves': 215, 'min_data_in_leaf': 15, 'n_estimators': 281, 'min_child_samples': 2}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:35:32,681] Trial 74 finished with value: 0.7386664249060763 and parameters: {'num_leaves': 227, 'min_data_in_leaf': 14, 'n_estimators': 261, 'min_child_samples': 6}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:35:48,479] Trial 75 finished with value: 0.7391223891562753 and parameters: {'num_leaves': 210, 'min_data_in_leaf': 21, 'n_estimators': 249, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:36:08,005] Trial 76 finished with value: 0.7390817602815334 and parameters: {'num_leaves': 243, 'min_data_in_leaf': 12, 'n_estimators': 300, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:36:30,391] Trial 77 finished with value: 0.7392036472726083 and parameters: {'num_leaves': 256, 'min_data_in_leaf': 14, 'n_estimators': 274, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:36:57,688] Trial 78 finished with value: 0.7382510946665103 and parameters: {'num_leaves': 288, 'min_data_in_leaf': 19, 'n_estimators': 352, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:37:16,566] Trial 79 finished with value: 0.7385671105854542 and parameters: {'num_leaves': 217, 'min_data_in_leaf': 16, 'n_estimators': 283, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:37:32,527] Trial 80 finished with value: 0.7397950490129226 and parameters: {'num_leaves': 233, 'min_data_in_leaf': 20, 'n_estimators': 291, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:37:49,607] Trial 81 finished with value: 0.739479031259732 and parameters: {'num_leaves': 236, 'min_data_in_leaf': 20, 'n_estimators': 294, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:38:07,183] Trial 82 finished with value: 0.7390546738351452 and parameters: {'num_leaves': 227, 'min_data_in_leaf': 18, 'n_estimators': 268, 'min_child_samples': 6}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:38:27,130] Trial 83 finished with value: 0.7387792880749161 and parameters: {'num_leaves': 245, 'min_data_in_leaf': 11, 'n_estimators': 322, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:38:41,625] Trial 84 finished with value: 0.738828946396917 and parameters: {'num_leaves': 206, 'min_data_in_leaf': 20, 'n_estimators': 251, 'min_child_samples': 4}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:38:59,410] Trial 85 finished with value: 0.7386528871856228 and parameters: {'num_leaves': 231, 'min_data_in_leaf': 21, 'n_estimators': 307, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:39:13,136] Trial 86 finished with value: 0.7386483718811485 and parameters: {'num_leaves': 240, 'min_data_in_leaf': 15, 'n_estimators': 236, 'min_child_samples': 7}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:39:39,285] Trial 87 finished with value: 0.7387070587944263 and parameters: {'num_leaves': 225, 'min_data_in_leaf': 18, 'n_estimators': 373, 'min_child_samples': 6}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:39:55,899] Trial 88 finished with value: 0.7394700030353044 and parameters: {'num_leaves': 216, 'min_data_in_leaf': 13, 'n_estimators': 284, 'min_child_samples': 9}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:40:17,562] Trial 89 finished with value: 0.7391449658009299 and parameters: {'num_leaves': 278, 'min_data_in_leaf': 19, 'n_estimators': 297, 'min_child_samples': 8}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:40:38,047] Trial 90 finished with value: 0.7384000724450246 and parameters: {'num_leaves': 298, 'min_data_in_leaf': 23, 'n_estimators': 264, 'min_child_samples': 10}. Best is trial 34 with value: 0.7398040757088111.\n",
    "# # [I 2024-01-07 18:40:56,382] Trial 91 finished with value: 0.7399033919859633 and parameters: {'num_leaves': 234, 'min_data_in_leaf': 17, 'n_estimators': 313, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:41:18,761] Trial 92 finished with value: 0.7393255381156019 and parameters: {'num_leaves': 252, 'min_data_in_leaf': 17, 'n_estimators': 335, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:41:39,432] Trial 93 finished with value: 0.7397995583255238 and parameters: {'num_leaves': 234, 'min_data_in_leaf': 17, 'n_estimators': 316, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:41:59,714] Trial 94 finished with value: 0.738837978901254 and parameters: {'num_leaves': 233, 'min_data_in_leaf': 18, 'n_estimators': 316, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:42:18,282] Trial 95 finished with value: 0.7389463297004149 and parameters: {'num_leaves': 240, 'min_data_in_leaf': 17, 'n_estimators': 289, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:42:35,240] Trial 96 finished with value: 0.738964384620731 and parameters: {'num_leaves': 263, 'min_data_in_leaf': 16, 'n_estimators': 274, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:43:00,141] Trial 97 finished with value: 0.7378673675840631 and parameters: {'num_leaves': 324, 'min_data_in_leaf': 20, 'n_estimators': 305, 'min_child_samples': 3}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:43:21,264] Trial 98 finished with value: 0.73887860832627 and parameters: {'num_leaves': 223, 'min_data_in_leaf': 19, 'n_estimators': 313, 'min_child_samples': 2}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # [I 2024-01-07 18:43:42,930] Trial 99 finished with value: 0.7388334651864604 and parameters: {'num_leaves': 246, 'min_data_in_leaf': 18, 'n_estimators': 326, 'min_child_samples': 3}. Best is trial 91 with value: 0.7399033919859633.\n",
    "# # Best trial: score 0.7399033919859633, params {'num_leaves': 234, 'min_data_in_leaf': 17, 'n_estimators': 313, 'min_child_samples': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #1. Get the hyperparameters of the model and print them\n",
    "# best_params = study.best_trial.params\n",
    "\n",
    "# # Add the static parameters\n",
    "# best_params.update({\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_class': 3,\n",
    "#     'metric': 'multi_logloss',\n",
    "#     'random_state': 42,\n",
    "# })\n",
    "\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Write the hyperparameters to a new file\n",
    "# with open('hyperparameters_model6.json', 'w') as f:\n",
    "#     json.dump(best_params, f)\n",
    "\n",
    "\n",
    "\n",
    "# # Create the model with the best parameters\n",
    "# model6 = LGBMClassifier(**best_params)\n",
    "\n",
    "# # Fit the model to the full data\n",
    "# model6.fit(train_X1_full, train_Y1_temp_full)\n",
    "\n",
    "\n",
    "\n",
    "# # #2. Plot bar plot of the feature importances to visualize the model\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a larger figure\n",
    "# plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# # Plot the feature importance\n",
    "# ax = lgb.plot_importance(model6) # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Decrease the font size of the y-axis labels since there are two many features\n",
    "# for label in ax.get_yticklabels():\n",
    "#     label.set_size(8)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save the plot\n",
    "# plt.savefig('feature_importance_model6.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "# # from optuna.visualization import plot_param_importances\n",
    "\n",
    "# # plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Concatenate the datasets\n",
    "# full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "# full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# # Refit the model on the full dataset\n",
    "# model6.fit(full_X, full_Y_temp)\n",
    "\n",
    "# # Save the model to a file using joblib\n",
    "# from joblib import dump\n",
    "# dump(model6, 'model6.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # # This is how to load the model from joblib\n",
    "# # from joblib import load\n",
    "# # model5 = load('model5.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# # Separate building_ids and features in the test data\n",
    "# competition_test_building_ids = test_data1['building_id']\n",
    "# competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# # Predict on the competition test data\n",
    "# competition_y_pred = model6.predict(competition_test_X)\n",
    "\n",
    "# # Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "# competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# # Create a DataFrame for submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'building_id': competition_test_building_ids,\n",
    "#     'damage_grade': competition_y_pred\n",
    "# })\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file for submission\n",
    "# submission.to_csv('submission6.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "# print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE TO TRY FEATURE SELECTON ON OPTUNA AND DEFINITELY TRY PRUNING\n",
    "\n",
    "THEN, try using the different feature selections like features2 and features3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Model 7, we will continue the hyper parameter from model 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-08 21:01:48,479] A new study created in memory with name: no-name-96b96c56-a910-4836-9456-6f9120537be6\n",
      "[I 2024-01-08 21:02:16,977] Trial 0 finished with value: 0.7269784709859567 and parameters: {'learning_rate': 0.00880897278383721, 'feature_fraction': 0.5419912198177617, 'bagging_fraction': 0.7371723310753362, 'lambda_l1': 0.25682076630843903, 'lambda_l2': 5.45556404079999e-08, 'max_bin': 364}. Best is trial 0 with value: 0.7269784709859567.\n",
      "[I 2024-01-08 21:02:38,529] Trial 1 finished with value: 0.7413119067023057 and parameters: {'learning_rate': 0.05598726432532204, 'feature_fraction': 0.827869849932211, 'bagging_fraction': 0.48612557019360647, 'lambda_l1': 0.002047688621043951, 'lambda_l2': 1.1842514647957114, 'max_bin': 338}. Best is trial 1 with value: 0.7413119067023057.\n",
      "[I 2024-01-08 21:03:02,919] Trial 2 finished with value: 0.731673519490157 and parameters: {'learning_rate': 0.0115864343052812, 'feature_fraction': 0.7242849958527782, 'bagging_fraction': 0.6026981987648514, 'lambda_l1': 0.007070988539942633, 'lambda_l2': 3.683818919307624e-08, 'max_bin': 262}. Best is trial 1 with value: 0.7413119067023057.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# LGBMClassifier expects the labels data to only include one column. Currently, they include two columns: building_id and damage_grade.\n",
    "# So, I am making it so the labels data only includes one column (damage_grade) and changing the range from 1-3 to 0-2 since LGBMClassifier wants to start at 0\n",
    "train_Y1_temp = train_Y1['damage_grade'] - 1  \n",
    "valid_Y1_temp = valid_Y1['damage_grade'] - 1\n",
    "test_Y1_temp = test_Y1['damage_grade'] - 1\n",
    "\n",
    "# Concatenate training and validation sets so that I have more data to fit the model on during GridSearchCV\n",
    "train_X1_full = pd.concat([train_X1, valid_X1])\n",
    "train_Y1_temp_full = pd.concat([train_Y1_temp, valid_Y1_temp])\n",
    "\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # I got these values from the previous optuna trial, from model 6\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        'random_state': 42,\n",
    "        'num_leaves': 234,\n",
    "        #'max_depth': trial.suggest_int('max_depth', -1, 50),\n",
    "        'min_data_in_leaf': 17,\n",
    "        'n_estimators': 313,\n",
    "        'min_child_samples': 12,\n",
    "\n",
    "        # These are the new hyper parameters which I want to test out\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True), # default value is 0.1\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0), # default value is 1.0\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0), # default value is 1.0\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True), # default value is 0.0\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True), # default value is 0.0\n",
    "        'max_bin': trial.suggest_int('max_bin', 255, 500), # default value is 255\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, train_X1_full, train_Y1_temp_full, cv=cv, scoring='f1_micro', n_jobs=-1)\n",
    "    f1 = score.mean()\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Get the hyperparameters of the model and print them\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Add the static parameters\n",
    "best_params.update({\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'random_state': 42,\n",
    "})\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Write the hyperparameters to a new file\n",
    "with open('hyperparameters_model7.json', 'w') as f: # Make sure to change the name to match whatever model you are using\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "\n",
    "\n",
    "# Create the model with the best parameters\n",
    "model7 = LGBMClassifier(**best_params) # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# Fit the model to the full data\n",
    "model7.fit(train_X1_full, train_Y1_temp_full)\n",
    "\n",
    "\n",
    "\n",
    "# #2. Plot bar plot of the feature importances to visualize the model\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a larger figure\n",
    "plt.figure(figsize=(80, 20), dpi=200)\n",
    "\n",
    "# Plot the feature importance\n",
    "ax = lgb.plot_importance(model7) \n",
    "\n",
    "# Decrease the font size of the y-axis labels since there are two many features\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_size(8)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('feature_importance_model7.png') # Make sure to change the name to match whatever model you are using\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# from optuna.visualization import plot_param_importances\n",
    "\n",
    "# plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Concatenate the datasets\n",
    "full_X = pd.concat([train_X1, valid_X1, test_X1])\n",
    "full_Y_temp = pd.concat([train_Y1_temp, valid_Y1_temp, test_Y1_temp])\n",
    "\n",
    "# Refit the model on the full dataset\n",
    "model7.fit(full_X, full_Y_temp)\n",
    "\n",
    "# Save the model to a file using joblib\n",
    "from joblib import dump\n",
    "dump(model7, 'model7.joblib') # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "\n",
    "# # This is how to load the model from joblib\n",
    "# from joblib import load\n",
    "# model5 = load('model5.joblib')  # Make sure to change the name of the file to match up with the model number!\n",
    "\n",
    "# Separate building_ids and features in the test data\n",
    "competition_test_building_ids = test_data1['building_id']\n",
    "competition_test_X = test_data1.drop('building_id', axis=1)\n",
    "\n",
    "# Predict on the competition test data\n",
    "competition_y_pred = model7.predict(competition_test_X)\n",
    "\n",
    "# Since the competition expects labels in the range 1-3, add 1 to the predictions\n",
    "competition_y_pred = competition_y_pred + 1\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'building_id': competition_test_building_ids,\n",
    "    'damage_grade': competition_y_pred\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file for submission\n",
    "submission.to_csv('submission7.csv', index=False) # Make sure to change the name of the submission file to match up with the model number!\n",
    "print(\"Successfully Submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE TO TRY FEATURE SELECTON ON OPTUNA AND DEFINITELY TRY PRUNING\n",
    "\n",
    "THEN, try using the different feature selections like features2 and features3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. The competition is based on the micro averaged F1 score. Find a way to try and optimize this score during hyper parameter tuning, and make sure to make it clear that I should be looking at the scoring metric of every competition and making sure I optimize this scoring metric specifically when I do hyper parameter tuning and such, and not only focusing on accuracy\n",
    "- Analyze confusion matrix and classification reports to understand model performance better.\n",
    "\n",
    "### 1. Try other hyperparameter tuning libraries other than keras tuner, like optuna\n",
    "\n",
    "### 2. Try XGBoost\n",
    "\n",
    "### 3. Maybe go back and normalize the data since you decided to skip that step\n",
    "\n",
    "### 4. Upload my notebook to ChatGPT Michael ML Tutor and ask it what else I can do to improve it as an ML engineer. turn this file into a python script and then copy and paste it into ChatGPT Michael ML Tutor \n",
    "\n",
    "### 5. Look into LightGBM GPU training and parallel training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
